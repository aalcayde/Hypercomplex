{"search-results":{"opensearch:totalResults":"5","opensearch:startIndex":"0","opensearch:itemsPerPage":"5","opensearch:Query":{"@role": "request", "@searchTerms": "TITLE-ABS-KEY ( hypercomplex OR hyper-complex OR hipercomplex OR quaternions OR octonions OR quaternionic ) AND TITLE-ABS-KEY ( signal OR image )", "@startPage": "0"},"link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=1989&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "first", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=1989&sort=+pubyear&view=complete", "@type": "application/json"}],"entry": [{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024867055"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024867055?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0024867055&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0024867055&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0024867055","dc:identifier":"SCOPUS_ID:0024867055","eid":"2-s2.0-0024867055","dc:title":"Using motion from orthographic projections to prune 3-D point matches.","dc:creator":"Chen H.","prism:isbn": [{"@_fa": "true", "$" :"0818619031"}],"prism:pageRange":"290-297","prism:coverDate":"1989-12-01","prism:coverDisplayDate":"1989","dc:description":"The goodness of a match between points, given their 3-D position estimates, can be evaluated by checking how close corresponding points can be brought to coincide by a rigid motion computed from the given data. The performance of this well-known technique, however, is affected by the data accuracy. In stereo imagery, the depth values (z coordinates) estimated are subject to a higher range of uncertainty than the other two coordinates. The authors present a novel pruning method that discards the z coordinates and uses only x and y coordinates of points to compute the motion. It is noted that this method is more effective in detecting false pairings than the traditional method, which uses full 3-D coordinates. Since only x and y coordinates are used, the method is virtually equivalent to computing motion from orthographic projections. The authors dervive a least-squares solution to this motion problem and show that the determination of rotation and translation can be decoupled. The locus of rotation is proved to be a great circle on a unit quaternion sphere. Results of testing this method on real and random data are shown.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60021378","afid":"60021378","affilname":"Nokia Bell Labs","affiliation-city":"Murray","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/8236841800","authid":"8236841800","authname":"Chen H.","surname":"Chen","given-name":"Homer H.","initials":"H.H.","afid": [{"@_fa": "true", "$" :"60021378"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/35513984600","authid":"35513984600","authname":"Huang T.","surname":"Huang","given-name":"Thomas S.","initials":"T.S.","afid": [{"@_fa": "true", "$" :"60021378"}]}],"source-id":"144420","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024864026"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024864026?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0024864026&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0024864026&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0024864026","dc:identifier":"SCOPUS_ID:0024864026","eid":"2-s2.0-0024864026","dc:title":"3-D object location determination using line-segments matching","dc:creator":"Shao L.","prism:pageRange":"145-150","prism:coverDate":"1989-12-01","prism:coverDisplayDate":"1989","dc:description":"A real-time high-accuracy object localization system is presented. This system uses a line range sensor to acquire range data from which it extracts line features of an object such as boundary edges for planar surfaces and axes for surfaces of revolution. A combination of Hough transform, filtering, and least-squares optimization techniques is used to guarantee high-precision feature extraction. Line-line matching is then used to compute the transformation parameters. When multiline features are known, a fast optimal algorithm utilizing dual number quaternions is used to carry out the computations.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025778","afid":"60025778","affilname":"University of Michigan, Ann Arbor","affiliation-city":"Ann Arbor","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7202842846","authid":"7202842846","authname":"Shao L.","surname":"Shao","given-name":"Lejun","initials":"L.","afid": [{"@_fa": "true", "$" :"60025778"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006417396","authid":"7006417396","authname":"Volz R.","surname":"Volz","given-name":"Richard A.","initials":"R.A.","afid": [{"@_fa": "true", "$" :"60025778"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/7403868450","authid":"7403868450","authname":"Walker M.","surname":"Walker","given-name":"Michael W.","initials":"M.W.","afid": [{"@_fa": "true", "$" :"60025778"}]}],"source-id":"144415","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024867340"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024867340?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0024867340&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0024867340&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-0893608089900026"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0024867340","dc:identifier":"SCOPUS_ID:0024867340","eid":"2-s2.0-0024867340","dc:title":"Invariant recognition of cluttered scenes by a self-organizing ART architecture: CORT-X boundary segmentation","dc:creator":"Carpenter G.A.","prism:publicationName":"Neural Networks","prism:issn":"08936080","prism:volume":"2","prism:issueIdentifier":"3","prism:pageRange":"169-181","prism:coverDate":"1989-01-01","prism:coverDisplayDate":"1989","prism:doi":"10.1016/0893-6080(89)90002-6","pii":"0893-6080(89)90002-6","dc:description":"A neural network architecture is outlined that self-organizes invariant pattern recognition codes of noisy images. The processing stages are figure-ground separation, boundary segmentation, invariant filtering, and self-organization of a pattern recognition code by an ART 2 network. The article describes a new circuit for boundary segmentation, called the CORT-X filter, that detects, regularizes, and completes sharp (even one-pixel wide) image boundaries in up to 50% noise, while simultaneously suppressing the noise. The CORT-X filter achieves this competence by using nonlinear interactions between multiple spatial scales to resolve a design trade-off that exists between the properties of boundary localization, boundary completion, and noise suppression. The processing levels of the CORT-X filter are analogous to those of the Grossberg-Mingolla Boundary Contour System, but contain only feedforward operations that are easier to implement in hardware. The network nodes in these levels are analogous to cortical simple cells, complex cells, hypercomplex cells, and unoriented and oriented cooperative cells. © 1989.","citedby-count":"44","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019674","afid":"60019674","affilname":"Boston University","affiliation-city":"Boston","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7201701766","authid":"7201701766","authname":"Carpenter G.A.","surname":"Carpenter","given-name":"Gail A.","initials":"G.A.","afid": [{"@_fa": "true", "$" :"60019674"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7101773583","authid":"7101773583","authname":"Grossberg S.","surname":"Grossberg","given-name":"Stephen","initials":"S.","afid": [{"@_fa": "true", "$" :"60019674"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/6507682376","authid":"6507682376","authname":"Mehanian C.","surname":"Mehanian","given-name":"Courosh","initials":"C.","afid": [{"@_fa": "true", "$" :"60019674"}]}],"authkeywords":"Boundary contour system | Boundary segmentation | Competition cooperation | Neural networks | Pattern recognition | Self-organization | Visual cortex","source-id":"24804","fund-acr":"NSF","fund-no":"DMS-86-11959","fund-sponsor":"National Science Foundation","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024855885"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024855885?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0024855885&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0024855885&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0024855885","dc:identifier":"SCOPUS_ID:0024855885","eid":"2-s2.0-0024855885","dc:title":"Faster Phong Shading via Angular Interpolation","dc:creator":"Kuijk A.","prism:publicationName":"Computer Graphics Forum","prism:issn":"01677055","prism:eIssn":"14678659","prism:volume":"8","prism:issueIdentifier":"4","prism:pageRange":"315-324","prism:coverDate":"1989-01-01","prism:coverDisplayDate":"December 1989","prism:doi":"10.1111/j.1467-8659.1989.tb00513.x","dc:description":"One of the most successful algorithms that brought realism to the world of 3D image generation is Phong shading. It is an algorithm for smooth shading meshes of planar polygons used to represent curved surfaces. The level of realism and depth perception that can be obtained by Phong shading is attractive for 3D CAD applications and related areas. However, per pixel computation costs which were too high and/or artifacts, introduced by some of the more efficient evaluation methods and apparent only when displaying moving objects, are major factors mat blocked the common usage of Phong shading in highly interactive applications. In this paper we present angular interpolation for Phong shading planar polygons. Angular interpolation was a method especially designed to meet requirements as imposed by special purpose hardware we developed1, but turned out to be generally applicable. The angular interpolation method appears to be very efficient and reduces artifacts when displaying moving objects. Ideally a shading algorithm imposes no need for subdivision of patches as presented by the solid modelling system. Shading calculation via angular interpolation yields such an ideal algorithm. We will describe two alternative evaluation methods that trade off evaluation cost against level of accuracy. They both can handle light source and view point at arbitrary distances, but differ in level of accuracy. As a consequence these alternative evaluation methods do impose restrictions on the topology of patches and light sources. However, generally, the limitations imposed by these alternative shading methods are much more liberal than the limitations on patch size imposed by the geometry. The most economic evaluation method we present can incrementally compute the colour intensity along a scanline by two additions per pixel. The methods presented are generally applicable and can easily be implemented in hardware. Copyright © 1989, Wiley Blackwell. All rights reserved","citedby-count":"35","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60011575","afid":"60011575","affilname":"Centrum Wiskunde &amp; Informatica","affiliation-city":"Amsterdam","affiliation-country":"Netherlands"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/35183219500","authid":"35183219500","authname":"Kuijk A.","surname":"Kuijk","given-name":"AAM","initials":"A.","afid": [{"@_fa": "true", "$" :"60011575"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006460560","authid":"7006460560","authname":"Blake E.H.","surname":"Blake","given-name":"E. H.","initials":"E.H.","afid": [{"@_fa": "true", "$" :"60011575"}]}],"authkeywords":"angular interpolation | image synthesis | quandratic approximation | quaternions | shading | spherical geometry","source-id":"25023","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024775135"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0024775135?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0024775135&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0024775135&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-0893608089900130"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0024775135","dc:identifier":"SCOPUS_ID:0024775135","eid":"2-s2.0-0024775135","dc:title":"Stereo boundary fusion by cortical complex cells: A system of maps, filters, and feedback networks for multiplexing distributed data","dc:creator":"Grossberg S.","prism:publicationName":"Neural Networks","prism:issn":"08936080","prism:volume":"2","prism:issueIdentifier":"1","prism:pageRange":"29-51","prism:coverDate":"1989-01-01","prism:coverDisplayDate":"1989","prism:doi":"10.1016/0893-6080(89)90013-0","pii":"0893-6080(89)90013-0","dc:description":"A neural network model of multiple-scale binocular fusion and rivalry in visual cortex is described and simulated on the computer. The model consists of three parts: a distributed spatial representation of binocular input patterns among simple cells that are organized into ocular dominance columns; an adaptive filter from simple cells to complex cells; and a nonlinear on-center off-surround shunting feedback network that joins together the complex cells. This data structure generates complex cell receptive fields which multiplex input position, orientation, spatial frequency, positional disparity, and orientational disparity, and which are insensitive to direction-of-contrast in the image. Multiple copies of this circuit are replicated in the model using receptive fields of different sizes. Within each such circuit, the simple cell and complex cell receptive field sizes covary. Together these circuits define a self-similar multiple-scale network. The self-similarity property across spatial scales enables the network to exhibit a size-disparity correlation, whereby simultaneous binocular fusion and rivalry can occur among the spatial scales corresponding to a given retinal region. It is shown that a laminar organization of the model interactions among the complex cells gives rise to conceptually simple growth rules for intercellular connections. The output patterns of the model complex cells are designed to feed into the model hypercomplex cells at the first competitive stage of a Boundary Contour System network, where they trigger a process of multiple-scale emergent binocular boundary segmentation. The modeling results are compared with psychophysical data about binocular fusion and rivalry, as well as with the cepstrum stereo model of Yeshurun and Schwartz. The results indicate that analogous self-similar multiple-scale neural networks may be used to carry out data fusion of many other types of spatially organized data structures. © 1989.","citedby-count":"37","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019674","afid":"60019674","affilname":"Boston University","affiliation-city":"Boston","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7101773583","authid":"7101773583","authname":"Grossberg S.","surname":"Grossberg","given-name":"Stephen","initials":"S.","afid": [{"@_fa": "true", "$" :"60019674"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/35502315700","authid":"35502315700","authname":"Marshall J.A.","surname":"Marshall","given-name":"Jonathan A.","initials":"J.A.","afid": [{"@_fa": "true", "$" :"60019674"}]}],"authkeywords":"Binocular rivalry | Binocular vision | Complex cells | Computational map | Data fusion | Multiplexing | Neural network | nonlinear feedback network | Ocular dominance columns | Self-similarity | Size-disparity correlation | Visual cortex","source-id":"24804","fund-acr":"NSF","fund-no":"IRI-84-17756","fund-sponsor":"National Science Foundation","openaccess":"0","openaccessFlag":false}]}}