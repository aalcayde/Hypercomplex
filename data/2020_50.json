{"search-results":{"opensearch:totalResults":"235","opensearch:startIndex":"50","opensearch:itemsPerPage":"25","opensearch:Query":{"@role": "request", "@searchTerms": "TITLE-ABS-KEY ( hypercomplex OR hyper-complex OR hipercomplex OR quaternions OR octonions OR quaternionic ) AND TITLE-ABS-KEY ( signal OR image )", "@startPage": "50"},"link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/search/scopus?start=50&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2020&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "first", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2020&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "prev", "@href": "https://api.elsevier.com/content/search/scopus?start=25&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2020&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "next", "@href": "https://api.elsevier.com/content/search/scopus?start=75&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2020&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "last", "@href": "https://api.elsevier.com/content/search/scopus?start=210&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2020&sort=+pubyear&view=complete", "@type": "application/json"}],"entry": [{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85097311701"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85097311701?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097311701&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85097311701&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85097311701","dc:identifier":"SCOPUS_ID:85097311701","eid":"2-s2.0-85097311701","dc:title":"Measurement of respiratory rate with inertial measurement units","dc:creator":"Beck S.","prism:publicationName":"Current Directions in Biomedical Engineering","prism:eIssn":"23645504","prism:volume":"6","prism:issueIdentifier":"3","prism:pageRange":null,"prism:coverDate":"2020-09-01","prism:coverDisplayDate":"1 September 2020","prism:doi":"10.1515/cdbme-2020-3060","dc:description":"Demographic changes and increasing air pollution entail that monitoring of respiratory parameters is in the focus of research. In this study, two customary inertial measurement units (IMUs) are used to measure the breathing rate by using quaternions. One IMU was located ventral, and one was located dorsal on the thorax with a belt. The relative angle between the quaternion of each IMU was calculated and compared to the respiratory frequency obtained by a spirometer, which was used as a reference. A frequency analysis of both signals showed that the obtained respiratory rates vary slightly (less than 0.2/min) between the two systems. The introduced belt can analyse the respiratory rate and can be used for surveillance tasks in clinical settings.","citedby-count":"6","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60078281","afid":"60078281","affilname":"Hochschule Furtwangen","affiliation-city":"Furtwangen","affiliation-country":"Germany"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57220483886","authid":"57220483886","authname":"Beck S.","surname":"Beck","given-name":"Simon","initials":"S.","afid": [{"@_fa": "true", "$" :"60078281"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57052594900","authid":"57052594900","authname":"Laufer B.","surname":"Laufer","given-name":"Bernhard","initials":"B.","afid": [{"@_fa": "true", "$" :"60078281"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/55820759400","authid":"55820759400","authname":"Krueger-Ziolek S.","surname":"Krueger-Ziolek","given-name":"Sabine","initials":"S.","afid": [{"@_fa": "true", "$" :"60078281"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/56794931400","authid":"56794931400","authname":"Moeller K.","surname":"Moeller","given-name":"Knut","initials":"K.","afid": [{"@_fa": "true", "$" :"60078281"}]}],"authkeywords":"inertial measurement unit | quaternions | respiratory rate | spontaneous breathing","article-number":"20203060","source-id":"21100894488","fund-acr":"BMBF","fund-no":"872488-DCPM","fund-sponsor":"Bundesministerium für Bildung und Forschung","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"},{"$" :"repository"},{"$" :"repositoryvor"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85097240941"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85097240941?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85097240941&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85097240941&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85097240941","dc:identifier":"SCOPUS_ID:85097240941","eid":"2-s2.0-85097240941","dc:title":"L-ICPSnet: LiDAR Indoor Camera Positioning System for RGB to Point Cloud Translation using End2End Generative Network","dc:creator":"Ghofrani A.","prism:publicationName":"8th Iranian Joint Congress on Fuzzy and Intelligent Systems, CFIS 2020","prism:isbn": [{"@_fa": "true", "$" :"9781728173016"}],"prism:pageRange":"110-115","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"September 2020","prism:doi":"10.1109/CFIS49607.2020.9238706","dc:description":"In this paper, we address the problem of finding the location of the camera based upon a query input RGB image for indoor navigation. This would be a difficult problem. Ever since the training data is gathered for the indoor positioning system, any type of modifications to the scene such as occlusions, illumination changes, or repetitive patterns can easily fool any positioning system. In this work, a tandem set of convolutional neural networks, have been leveraged to perform as the scene classifier. Moreover a scene RGB image is converted to its corresponding point cloud data through a GAN network. Finally, the position regression is performed over the point cloud input using a CNN structure. The proposed architecture has been compared with the related works and achieved a better performance in the sense that, 1) it simplifies the data generation, 2) it is more robust against small variations in the scene, and 3) the accuracy of the camera position, as well as its quaternion is remarkable.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/125269423","afid":"125269423","affilname":"AR/VR Solution Company","affiliation-city":"Tehran","affiliation-country":"Iran"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/122797799","afid":"122797799","affilname":"Iran Broadcasting University (IRIBU)","affiliation-city":"Tehran","affiliation-country":"Iran"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57211940470","authid":"57211940470","authname":"Ghofrani A.","surname":"Ghofrani","given-name":"Ali","initials":"A.","afid": [{"@_fa": "true", "$" :"122797799"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57220194394","authid":"57220194394","authname":"Mahdian R.","surname":"Mahdian","given-name":"Rahil","initials":"R.","afid": [{"@_fa": "true", "$" :"122797799"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57215564444","authid":"57215564444","authname":"Tabatabaie S.M.","surname":"Tabatabaie","given-name":"Seyed Mojtaba","initials":"S.M.","afid": [{"@_fa": "true", "$" :"125269423"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57219589339","authid":"57219589339","authname":"Tabasi S.M.","surname":"Tabasi","given-name":"Seyed Maziyar","initials":"S.M.","afid": [{"@_fa": "true", "$" :"125269423"}]}],"authkeywords":"Camera positioning | Generative network | Image to image translation | Indoor localization | Point cloud data","article-number":"9238706","source-id":"21101029741","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85096464055"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85096464055?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85096464055&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85096464055&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85096464055","dc:identifier":"SCOPUS_ID:85096464055","eid":"2-s2.0-85096464055","dc:title":"Quaternion neural networks for 3d sound source localization in reverberant environments","dc:creator":"Celsi M.R.","prism:publicationName":"IEEE International Workshop on Machine Learning for Signal Processing, MLSP","prism:issn":"21610363","prism:eIssn":"21610371","prism:isbn": [{"@_fa": "true", "$" :"9781728166629"}],"prism:volume":"2020-September","prism:pageRange":null,"prism:coverDate":"2020-09-01","prism:coverDisplayDate":"September 2020","prism:doi":"10.1109/MLSP49062.2020.9231809","dc:description":"Localization of sound sources in 3D sound fields is an extremely challenging task, especially when the environments are reverberant and involve multiple sources. In this work, we propose a deep neural network to analyze audio signals recorded by 3D microphones and localize sound sources in a spatial sound field. In particular, we consider first-order Ambisonics microphones to capture 3D acoustic signals and represent them by spherical harmonic decomposition in the quaternion domain. Moreover, to improve the localization performance, we use quaternion input features derived from the acoustic intensity, which is strictly related to the direction of arrival (DOA) of a sound source. The proposed network architecture involves both quaternion-valued convolutional and recurrent layers. Results show that the proposed method is able to exploit both the quaternion-valued representation of ambisonic signals and to improve the localization performance with respect to existing methods.","citedby-count":"9","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60032350","afid":"60032350","affilname":"Sapienza Università di Roma","affiliation-city":"Rome","affiliation-country":"Italy"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57220003605","authid":"57220003605","authname":"Celsi M.R.","surname":"Celsi","given-name":"Michela Ricciardi","initials":"M.R.","afid": [{"@_fa": "true", "$" :"60032350"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55772102700","authid":"55772102700","authname":"Scardapane S.","surname":"Scardapane","given-name":"Simone","initials":"S.","afid": [{"@_fa": "true", "$" :"60032350"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/36444807900","authid":"36444807900","authname":"Comminiello D.","surname":"Comminiello","given-name":"Danilo","initials":"D.","afid": [{"@_fa": "true", "$" :"60032350"}]}],"authkeywords":"3D audio | Convolutional recurrent neural networks | Hypercomplex-valued neural networks | Quaternion neural networks | Source localization","article-number":"9231809","source-id":"21100222921","fund-no":"RG11916B88E1942F","fund-sponsor":"Sapienza Università di Roma","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094096537"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094096537?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094096537&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85094096537&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85094096537","dc:identifier":"SCOPUS_ID:85094096537","eid":"2-s2.0-85094096537","dc:title":"Multi-Channel Chaotic System","dc:creator":"Voliansky R.","prism:publicationName":"2020 10th International Conference on Advanced Computer Information Technologies, ACIT 2020 - Proceedings","prism:isbn": [{"@_fa": "true", "$" :"9781728167602"}],"prism:pageRange":"196-199","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"September 2020","prism:doi":"10.1109/ACIT49673.2020.9209000","dc:description":"The paper deals with the construction and study of a multi-channel secured communication system based on a chaotic dynamic. Our design is based on the study of the piecewise linear time-delayed system. We offer to replace real numbers in the description of state variable and object parameters with complex and hyper-complex ones. Such an approach gives us the possibility to extend the system's dimension and design a multi-channel chaotic system. We suggest using this system as a multichannel chaotic transmitter which modulates some message with a random signal sequence. It is shown that usage of irrational function makes it possible to deform chaotic oscillations form and makes them different from known ones. We use the symmetry principle to construct a decryptor for the received messages. Some recommendations to design multi-channel chaotic communication system are given as generalization of the proposed approach.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60268787","afid":"60268787","affilname":"Ukrainian State University of Science and Technologies","affiliation-city":"Dnipro","affiliation-country":"Ukraine"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60102132","afid":"60102132","affilname":"Dniprovsk State Technical University","affiliation-city":"Kamianske","affiliation-country":"Ukraine"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6507010480","authid":"6507010480","authname":"Voliansky R.","surname":"Voliansky","given-name":"Roman","initials":"R.","afid": [{"@_fa": "true", "$" :"60102132"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57217113986","authid":"57217113986","authname":"Kluev O.","surname":"Kluev","given-name":"Oleg","initials":"O.","afid": [{"@_fa": "true", "$" :"60102132"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57211997772","authid":"57211997772","authname":"Shramko I.","surname":"Shramko","given-name":"Iurii","initials":"I.","afid": [{"@_fa": "true", "$" :"60102132"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57188644610","authid":"57188644610","authname":"Kuznetsov V.","surname":"Kuznetsov","given-name":"Vitaliy","initials":"V.","afid": [{"@_fa": "true", "$" :"60268787"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/57202236201","authid":"57202236201","authname":"Volianska N.","surname":"Volianska","given-name":"Nina","initials":"N.","afid": [{"@_fa": "true", "$" :"60102132"}]}],"authkeywords":"chaotic dynamic | first-order time-delayed dynamical system | nonlinear system | piece-wise linear approximation | secured communication","article-number":"9209000","source-id":"21101023802","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091752138"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091752138?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091752138&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85091752138&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85091752138","dc:identifier":"SCOPUS_ID:85091752138","eid":"2-s2.0-85091752138","dc:title":"Compressing deep-quaternion neural networks with targeted regularisation","dc:creator":"Vecchi R.","prism:publicationName":"CAAI Transactions on Intelligence Technology","prism:issn":"24686557","prism:eIssn":"24682322","prism:volume":"5","prism:issueIdentifier":"3","prism:pageRange":"172-176","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"1 September 2020","prism:doi":"10.1049/trit.2020.0020","dc:description":"In recent years, hyper-complex deep networks (such as complex-valued and quaternion-valued neural networks - QVNNs) have received a renewed interest in the literature. They find applications in multiple fields, ranging from image reconstruction to 3D audio processing. Similar to their real-valued counterparts, quaternion neural networks require custom regularisation strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations, there is the need of designing sufficiently compact networks, with few weights and neurons. However, the problem of regularising and/or sparsifying QVNNs has not been properly addressed in the literature as of now. In this study, the authors show how to address both problems by designing targeted regularisation strategies, which can minimise the number of connections and neurons of the network during training. To this end, they investigate two extensions of l1and structured regularisations to the quaternion domain. In the authors' experimental evaluation, they show that these tailored strategies significantly outperform classical (realvalued) regularisation approaches, resulting in small networks especially suitable for low-power and real-time applications.","citedby-count":"12","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60032350","afid":"60032350","affilname":"Sapienza Università di Roma","affiliation-city":"Rome","affiliation-country":"Italy"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57219222613","authid":"57219222613","authname":"Vecchi R.","surname":"Vecchi","given-name":"Riccardo","initials":"R.","afid": [{"@_fa": "true", "$" :"60032350"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55772102700","authid":"55772102700","authname":"Scardapane S.","surname":"Scardapane","given-name":"Simone","initials":"S.","afid": [{"@_fa": "true", "$" :"60032350"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/36444807900","authid":"36444807900","authname":"Comminiello D.","surname":"Comminiello","given-name":"Danilo","initials":"D.","afid": [{"@_fa": "true", "$" :"60032350"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/7005621339","authid":"7005621339","authname":"Uncini A.","surname":"Uncini","given-name":"Aurelio","initials":"A.","afid": [{"@_fa": "true", "$" :"60032350"}]}],"source-id":"21100970248","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"},{"$" :"repository"},{"$" :"repositoryvor"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089728752"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089728752?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089728752&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85089728752&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S0016003220305391"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85089728752","dc:identifier":"SCOPUS_ID:85089728752","eid":"2-s2.0-85089728752","dc:title":"Tessarine signal processing under the T-properness condition","dc:creator":"Navarro-Moreno J.","prism:publicationName":"Journal of the Franklin Institute","prism:issn":"00160032","prism:volume":"357","prism:issueIdentifier":"14","prism:pageRange":"10100-10126","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"September 2020","prism:doi":"10.1016/j.jfranklin.2020.08.002","pii":"S0016003220305391","dc:description":"The paper analyzes the processing of 4D commutative hypercomplex or tessarine signals under properness conditions. Firstly, the concept of T-properness is introduced and a procedure to test experimentally whether a tessarine random signal is proper or not is proposed. Then, for the class of T-proper signals, the linear minimum mean square error estimation problem is addressed. In this regard, it should be highlighted that although the tessarine algebra is not a Hilbert space, a metric which guarantees the existence and unicity of the optimal estimator is defined. Moreover, the equivalence, under T-properness conditions, between the optimal estimator based on a tessarine widely linear processing and the one based on a tessarine strictly linear (TSL) processing is also shown, attaining thus a notable reduction in computational burden. Finally, two T-proper models, a TSL state-space model and a TSL stationary model, from which the optimal estimator can be recursively obtained are considered. In both cases, simulated examples are developed where the superiority of TSL processing over the counterparts in the quaternion domain is exhibited.","citedby-count":"10","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60014586","afid":"60014586","affilname":"Universidad de Jaén","affiliation-city":"Jaen","affiliation-country":"Spain"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6602299200","authid":"6602299200","authname":"Navarro-Moreno J.","surname":"Navarro-Moreno","given-name":"Jesús","initials":"J.","afid": [{"@_fa": "true", "$" :"60014586"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6506232550","authid":"6506232550","authname":"Fernández-Alcalá R.M.","surname":"Fernández-Alcalá","given-name":"Rosa María","initials":"R.M.","afid": [{"@_fa": "true", "$" :"60014586"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56589183200","authid":"56589183200","authname":"Jiménez-López J.D.","surname":"Jiménez-López","given-name":"José Domingo","initials":"J.D.","afid": [{"@_fa": "true", "$" :"60014586"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/6701415559","authid":"6701415559","authname":"Ruiz-Molina J.C.","surname":"Ruiz-Molina","given-name":"Juan Carlos","initials":"J.C.","afid": [{"@_fa": "true", "$" :"60014586"}]}],"source-id":"27959","fund-acr":"FEDER","fund-no":"1256911","fund-sponsor":"Federación Española de Enfermedades Raras","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089541722"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089541722?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089541722&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85089541722&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S0165168420301626"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85089541722","dc:identifier":"SCOPUS_ID:85089541722","eid":"2-s2.0-85089541722","dc:title":"Design of quaternion-valued second-order Volterra adaptive filters for nonlinear 3-D and 4-D signals","dc:creator":"Mengüç E.C.","prism:publicationName":"Signal Processing","prism:issn":"01651684","prism:volume":"174","prism:pageRange":null,"prism:coverDate":"2020-09-01","prism:coverDisplayDate":"September 2020","prism:doi":"10.1016/j.sigpro.2020.107619","pii":"S0165168420301626","dc:description":"In this paper, the quaternion-valued second-order Volterra adaptive filters (QSOVAFs) are designed for the processing of nonlinear three-dimensional (3-D) and four-dimensional (4-D) signals. In the proposed frameworks, the structure of the strictly nonlinear (SNL), semi-widely nonlinear (SWNL), and widely nonlinear (WNL) QSOVAFs are primarily constructed in the quaternion domain. Then, their loss functions defined by the instantaneous error signals are minimized in the quaternion domain by using the recent generalized Hamilton-real (GHR) calculus. Thus, novel weight update equations are obtained for training the proposed SNL-QSOVAF, SWNL-QSOVAF, and WNL-QSOVAF. Furthermore, the stability bounds for each quaternion-valued kernel functions of them are derived from the convergence in the mean analysis. The comprehensive simulations on the nonlinear system identification and one-step-ahead prediction experiments support that the proposed SWNL-QSOVAF and WNL-QSOVAF can be effectively used in the processing of nonlinear noncircular quaternion signals, whereas that their SNL version can produce optimal results for nonlinear circular quaternion signals.","citedby-count":"12","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60004366","afid":"60004366","affilname":"Nigde Omer Halisdemir University","affiliation-city":"Niğde","affiliation-country":"Turkey"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/43261698200","authid":"43261698200","orcid":"0000-0002-0619-549X","authname":"Mengüç E.C.","surname":"Mengüç","given-name":"Engin Cemal","initials":"E.C.","afid": [{"@_fa": "true", "$" :"60004366"}]}],"authkeywords":"Circular and noncircular quaternion signals | Quaternion-valued second-order volterra adaptive filter | Semi-widely nonlinear | Strictly nonlinear | Widely nonlinear","article-number":"107619","source-id":"25548","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85088036357"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85088036357?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85088036357&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85088036357&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85088036357","dc:identifier":"SCOPUS_ID:85088036357","eid":"2-s2.0-85088036357","dc:title":"Color image analysis of quaternion discrete radial Krawtchouk moments","dc:creator":"Amakdouf H.","prism:publicationName":"Multimedia Tools and Applications","prism:issn":"13807501","prism:eIssn":"15737721","prism:volume":"79","prism:issueIdentifier":"35-36","prism:pageRange":"26571-26586","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"1 September 2020","prism:doi":"10.1007/s11042-020-09120-0","dc:description":"In this work, we suggest a new set of quaternion discrete radial Krawtchouk moments (QDRKMs) for color image reconstruction and classification. These new discrete moments are represented over a disk by using discrete orthogonal radial Krawtchouk moments. The use of Quaternion discrete moments for color image eliminates the discretization errors produced when the Quaternion continuous moments are used. Furthermore, this approach is suggested for highly accurate calculation of QDRKMs in polar coordinates where the kernel is exactly calculated by over circular color pixels. The translation, scaling, and rotation (TSR) invariances for QDRKMs are proved. Theoretical analysis and numerical experiments investigation were shown in terms of the performance description of TSR invariances, classification and robustness to different noises of the QDRKMs compared with continuous quaternion Legendre–Fourier moments using COIL 100 database.","citedby-count":"15","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60017021","afid":"60017021","affilname":"Université Sidi Mohamed Ben Abdellah","affiliation-city":"Fez","affiliation-country":"Morocco"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60012964","afid":"60012964","affilname":"Faculté des Sciences Dhar El Mahraz, Université Sidi Mohamed Ben Abdellah","affiliation-city":"Fez","affiliation-country":"Morocco"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57201499521","authid":"57201499521","authname":"Amakdouf H.","surname":"Amakdouf","given-name":"Hicham","initials":"H.","afid": [{"@_fa": "true", "$" :"60012964"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56252467100","authid":"56252467100","authname":"Zouhri A.","surname":"Zouhri","given-name":"Amal","initials":"A.","afid": [{"@_fa": "true", "$" :"60012964"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56426325300","authid":"56426325300","authname":"EL Mallahi M.","surname":"EL Mallahi","given-name":"Mostafa","initials":"M.","afid": [{"@_fa": "true", "$" :"60012964"},{"@_fa": "true", "$" :"60017021"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/12144280900","authid":"12144280900","authname":"Qjidaa H.","surname":"Qjidaa","given-name":"Hassan","initials":"H.","afid": [{"@_fa": "true", "$" :"60012964"}]}],"authkeywords":"Classification and recognition of color image | Color image analysis; quaternion | Discrete radial Krawtchouk moments","source-id":"25627","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85087960893"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85087960893?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85087960893&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85087960893&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85087960893","dc:identifier":"SCOPUS_ID:85087960893","eid":"2-s2.0-85087960893","dc:title":"Disturbance observer enhanced variable gain controller for robot teleoperation with motion capture using wearable armbands","dc:creator":"Huang D.","prism:publicationName":"Autonomous Robots","prism:issn":"09295593","prism:eIssn":"15737527","prism:volume":"44","prism:issueIdentifier":"7","prism:pageRange":"1217-1231","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"1 September 2020","prism:doi":"10.1007/s10514-020-09928-7","dc:description":"Disturbance observer (DOB) based controller performs well in estimating and compensating for perturbation when the external or internal unknown disturbance is slowly time varying. However, to some extent, robot manipulators usually work in complex environment with high-frequency disturbance. Thereby, to enhance tracking performance in a teleoperation system, only traditional DOB technique is insufficient. In this paper, for the purpose of constructing a feasible teleoperation scheme, we develop a novel controller that contains a variable gain scheme to deal with fast-time varying perturbation, whose gain is adjusted linearly according to human surface electromyographic signals collected from Myo wearable armband. In addition, for tracking the motion of operator’s arm, we derive five-joint-angle data of a moving human arm through two groups of quaternions generated from the armbands. Besides, the radial basis function neural networks and the disturbance observer-based control (DOBC) approaches are fused together into the proposed controller to compensate the unknown dynamics uncertainties of the slave robot as well as environmental perturbation. Experiments and simulations are conducted to demonstrated the effectiveness of the proposed strategy.","citedby-count":"11","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025475","afid":"60025475","affilname":"University of Portsmouth","affiliation-city":"Portsmouth","affiliation-country":"United Kingdom"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60024542","afid":"60024542","affilname":"South China University of Technology","affiliation-city":"Guangzhou","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019611","afid":"60019611","affilname":"University of the West of England","affiliation-city":"Bristol","affiliation-country":"United Kingdom"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57218113591","authid":"57218113591","authname":"Huang D.","surname":"Huang","given-name":"Darong","initials":"D.","afid": [{"@_fa": "true", "$" :"60024542"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55671767000","authid":"55671767000","authname":"Yang C.","surname":"Yang","given-name":"Chenguang","initials":"C.","afid": [{"@_fa": "true", "$" :"60019611"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/25655056800","authid":"25655056800","authname":"Ju Z.","surname":"Ju","given-name":"Zhaojie","initials":"Z.","afid": [{"@_fa": "true", "$" :"60025475"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/36877968100","authid":"36877968100","authname":"Dai S.L.","surname":"Dai","given-name":"Shi Lu","initials":"S.L.","afid": [{"@_fa": "true", "$" :"60024542"}]}],"authkeywords":"Disturbance observer | Motion capture | Radial basis function neural networks | Teleoperation | Variable gain control","source-id":"18016","fund-acr":"EPSRC","fund-no":"EP/S001913","fund-sponsor":"Engineering and Physical Sciences Research Council","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherhybridgold"},{"$" :"repository"},{"$" :"repositoryvor"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Hybrid Gold"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080025987"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080025987?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080025987&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85080025987&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85080025987","dc:identifier":"SCOPUS_ID:85080025987","eid":"2-s2.0-85080025987","dc:title":"Uncertainty Principles for the Two-Sided Quaternion Linear Canonical Transform","dc:creator":"Zhu X.","prism:publicationName":"Circuits, Systems, and Signal Processing","prism:issn":"0278081X","prism:eIssn":"15315878","prism:volume":"39","prism:issueIdentifier":"9","prism:pageRange":"4436-4458","prism:coverDate":"2020-09-01","prism:coverDisplayDate":"1 September 2020","prism:doi":"10.1007/s00034-020-01376-z","dc:description":"The quaternion linear canonical transform (QLCT), as a generalized form of the quaternion Fourier transform, is a powerful analyzing tool in image and signal processing. In this paper, we propose three different forms of uncertainty principles for the two-sided QLCT, which include Hardy’s uncertainty principle, Beurling’s uncertainty principle and Donoho–Stark’s uncertainty principle. These consequences actually describe the quantitative relationships of the quaternion-valued signal in arbitrary two different QLCT domains, which have many applications in signal recovery and color image analysis. In addition, in order to analyze the non-stationary signal and time-varying system, we present Lieb’s uncertainty principle for the two-sided short-time quaternion linear canonical transform (SQLCT) based on the Hausdorff–Young inequality. By adding the nonzero quaternion-valued window function, the two-sided SQLCT has a great significant application in the study of signal local frequency spectrum. Finally, we also give a lower bound for the essential support of the two-sided SQLCT.","citedby-count":"14","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60022381","afid":"60022381","affilname":"Beijing Jiaotong University","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57215203997","authid":"57215203997","authname":"Zhu X.","surname":"Zhu","given-name":"Xiaoyu","initials":"X.","afid": [{"@_fa": "true", "$" :"60022381"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/51565362500","authid":"51565362500","orcid":"0000-0002-7909-0517","authname":"Zheng S.","surname":"Zheng","given-name":"Shenzhou","initials":"S.","afid": [{"@_fa": "true", "$" :"60022381"}]}],"authkeywords":"Quaternion Fourier transform (QFT) | Quaternion linear canonical transform (QLCT) | Short-time quaternion linear canonical transform (SQLCT) | Uncertainty principle","source-id":"18780","fund-acr":"NSFC","fund-no":"11371050","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85102127955"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85102127955?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85102127955&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85102127955&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85102127955","dc:identifier":"SCOPUS_ID:85102127955","eid":"2-s2.0-85102127955","dc:title":"A Visual saliency based railway intrusion detection method by uav remote sensing image","dc:creator":"Guan L.","prism:publicationName":"Proceedings of 2020 International Conference on Sensing, Diagnostics, Prognostics, and Control, SDPC 2020","prism:isbn": [{"@_fa": "true", "$" :"9781728170503"}],"prism:pageRange":"291-295","prism:coverDate":"2020-08-05","prism:coverDisplayDate":"5 August 2020","prism:doi":"10.1109/SDPC49476.2020.9353141","dc:description":"Railway intrusion is an important factor affecting the safety of train operations. The detection of railway intrusions based on the remote sensing image of low altitude UAV is a new and promising technology. This paper presents an intrusion detection method of UAV remote sensing image based on visual saliency. This method consists of four stages. First, the positions of the rails are fitted by differential excitation and Hough transform method and then the region of interest is extracted. Next, perspective transformation is applied to obtain the birds-eye view image. Then, motivated by the human visual attention mechanism, the salient areas are extracted based on the Hypercomplex Fourier Transform visual saliency detection and are considered as intrusions. Finally, the threshold method and morphological processing are used to extract the areas of intrusions. Experiments show that the proposed method can effectively detect and segment the areas of intrusion.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60022381","afid":"60022381","affilname":"Beijing Jiaotong University","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57200193234","authid":"57200193234","authname":"Guan L.","surname":"Guan","given-name":"Ling","initials":"L.","afid": [{"@_fa": "true", "$" :"60022381"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/47962147500","authid":"47962147500","authname":"Li X.","surname":"Li","given-name":"Xiaofeng","initials":"X.","afid": [{"@_fa": "true", "$" :"60022381"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57200192744","authid":"57200192744","authname":"Yang H.","surname":"Yang","given-name":"Han","initials":"H.","afid": [{"@_fa": "true", "$" :"60022381"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/7201396387","authid":"7201396387","authname":"Jia L.","surname":"Jia","given-name":"Limin","initials":"L.","afid": [{"@_fa": "true", "$" :"60022381"}]}],"authkeywords":"Differential excitation | Hypercomplex Fourier Transform | railway intrusion detection | UAV remote sensing image | visual saliency","article-number":"9353141","source-id":"21101039504","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85098174998"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85098174998?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098174998&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85098174998&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85098174998","dc:identifier":"SCOPUS_ID:85098174998","eid":"2-s2.0-85098174998","dc:title":"Monocular visual-inertial odometry method based on IMU pre-integrated closed solution","dc:creator":"Xu X.","prism:publicationName":"Zhongguo Guanxing Jishu Xuebao/Journal of Chinese Inertial Technology","prism:issn":"10056734","prism:volume":"28","prism:issueIdentifier":"4","prism:pageRange":"440-447","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.13695/j.cnki.12-1222/o3.2020.04.004","dc:description":"The visual-inertial odometry method based on the extended Kalman filter is widely used in practical environments due to its high real-time performance and high accuracy. Aiming at how to quickly and accurately process IMU data between two frames of images, an algorithm based on IMU pre-integration closed solution is proposed. Compared with the traditional optimization-based visual inertial odometry method which uses discrete quaternion integration to simplify the required pre-integration value under the piecewise constant acceleration approximation, the IMU pre-integration closed solution algorithm solves the analytical solution within the IMU time period, and is applied to the frame work of the multi-state constraint Kalman filter (MSCKF) visual inertial odometry method to improve the accuracy of system positioning. Aiming at the numerical stability problem of the MSCKF algorithm observation equation, an inverse depth parameterization method is proposed which eliminates the singularities of the observed values of the system using the MSCKF algorithm when the z-axis depth value of the spatial point coordinate approaches zero. Therefore the robustness of the system is effectively increased. The experimental results on the six flight sequences of the EuRoc dataset show that the improved algorithm has less drift and root-mean-square error (RMSE) than the traditional MSCKF visual inertial odomety method. The root-mean-square error decreases by about 36.5%, and the positioning accuracy is effectively improved.","citedby-count":"5","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60005244","afid":"60005244","affilname":"Southeast University","affiliation-city":"Nanjing","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/8356045200","authid":"8356045200","authname":"Xu X.","surname":"Xu","given-name":"Xiaosu","initials":"X.","afid": [{"@_fa": "true", "$" :"60005244"},{"@_fa": "true", "$" :"60005244"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57221108945","authid":"57221108945","authname":"Wu X.","surname":"Wu","given-name":"Xian","initials":"X.","afid": [{"@_fa": "true", "$" :"60005244"},{"@_fa": "true", "$" :"60005244"}]}],"authkeywords":"IMU pre-integration closed solution | Inverse depth parameterization | Monocular visual-inertial odometry | Multi-state constraint Kalman filter","source-id":"19700186905","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094148710"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094148710?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094148710&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85094148710&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85094148710","dc:identifier":"SCOPUS_ID:85094148710","eid":"2-s2.0-85094148710","dc:title":"Orienting Novel 3D Objects Using Self-Supervised Learning of Rotation Transforms","dc:creator":"Devgon S.","prism:publicationName":"IEEE International Conference on Automation Science and Engineering","prism:issn":"21618070","prism:eIssn":"21618089","prism:isbn": [{"@_fa": "true", "$" :"9781728169040"}],"prism:volume":"2020-August","prism:pageRange":"1453-1460","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.1109/CASE48305.2020.9217018","dc:description":"Orienting objects is a critical component in the automation of many packing and assembly tasks. We present an algorithm to orient novel objects given a depth image of the object in its current and desired orientation. We formulate a self-supervised objective for this problem and train a deep neural network to estimate the 3D rotation as parameterized by a quaternion, between these current and desired depth images. We then use the trained network in a proportional controller to re-orient objects based on the estimated rotation between the two depth images. Results suggest that in simulation we can rotate unseen objects with unknown geometries by up to 30° with a median angle error of 1.47° over 100 random initial/desired orientations each for 22 novel objects. Experiments on physical objects suggest that the controller can achieve a median angle error of 4.2° over 10 random initial/desired orientations each for 5 objects.","citedby-count":"6","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025038","afid":"60025038","affilname":"University of California, Berkeley","affiliation-city":"Berkeley","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57219592699","authid":"57219592699","authname":"Devgon S.","surname":"Devgon","given-name":"Shivin","initials":"S.","afid": [{"@_fa": "true", "$" :"60025038"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6505971468","authid":"6505971468","authname":"Ichnowski J.","surname":"Ichnowski","given-name":"Jeffrey","initials":"J.","afid": [{"@_fa": "true", "$" :"60025038"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57210794787","authid":"57210794787","authname":"Balakrishna A.","surname":"Balakrishna","given-name":"Ashwin","initials":"A.","afid": [{"@_fa": "true", "$" :"60025038"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57219441121","authid":"57219441121","authname":"Zhang H.","surname":"Zhang","given-name":"Harry","initials":"H.","afid": [{"@_fa": "true", "$" :"60025038"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/35453491100","authid":"35453491100","authname":"Goldberg K.","surname":"Goldberg","given-name":"Ken","initials":"K.","afid": [{"@_fa": "true", "$" :"60025038"}]}],"article-number":"9217018","source-id":"20500195424","fund-acr":"NSF","fund-no":"1734633","fund-sponsor":"National Science Foundation","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094134472"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85094134472?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85094134472&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85094134472&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85094134472","dc:identifier":"SCOPUS_ID:85094134472","eid":"2-s2.0-85094134472","dc:title":"Functional replicas of proprietary three-axis attitude sensors via LSTM neural networks","dc:creator":"Fu H.","prism:publicationName":"CCTA 2020 - 4th IEEE Conference on Control Technology and Applications","prism:isbn": [{"@_fa": "true", "$" :"9781728171401"}],"prism:pageRange":"70-75","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.1109/CCTA41146.2020.9206312","dc:description":"In this paper, machine-learning-based tools are utilized to learn a model of the functionality of a commercial chip with built-in signal processing and sensor fusion algorithms. Specifically, a fully integrated 9-axis Inertial Measurement Unit (IMU) with embedded algorithms providing the three-axis attitude and corresponding quaternion by fusing all the sensors is considered. Traditionally, extended Kalman filters are used for fusing IMU sensors; however, subtle algorithmic fixes (e.g., magnetic and angular alignment calibration for all sensors, Kalman filter tuning, temperature drift compensation, dynamic magnetic effects) need to be deployed to attain precise attitude (especially heading). A recurrent neural network (RNN) was trained using the chip to substitute for the built-in algorithms of the IMU Chip to output the approximate attitude given the 9-axis sensor data. We show the efficacy of our approach by mounting two IMUs on a board and utilize one IMU, which has its own internal algorithms, to train a machine learning system to fuse the raw data from the sensors on the second IMU to generate comparable accuracy (and in some cases, even outperform the original IMU).","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60108318","afid":"60108318","affilname":"NYU Tandon School of Engineering","affiliation-city":"New York","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57220990686","authid":"57220990686","authname":"Fu H.","surname":"Fu","given-name":"Hao","initials":"H.","afid": [{"@_fa": "true", "$" :"60108318"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006328196","authid":"7006328196","authname":"Krishnamurthy P.","surname":"Krishnamurthy","given-name":"Prashanth","initials":"P.","afid": [{"@_fa": "true", "$" :"60108318"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/55663722200","authid":"55663722200","authname":"Khorrami F.","surname":"Khorrami","given-name":"Farshad","initials":"F.","afid": [{"@_fa": "true", "$" :"60108318"}]}],"article-number":"9206312","source-id":"21101023804","fund-acr":"ONR","fund-no":"N00014-18-1-2672","fund-sponsor":"Office of Naval Research","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85093858731"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85093858731?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85093858731&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85093858731&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85093858731","dc:identifier":"SCOPUS_ID:85093858731","eid":"2-s2.0-85093858731","dc:title":"Facial Micro-Expression Recognition Using Quaternion-Based Sparse Representation","dc:creator":"Yang H.","prism:publicationName":"Proceedings - International Conference on Computer Communications and Networks, ICCCN","prism:issn":"10952055","prism:isbn": [{"@_fa": "true", "$" :"9781728166070"}],"prism:volume":"2020-August","prism:pageRange":null,"prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.1109/ICCCN49398.2020.9209630","dc:description":"Facial micro-expressions are characterized by their extremely short duration and low intensity, can provide an important basis for judging people's emotions, and therefore have promising potential applications in numerous fields. This paper puts forward a novel method for recognizing micro-expressions by using a quaternion-based sparse representation (QSR) model combined with the integral projection of difference energy image (IP-DEI) to extract features from color images of human faces. Using the quaternion model to jointly process color images can obtain greater feature information than gray or RGB images, and the QSR model helps reduce feature dimensions and enables greater discriminative representation. First, each micro-expression sample undergoes IP-DEI to allow the features of all samples to be displayed in the form of a quaternion matrix dot Y. Next we find overcomplete dictionary matrix dot D and sparse coefficient matrix X such that dot Y = dot D dot X in ideal scenarios, and consider dot X to be the features contained within the micro-expression samples. Finally, we apply our method to the SMIC, CAMSE I and CAMSE II micro-expression databases while using SVM as classifier. The results of the experiment demonstrate that our method outperformed the currently most advanced methods in terms of micro-expression recognition accuracy.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60013614","afid":"60013614","affilname":"Hangzhou Dianzi University","affiliation-city":"Hangzhou","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60002836","afid":"60002836","affilname":"Hefei University of Technology","affiliation-city":"Hefei","affiliation-country":"China"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57882132100","authid":"57882132100","authname":"Yang H.","surname":"Yang","given-name":"Hang","initials":"H.","afid": [{"@_fa": "true", "$" :"60002836"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/9942016100","authid":"9942016100","authname":"Wang Q.","surname":"Wang","given-name":"Qingshan","initials":"Q.","afid": [{"@_fa": "true", "$" :"60002836"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57194435359","authid":"57194435359","authname":"Wang Q.","surname":"Wang","given-name":"Qi","initials":"Q.","afid": [{"@_fa": "true", "$" :"60002836"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/55574230507","authid":"55574230507","authname":"Liu P.","surname":"Liu","given-name":"Peng","initials":"P.","afid": [{"@_fa": "true", "$" :"60013614"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/57199043189","authid":"57199043189","authname":"Huang W.","surname":"Huang","given-name":"Wei","initials":"W.","afid": [{"@_fa": "true", "$" :"60002836"}]}],"authkeywords":"Difference energy image | Integral projection | Micro-expression recognition | Quaternion matrix | Sparse presentation","article-number":"9209630","source-id":"145205","fund-acr":"NSFC","fund-no":"61401144","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089472285"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85089472285?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85089472285&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85089472285&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85089472285","dc:identifier":"SCOPUS_ID:85089472285","eid":"2-s2.0-85089472285","dc:title":"3D Object Pose Estimation Using Multi-Objective Quaternion Learning","dc:creator":"Papaioannidis C.","prism:publicationName":"IEEE Transactions on Circuits and Systems for Video Technology","prism:issn":"10518215","prism:eIssn":"15582205","prism:volume":"30","prism:issueIdentifier":"8","prism:pageRange":"2683-2693","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.1109/TCSVT.2019.2929600","dc:description":"In this paper, a framework is proposed for object recognition and pose estimation from color images using convolutional neural networks (CNNs). 3D object pose estimation along with object recognition has numerous applications, such as robot positioning versus a target object and robotic object grasping. Previous methods addressing this problem relied on both color and depth (RGB-D) images to learn low-dimensional viewpoint descriptors for object pose retrieval. In the proposed method, a novel quaternion-based multi-objective loss function is used, which combines manifold learning and regression to learn 3D pose descriptors and direct 3D object pose estimation, using only color (RGB) images. The 3D object pose can then be obtained either by using the learned descriptors in the nearest neighbor (NN) search or by direct neural network regression. An extensive experimental evaluation has proven that such descriptors provide greater pose estimation accuracy than the state-of-the-art methods. In addition, the learned 3D pose descriptors are almost object-independent and, thus, generalizable to unseen objects. Finally, when the object identity is not of interest, the 3D object pose can be regressed directly from the network, by overriding the NN search, thus significantly reducing the object pose inference time.","citedby-count":"10","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60015331","afid":"60015331","affilname":"Aristotle University of Thessaloniki","affiliation-city":"Thessaloniki","affiliation-country":"Greece"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57212479498","authid":"57212479498","orcid":"0000-0002-3839-4514","authname":"Papaioannidis C.","surname":"Papaioannidis","given-name":"Christos","initials":"C.","afid": [{"@_fa": "true", "$" :"60015331"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/35569279700","authid":"35569279700","authname":"Pitas I.","surname":"Pitas","given-name":"Ioannis","initials":"I.","afid": [{"@_fa": "true", "$" :"60015331"}]}],"authkeywords":"3D object pose estimation | convolutional neural networks (CNNs) | multi-objective learning | object recognition | quaternion","article-number":"8765585","source-id":"26027","fund-acr":"H2020","fund-no":"731667","fund-sponsor":"Horizon 2020 Framework Programme","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85086826555"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85086826555?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086826555&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85086826555&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S1746809420301798"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85086826555","dc:identifier":"SCOPUS_ID:85086826555","eid":"2-s2.0-85086826555","dc:title":"A wireless, real-time respiratory effort and body position monitoring system for sleep","dc:creator":"Hernandez J.E.","prism:publicationName":"Biomedical Signal Processing and Control","prism:issn":"17468094","prism:eIssn":"17468108","prism:volume":"61","prism:pageRange":null,"prism:coverDate":"2020-08-01","prism:coverDisplayDate":"August 2020","prism:doi":"10.1016/j.bspc.2020.102023","pii":"S1746809420301798","dc:description":"The number of people suffering from a sleep disorder is growing considerably, with obstructive sleep apnea syndrome (OSAS) being the most common one. Unfortunately, it is estimated that at least 75% of OSAS cases are currently undiagnosed and without treatment, mainly due to the lack of availability and high costs of the polysomnography (PSG) test required. OSAS is known to increase the risk for many severe health complications such as hypertension and neurodegenerative conditions, to mention a few, making early diagnosis important. The monitoring of respiratory effort (RE) and body position (BP) during sleep is essential for OSAS detection. Compared to existing commercial systems for respiratory effort monitoring using piezoelectric sensors wired to a central processing system, we present an embedded system solution that is wireless, runs standalone for up to 12 h, and implements on board an extended Kalman filter (EKF) for performing data fusion. An inertial measurement unit (3D acceleration, angular rate, and orientation sensors) is used for real-time measurement of both the respiratory effort and the body position. The experimental tests, when we compare the collected data with a reference signal, acquired by a commercial belt, indicate a general average Pearson correlation coefficient of ρ=0.963 with results ranging from 0.928 to 0.981, improving over previous works. Low-cost technologies, such as the one presented here, can help to reduce the number of undiagnosed cases of OSAS, and sleep disorders in general, and improve the health of those patients.","citedby-count":"10","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60010365","afid":"60010365","affilname":"The University of British Columbia","affiliation-city":"Vancouver","affiliation-country":"Canada"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57201704215","authid":"57201704215","authname":"Hernandez J.E.","surname":"Hernandez","given-name":"Joel Ezequiel","initials":"J.E.","afid": [{"@_fa": "true", "$" :"60010365"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6603679542","authid":"6603679542","authname":"Cretu E.","surname":"Cretu","given-name":"Edmond","initials":"E.","afid": [{"@_fa": "true", "$" :"60010365"}]}],"authkeywords":"Body position | Data fusion | Inertial measurement units | Respiratory effort | Sleep monitoring","article-number":"102023","source-id":"4700152237","fund-acr":"NSERC","fund-no":"STPGP 493908","fund-sponsor":"Natural Sciences and Engineering Research Council of Canada","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080988185"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080988185?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080988185&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85080988185&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85080988185","dc:identifier":"SCOPUS_ID:85080988185","eid":"2-s2.0-85080988185","dc:title":"Correction to: Adopting Quaternion Wavelet Transform to Fuse Multi-Modal Medical Images (Journal of Medical and Biological Engineering, (2017), 37, 2, (230-239), 10.1007/s40846-016-0200-6)","dc:creator":"Geng P.","prism:publicationName":"Journal of Medical and Biological Engineering","prism:issn":"16090985","prism:eIssn":"21994757","prism:volume":"40","prism:issueIdentifier":"4","prism:pageRange":null,"prism:coverDate":"2020-08-01","prism:coverDisplayDate":"1 August 2020","prism:doi":"10.1007/s40846-018-0407-9","dc:description":"The article “Adopting Quaternion Wavelet Transform to Fuse Multi-Modal Medical Images”, written by Peng Geng, Xiuming Sun, Jianhua Liu was originally published Online First without open access. After publication in volume [37], issue [2], page [230–239] the author decided to opt for Open Choice and to make the article an open access publication. Therefore, the copyright of the article has been changed to © The Author(s) [2018] and the article is forthwith distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60268352","afid":"60268352","affilname":"Zhangjiakou University","affiliation-city":"Zhangjiakou","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60032274","afid":"60032274","affilname":"Shijiazhuang Tiedao University","affiliation-city":"Shijiazhuang","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"er","subtypeDescription":"Erratum","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/35843974800","authid":"35843974800","authname":"Geng P.","surname":"Geng","given-name":"Peng","initials":"P.","afid": [{"@_fa": "true", "$" :"60032274"},{"@_fa": "true", "$" :"60032274"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57221082729","authid":"57221082729","authname":"Sun X.","surname":"Sun","given-name":"Xiuming","initials":"X.","afid": [{"@_fa": "true", "$" :"60268352"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/55851920200","authid":"55851920200","authname":"Liu J.","surname":"Liu","given-name":"Jianhua","initials":"J.","afid": [{"@_fa": "true", "$" :"60032274"}]}],"source-id":"15949","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherhybridgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Hybrid Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85079725252"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85079725252?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85079725252&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85079725252&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85079725252","dc:identifier":"SCOPUS_ID:85079725252","eid":"2-s2.0-85079725252","dc:title":"Robust hand gesture recognition system based on a new set of quaternion Tchebichef moment invariants","dc:creator":"Elouariachi I.","prism:publicationName":"Pattern Analysis and Applications","prism:issn":"14337541","prism:eIssn":"1433755X","prism:volume":"23","prism:issueIdentifier":"3","prism:pageRange":"1337-1353","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"1 August 2020","prism:doi":"10.1007/s10044-020-00866-9","dc:description":"Hand gesture recognition is a challenging task due to the complexity of hand movements and to the variety among the same gesture performed by distinct subjects. Recent technologies, such as Kinect sensor, provide new opportunities, allowing to capture both RGB and depth (RGB-D) images, which offer high discriminant information for efficient hand gesture recognition. In the aspect of feature extraction, the traditional methods process the RGB and depth information independently. In this paper, we propose a robust hand gesture recognition system based on a new feature extraction method, fusing RGB images and depth information simultaneously, by using the quaternion algebra that provide a more robust and holistical representation. In fact, we introduce, for the first time, a novel type of feature extraction method, named quaternion Tchebichef moment invariants. The novelty of the proposed method in this paper lies in the direct derivation of invariants from their orthogonal moments, based on the algebraic properties of the discrete Tchebichef polynomials. The proposed approach based on quaternion algebra is suggested to process the four components holistically, for a robust and efficient hand gesture recognition system. The obtained experimental and theoretical results demonstrate that the present approach is very effective for addressing the problem of hand gesture recognition and have proved its robustness against geometrical distortion, noisy conditions and complex background compared to the state of the art, indicating that it could be highly useful for many computer vision applications.","citedby-count":"14","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60017021","afid":"60017021","affilname":"Université Sidi Mohamed Ben Abdellah","affiliation-city":"Fez","affiliation-country":"Morocco"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57207617462","authid":"57207617462","orcid":"0000-0002-9386-3651","authname":"Elouariachi I.","surname":"Elouariachi","given-name":"Ilham","initials":"I.","afid": [{"@_fa": "true", "$" :"60017021"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57193524950","authid":"57193524950","authname":"Benouini R.","surname":"Benouini","given-name":"Rachid","initials":"R.","afid": [{"@_fa": "true", "$" :"60017021"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/12142694700","authid":"12142694700","authname":"Zenkouar K.","surname":"Zenkouar","given-name":"Khalid","initials":"K.","afid": [{"@_fa": "true", "$" :"60017021"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/7801340942","authid":"7801340942","authname":"Zarghili A.","surname":"Zarghili","given-name":"Arsalane","initials":"A.","afid": [{"@_fa": "true", "$" :"60017021"}]}],"authkeywords":"Complex background | Hand gesture recognition | Moment invariants | Quaternion algebra | RST invariants | Tchebichef moments","source-id":"24822","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85078343881"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85078343881?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078343881&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85078343881&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85078343881","dc:identifier":"SCOPUS_ID:85078343881","eid":"2-s2.0-85078343881","dc:title":"Paper currency defect detection algorithm using quaternion uniform strength","dc:creator":"Gai S.","prism:publicationName":"Neural Computing and Applications","prism:issn":"09410643","prism:eIssn":"14333058","prism:volume":"32","prism:issueIdentifier":"16","prism:pageRange":"12999-13016","prism:coverDate":"2020-08-01","prism:coverDisplayDate":"1 August 2020","prism:doi":"10.1007/s00521-020-04745-6","dc:description":"In this paper, we propose a novel paper currency defect detection algorithm using quaternion uniform strength. We first build paper currency image preprocessing integration framework which includes intensity balancing, paper currency location, and geometric correction. We then propose a global–local paper currency image registration algorithm by moving key areas within certain range which can eliminate the false difference effectively. Finally, the quaternion uniform strength is calculated by using quaternion convolution edge detector. The defect degree of paper currency is determined by using the quaternion uniform color difference. The proposed algorithm is tested using different datasets from five countries: CNY, USD, EUR, VND, and RUB. The experimental results demonstrate that the proposed algorithm yields better results than the existing state-of-the-art paper currency defect detection techniques. The demo of the proposed paper currency defect detection algorithm will be publicly available.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60083519","afid":"60083519","affilname":"Nanchang Hangkong University","affiliation-city":"Nanchang","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/35748407200","authid":"35748407200","orcid":"0000-0001-6139-1410","authname":"Gai S.","surname":"Gai","given-name":"Shan","initials":"S.","afid": [{"@_fa": "true", "$" :"60083519"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57214143457","authid":"57214143457","authname":"Xu X.","surname":"Xu","given-name":"Xiaolin","initials":"X.","afid": [{"@_fa": "true", "$" :"60083519"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/22735929700","authid":"22735929700","authname":"Xiong B.","surname":"Xiong","given-name":"Bangshu","initials":"B.","afid": [{"@_fa": "true", "$" :"60083519"}]}],"authkeywords":"Defect detection | Paper currency image registration | Quaternion convolution | Quaternion uniform strength","source-id":"24800","fund-acr":"NSFC","fund-no":"20171BBE50013","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091706760"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091706760?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091706760&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85091706760&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85091706760","dc:identifier":"SCOPUS_ID:85091706760","eid":"2-s2.0-85091706760","dc:title":"Quaternion markov splicing detection for color images based on quaternion discrete cosine transform","dc:creator":"Wang J.","prism:publicationName":"KSII Transactions on Internet and Information Systems","prism:issn":"19767277","prism:eIssn":"22881468","prism:volume":"14","prism:issueIdentifier":"7","prism:pageRange":"2981-2996","prism:coverDate":"2020-07-31","prism:coverDisplayDate":"31 July 2020","prism:doi":"10.3837/tiis.2020.07.014","dc:description":"With the increasing amount of splicing images, many detection schemes of splicing images are proposed. In this paper, a splicing detection scheme for color image based on the quaternion discrete cosine transform (QDCT) is proposed. Firstly, the proposed quaternion Markov features are extracted in QDCT domain. Secondly, the proposed quaternion Markov features consist of global and local quaternion Markov, which utilize both magnitude and three phases to extract Markov features by using two different ways. In total, 2916-D features are extracted. Finally, the support vector machine (SVM) is used to detect the splicing images. In our experiments, the accuracy of the proposed scheme reaches 99.16% and 97.52% in CASIA TIDE v1.0 and CASIA TIDE v2.0, respectively, which exceeds that of the existing schemes.","citedby-count":"5","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60273040","afid":"60273040","affilname":"Institute of Information Engineering","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60064143","afid":"60064143","affilname":"Nanjing University of Information Science &amp; Technology","affiliation-city":"Nanjing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025578","afid":"60025578","affilname":"Xidian University","affiliation-city":"Xi'an","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019499","afid":"60019499","affilname":"Chinese Academy of Sciences","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/113880533","afid":"113880533","affilname":"New Jersey Institution of Technology","affiliation-city":null,"affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57192930875","authid":"57192930875","authname":"Wang J.","surname":"Wang","given-name":"Jinwei","initials":"J.","afid": [{"@_fa": "true", "$" :"60064143"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57204288067","authid":"57204288067","authname":"Liu R.","surname":"Liu","given-name":"Renfeng","initials":"R.","afid": [{"@_fa": "true", "$" :"60019499"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/58466208300","authid":"58466208300","authname":"Wang H.","surname":"Wang","given-name":"Hao","initials":"H.","afid": [{"@_fa": "true", "$" :"60025578"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/58388914400","authid":"58388914400","authname":"Wu B.","surname":"Wu","given-name":"Bin","initials":"B.","afid": [{"@_fa": "true", "$" :"60273040"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7404964736","authid":"7404964736","authname":"Shi Y.Q.","surname":"Shi","given-name":"Yun Qing","initials":"Y.Q.","afid": [{"@_fa": "true", "$" :"113880533"}]}],"authkeywords":"Color image | QDCT | Quaternion Markov | splicing detection","source-id":"17700155805","fund-acr":"NSFC","fund-no":"61232016","fund-sponsor":"National Natural Science Foundation of China","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfree2read"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Bronze"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091011979"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85091011979?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85091011979&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85091011979&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85091011979","dc:identifier":"SCOPUS_ID:85091011979","eid":"2-s2.0-85091011979","dc:title":"Median filtering detection based on quaternion convolutional neural network","dc:creator":"Wang J.","prism:publicationName":"Computers, Materials and Continua","prism:issn":"15462218","prism:eIssn":"15462226","prism:volume":"65","prism:issueIdentifier":"1","prism:pageRange":"929-943","prism:coverDate":"2020-07-23","prism:coverDisplayDate":"23 July 2020","prism:doi":"10.32604/cmc.2020.06569","dc:description":"Median filtering is a nonlinear signal processing technique and has an advantage in the field of image anti-forensics. Therefore, more attention has been paid to the forensics research of median filtering. In this paper, a median filtering forensics method based on quaternion convolutional neural network (QCNN) is proposed. The median filtering residuals (MFR) are used to preprocess the images. Then the output of MFR is expanded to four channels and used as the input of QCNN. In QCNN, quaternion convolution is designed that can better mix the information of different channels than traditional methods. The quaternion pooling layer is designed to evaluate the result of quaternion convolution. QCNN is proposed to features well combine the three-channel information of color image and fully extract forensics features. Experiments show that the proposed method has higher accuracy and shorter training time than the traditional convolutional neural network with the same convolution depth.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60273040","afid":"60273040","affilname":"Institute of Information Engineering","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60064143","afid":"60064143","affilname":"Nanjing University of Information Science &amp; Technology","affiliation-city":"Nanjing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60022904","afid":"60022904","affilname":"New Jersey Institute of Technology","affiliation-city":"Newark","affiliation-country":"United States"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/113621902","afid":"113621902","affilname":"State Key Laboratory of Mathematical Engineering and Advanced Computing","affiliation-city":"Zhengzhou","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "7", "$" :"7"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57219116697","authid":"57219116697","authname":"Wang J.","surname":"Wang","given-name":"Jinwei","initials":"J.","afid": [{"@_fa": "true", "$" :"60064143"},{"@_fa": "true", "$" :"113621902"},{"@_fa": "true", "$" :"60064143"},{"@_fa": "true", "$" :"60273040"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57216842817","authid":"57216842817","authname":"Ni Q.","surname":"Ni","given-name":"Qiye","initials":"Q.","afid": [{"@_fa": "true", "$" :"60064143"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57216699124","authid":"57216699124","authname":"Zhang Y.","surname":"Zhang","given-name":"Yang","initials":"Y.","afid": [{"@_fa": "true", "$" :"60064143"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/8976166200","authid":"8976166200","authname":"Luo X.","surname":"Luo","given-name":"Xiangyang","initials":"X.","afid": [{"@_fa": "true", "$" :"113621902"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7404964736","authid":"7404964736","authname":"Shi Y.","surname":"Shi","given-name":"Yunqing","initials":"Y.","afid": [{"@_fa": "true", "$" :"60022904"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/35771838800","authid":"35771838800","authname":"Zhai J.","surname":"Zhai","given-name":"Jiangtao","initials":"J.","afid": [{"@_fa": "true", "$" :"60064143"}]},{"@_fa": "true", "@seq": "7", "author-url":"https://api.elsevier.com/content/author/author_id/57221974224","authid":"57221974224","authname":"Jha S.K.","surname":"Jha","given-name":"Sunil Kr","initials":"S.K.","afid": [{"@_fa": "true", "$" :"60064143"}]}],"authkeywords":"Color image | Median filtering forensics | Quaternion convolution layer | Quaternion pooling layer","source-id":"24364","fund-acr":"NSFC","fund-no":"2016QY 01W0105","fund-sponsor":"National Natural Science Foundation of China","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080054139"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080054139?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080054139&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85080054139&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S0925231220302460"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85080054139","dc:identifier":"SCOPUS_ID:85080054139","eid":"2-s2.0-85080054139","dc:title":"Matrix-valued twin-multistate Hopfield neural networks","dc:creator":"Kobayashi M.","prism:publicationName":"Neurocomputing","prism:issn":"09252312","prism:eIssn":"18728286","prism:volume":"397","prism:pageRange":"108-113","prism:coverDate":"2020-07-15","prism:coverDisplayDate":"15 July 2020","prism:doi":"10.1016/j.neucom.2020.02.056","pii":"S0925231220302460","dc:description":"A complex-valued Hopfield neural network (CHNN) has been widely used for the storage of image data. The CHNN has been extended using hypercomplex numbers. A couple of hypercomplex-valued Hopfield neural networks employ a twin-multistate activation function to reduce the numbers of weight parameters. In this work, we propose a matrix-valued twin-multistate Hopfield neural network (MTMHNN), whose neuron states and weights are 2 × 2 matrices. Computer simulations show that the MTMHNN has better noise tolerance than the hypercomplex-valued twin-multistate Hopfield neural networks.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60009064","afid":"60009064","affilname":"University of Yamanashi","affiliation-city":"Kofu","affiliation-country":"Japan"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55741799500","authid":"55741799500","orcid":"0000-0002-4453-0615","authname":"Kobayashi M.","surname":"Kobayashi","given-name":"Masaki","initials":"M.","afid": [{"@_fa": "true", "$" :"60009064"}]}],"authkeywords":"Complex-valued neural networks | Hopfield neural networks | Twin-multistate activation function","source-id":"24807","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080026270"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85080026270?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85080026270&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85080026270&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S0925231220302435"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85080026270","dc:identifier":"SCOPUS_ID:85080026270","eid":"2-s2.0-85080026270","dc:title":"Deep octonion networks","dc:creator":"Wu J.","prism:publicationName":"Neurocomputing","prism:issn":"09252312","prism:eIssn":"18728286","prism:volume":"397","prism:pageRange":"179-191","prism:coverDate":"2020-07-15","prism:coverDisplayDate":"15 July 2020","prism:doi":"10.1016/j.neucom.2020.02.053","pii":"S0925231220302435","dc:description":"Deep learning is a hot research topic in the field of machine learning methods and applications. Real-value neural networks (Real NNs), especially deep real networks (DRNs), have been widely used in many research fields. In recent years, the deep complex networks (DCNs) and the deep quaternion networks (DQNs) have attracted more and more attentions. The octonion algebra, which is an extension of complex algebra and quaternion algebra, can provide more efficient and compact expressions. This paper constructs a general framework of deep octonion networks (DONs) and provides the main building blocks of DONs such as octonion convolution, octonion batch normalization and octonion weight initialization; DONs are then used in image classification tasks for CIFAR-10 and CIFAR-100 data sets. Compared with the DRNs, the DCNs, and the DQNs, the proposed DONs have better convergence and higher classification accuracy. The success of DONs is also explained by multi-task learning.","citedby-count":"17","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60128870","afid":"60128870","affilname":"Key Laboratory of Computer Network and Information Integration, Ministry of Education","affiliation-city":"Nanjing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60000905","afid":"60000905","affilname":"Inserm","affiliation-city":"Paris","affiliation-country":"France"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "6", "$" :"6"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/34874119700","authid":"34874119700","orcid":"0000-0001-7171-1318","authname":"Wu J.","surname":"Wu","given-name":"Jiasong","initials":"J.","afid": [{"@_fa": "true", "$" :"60128870"},{"@_fa": "true", "$" :"60000905"},{"@_fa": "true", "$" :"60000905"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57216519819","authid":"57216519819","authname":"Xu L.","surname":"Xu","given-name":"Ling","initials":"L.","afid": [{"@_fa": "true", "$" :"60128870"},{"@_fa": "true", "$" :"60000905"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57198943431","authid":"57198943431","authname":"Wu F.","surname":"Wu","given-name":"Fuzhi","initials":"F.","afid": [{"@_fa": "true", "$" :"60128870"},{"@_fa": "true", "$" :"60000905"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/36082608100","authid":"36082608100","orcid":"0000-0003-2095-8470","authname":"Kong Y.","surname":"Kong","given-name":"Youyong","initials":"Y.","afid": [{"@_fa": "true", "$" :"60128870"},{"@_fa": "true", "$" :"60000905"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7004293671","authid":"7004293671","authname":"Senhadji L.","surname":"Senhadji","given-name":"Lotfi","initials":"L.","afid": [{"@_fa": "true", "$" :"60000905"},{"@_fa": "true", "$" :"60000905"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/7203086899","authid":"7203086899","orcid":"0000-0002-3833-7915","authname":"Shu H.","surname":"Shu","given-name":"Huazhong","initials":"H.","afid": [{"@_fa": "true", "$" :"60128870"},{"@_fa": "true", "$" :"60000905"}]}],"authkeywords":"Complex | Convolutional neural network | Image classification | Octonion | Quaternion","source-id":"24807","fund-acr":"Inserm","fund-no":"WQ20163200398","fund-sponsor":"Institut National de la Santé et de la Recherche Médicale","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfree2read"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Bronze"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85098250783"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85098250783?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098250783&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85098250783&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85098250783","dc:identifier":"SCOPUS_ID:85098250783","eid":"2-s2.0-85098250783","dc:title":"A Novel Triple Layer Method to Hide Secret Image Using Steganography","dc:creator":"Nirenjena S.","prism:publicationName":"2020 International Conference on System, Computation, Automation and Networking, ICSCAN 2020","prism:isbn": [{"@_fa": "true", "$" :"9781728162027"}],"prism:pageRange":null,"prism:coverDate":"2020-07-03","prism:coverDisplayDate":"3 July 2020","prism:doi":"10.1109/ICSCAN49426.2020.9262406","dc:description":"A new quaternion-based lossless encryption strategy for computerized picture and correspondence on medication images. However, We have investigated and marginally changed the idea of the system to call attention to the best area for the proposed encryption plot. which altogether improves speed of pictures encryption in correlation with those initially installed into cutting edge encryption standard and triple information encryption standard.The security of delicate and classified information become a difficult assignment in the present situation as to an ever increasing extent computerized information is put away and transmitted between the end clients. The protection is fundamentally vital if there should be an occurrence of clinical information, which contains the significant data of the patients. The proposed method uses the biometrics of the patient/proprietor to create a key administration framework to acquire the parameters engaged with the proposed procedure.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60115478","afid":"60115478","affilname":"IFET College of Engineering","affiliation-city":"Viluppuram","affiliation-country":"India"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57221124231","authid":"57221124231","authname":"Nirenjena S.","surname":"Nirenjena","given-name":"S.","initials":"S.","afid": [{"@_fa": "true", "$" :"60115478"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57221113720","authid":"57221113720","authname":"Jayapriya M.","surname":"Jayapriya","given-name":"M.","initials":"M.","afid": [{"@_fa": "true", "$" :"60115478"}]}],"authkeywords":"Decrption | Loseless encryption | Medical images","article-number":"9262406","source-id":"21101032763","fund-no":"undefined","openaccess":"0","openaccessFlag":false}]}}