{"search-results":{"opensearch:totalResults":"230","opensearch:startIndex":"150","opensearch:itemsPerPage":"25","opensearch:Query":{"@role": "request", "@searchTerms": "TITLE-ABS-KEY ( hypercomplex OR hyper-complex OR hipercomplex OR quaternions OR octonions OR quaternionic ) AND TITLE-ABS-KEY ( signal OR image )", "@startPage": "150"},"link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/search/scopus?start=150&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "first", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "prev", "@href": "https://api.elsevier.com/content/search/scopus?start=125&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "next", "@href": "https://api.elsevier.com/content/search/scopus?start=175&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "last", "@href": "https://api.elsevier.com/content/search/scopus?start=205&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"}],"entry": [{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123223054"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123223054?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123223054&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85123223054&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85123223054","dc:identifier":"SCOPUS_ID:85123223054","eid":"2-s2.0-85123223054","dc:title":"Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis","dc:creator":"Zhu Y.","prism:publicationName":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","prism:issn":"10636919","prism:isbn": [{"@_fa": "true", "$" :"9781665445092"}],"prism:pageRange":"9954-9963","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/CVPR46437.2021.00983","dc:description":"How to effectively represent camera pose is an essential problem in 3D computer vision, especially in tasks such as camera pose regression and novel view synthesis. Traditionally, 3D position of the camera is represented by Cartesian coordinate and the orientation is represented by Euler angle or quaternions. These representations are manually designed, which may not be the most effective representation for downstream tasks. In this work, we propose an approach to learn neural representations of camera poses and 3D scenes, coupled with neural representations of local camera movements. Specifically, the camera pose and 3D scene are represented as vectors and the local camera movement is represented as a matrix operating on the vector of the camera pose. We demonstrate that the camera movement can further be parametrized by a matrix Lie algebra that underlies a rotation system in the neural space. The vector representations are then concatenated and generate the posed 2D image through a decoder network. The model is learned from only posed 2D images and corresponding camera poses, without access to depths or shapes. We conduct extensive experiments on synthetic and real datasets. The results show that compared with other camera pose representations, our learned representation is more robust to noise in novel view synthesis and more effective in camera pose regression.","citedby-count":"6","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60027550","afid":"60027550","affilname":"University of California, Los Angeles","affiliation-city":"Los Angeles","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57713772700","authid":"57713772700","authname":"Zhu Y.","surname":"Zhu","given-name":"Yaxuan","initials":"Y.","afid": [{"@_fa": "true", "$" :"60027550"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57193644606","authid":"57193644606","authname":"Gao R.","surname":"Gao","given-name":"Ruiqi","initials":"R.","afid": [{"@_fa": "true", "$" :"60027550"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57089284100","authid":"57089284100","authname":"Huang S.","surname":"Huang","given-name":"Siyuan","initials":"S.","afid": [{"@_fa": "true", "$" :"60027550"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/22236080700","authid":"22236080700","authname":"Zhu S.C.","surname":"Zhu","given-name":"Song Chun","initials":"S.C.","afid": [{"@_fa": "true", "$" :"60027550"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/56030126800","authid":"56030126800","authname":"Wu Y.N.","surname":"Wu","given-name":"Ying Nian","initials":"Y.N.","afid": [{"@_fa": "true", "$" :"60027550"}]}],"source-id":"24212","fund-acr":"NSF","fund-no":"DMS-2015577","fund-sponsor":"National Science Foundation","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123050472"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123050472?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123050472&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85123050472&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85123050472","dc:identifier":"SCOPUS_ID:85123050472","eid":"2-s2.0-85123050472","dc:title":"SignPose: Sign Language Animation Through 3D Pose Lifting","dc:creator":"Krishna S.","prism:publicationName":"Proceedings of the IEEE International Conference on Computer Vision","prism:issn":"15505499","prism:isbn": [{"@_fa": "true", "$" :"9781665401913"}],"prism:volume":"2021-October","prism:pageRange":"2640-2649","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ICCVW54120.2021.00298","dc:description":"Sign Language Generation (SLG) is a challenging task in computer animation as it involves capturing intricate hand gestures accurately, for several thousand signs in each sign language. Traditional methods require expensive equipment and considerable human involvement. In this paper, we provide a method to automate this process using only plain RGB images to generate sign poses for an avatar - the first of its kind for SLG. Current state of the art models for human 3D pose estimation do not perform satisfactorily in SLG due to the large difference between tasks. The datasets they are trained on contain only tasks like walking and playing sports, which involve significantly different types of motion compared to signing. Synthetic, manually created 3D animations are available for diverse tasks including sign language performance. Modern 2D pose estimation models which work on real world images are also robust enough to work on these animations accurately. Inspired by this, we formulate a novel method of leveraging animation data, using an intermediate 2D pose representation, to train an SLG animation model that works on real world sign language performance videos. To create the dataset for training, we extend an available animated dataset of signs in the Indian Sign Language (ISL) by permuting different hand and body motions. A novel quaternion based architecture is created to perform the task of lifting the 2D keypoints to 3D. The architecture is simplified to match the requirements of our task as well as to work with our smaller dataset size. We train a model, SignPose, using this architecture on the constructed dataset and demonstrate that it matches or outperforms current models for human pose reconstruction for the Sign Language Generation task. We will release both the dataset as well the model to the public to encourage further research in this field.","citedby-count":"4","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60007069","afid":"60007069","affilname":"IIIT Bangalore","affiliation-city":"Bengaluru","affiliation-country":"India"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57222509016","authid":"57222509016","authname":"Krishna S.","surname":"Krishna","given-name":"Shyam","initials":"S.","afid": [{"@_fa": "true", "$" :"60007069"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57417078200","authid":"57417078200","authname":"Vignesh V.P.","surname":"Vignesh","given-name":"Vijay P.","initials":"V.P.","afid": [{"@_fa": "true", "$" :"60007069"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57417396000","authid":"57417396000","authname":"Babu D.J.","surname":"Babu","given-name":"Dinesh J.","initials":"D.J.","afid": [{"@_fa": "true", "$" :"60007069"}]}],"source-id":"110561","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123043995"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85123043995?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85123043995&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85123043995&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85123043995","dc:identifier":"SCOPUS_ID:85123043995","eid":"2-s2.0-85123043995","dc:title":"Deep Quaternion Pose Proposals for 6D Object Pose Tracking","dc:creator":"Majcher M.","prism:publicationName":"Proceedings of the IEEE International Conference on Computer Vision","prism:issn":"15505499","prism:isbn": [{"@_fa": "true", "$" :"9781665401913"}],"prism:volume":"2021-October","prism:pageRange":"243-251","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ICCVW54120.2021.00032","dc:description":"In this work we study quaternion pose distributions for tracking in RGB image sequences the 6D pose of an object selected from a set of objects, for which common models were trained in advance. We propose an unit quaternion representation of the rotational state space for a particle filter, which is then integrated with the particle swarm optimization to shift samples toward local maximas. Owing to k-means++ we better maintain multimodal probability distributions. We train convolutional neural networks to estimate the 2D positions of fiducial points and then to determine PnP-based object pose hypothesis. A CNN is utilized to estimate the positions of fiducial points in order to calculate PnP-based object pose hypothesis. A common Siamese neural network for all objects, which is trained on keypoints from current and previous frame is employed to guide the particles towards predicted pose of the object. Such a key-point based pose hypothesis is injected into the probability distribution that is recursively updated in a Bayesian framework. The 6D object pose tracker is evaluated on Nvidia Jetson AGX Xavier both on synthetic and real sequences of images acquired from a calibrated RGB camera.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60017351","afid":"60017351","affilname":"AGH University of Krakow","affiliation-city":"Krakow","affiliation-country":"Poland"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57215896487","authid":"57215896487","authname":"Majcher M.","surname":"Majcher","given-name":"Mateusz","initials":"M.","afid": [{"@_fa": "true", "$" :"60017351"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55948049900","authid":"55948049900","authname":"Kwolek B.","surname":"Kwolek","given-name":"Bogdan","initials":"B.","afid": [{"@_fa": "true", "$" :"60017351"}]}],"source-id":"110561","fund-acr":"NCN","fund-no":"2017/27/B/ST6/01743","fund-sponsor":"Narodowe Centrum Nauki","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122897686"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122897686?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122897686&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85122897686&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85122897686","dc:identifier":"SCOPUS_ID:85122897686","eid":"2-s2.0-85122897686","dc:title":"3D Clifford Analytic Signal for 3D Envelope Detection on Ultrasound Volume","dc:creator":"Wang L.","prism:publicationName":"IEEE International Ultrasonics Symposium, IUS","prism:issn":"19485719","prism:eIssn":"19485727","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/IUS52206.2021.9593437","dc:description":"Envelope detection is one of the essential steps for ultrasound B-mode image reconstruction. Typically, this reconstruction is based on one dimensional (1D) radio frequency (RF) signal demodulation. In the literature, 1D envelopes are usually stacked to achieve two or three dimensional (2D or 3D) envelope detection, which leads to possible losses of multidimensional con information. In this paper, we propose a hypercomplex analytical signal in the form of Clifford biquater-nion, which is called 3D Clifford analytic signal (3D CAS). Firstly, the convolution is utilized to define the 3D CAS in terms of biquaternion. Next, the relation between the 3D CAS and the classical Hahn's analytical signals in the 3D case is illustrated with the help of Hilbert transforms, which enables a reliable numerical implementation of 3D CAS. Then, the modulus of 3D CAS is proposed and applied to a straightforward 3D ultrasonic RF envelope detection by taking into consideration the information of amplitude of all three directions of space. Experiments were carried out on the ultrasound volume from a phantom with a biopsy needle inserted. The experiments provide a better visual experience from envelope images of RF volume than the compared 1D and 2D methods. The 3D and 2D envelopes present 36.5% and 15.2% improvement to 1D, respectively, from the contrast to noise ratio between the needle region and the adjacent background. From an independent public 2D liver tumor RF sequences of 29 mice, the average structural index similarity (SSIM) and peak signal to noise ratio (PSNR) were improved by 53.0% and 39.0%, respectively, from 2D envelopes comparing to 1D envelopes.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60016253","afid":"60016253","affilname":"INSA Lyon","affiliation-city":"Villeurbanne","affiliation-country":"France"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55637319700","authid":"55637319700","authname":"Wang L.","surname":"Wang","given-name":"Liang","initials":"L.","afid": [{"@_fa": "true", "$" :"60016253"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57198224205","authid":"57198224205","authname":"Girard P.R.","surname":"Girard","given-name":"Patrick R.","initials":"P.R.","afid": [{"@_fa": "true", "$" :"60016253"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/6701664073","authid":"6701664073","authname":"Clarysse P.","surname":"Clarysse","given-name":"Patrick","initials":"P.","afid": [{"@_fa": "true", "$" :"60016253"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/6602903906","authid":"6602903906","authname":"Delachartre P.","surname":"Delachartre","given-name":"Philippe","initials":"P.","afid": [{"@_fa": "true", "$" :"60016253"}]}],"authkeywords":"3D Clifford analytic signal | Clifford biquater-nion | hypercomplex analytical signal | Ultrasound imaging","source-id":"21100219905","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122873024"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122873024?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122873024&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85122873024&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85122873024","dc:identifier":"SCOPUS_ID:85122873024","eid":"2-s2.0-85122873024","dc:title":"Accurate Quaternion Polar Harmonic Transform for Color Image Analysis","dc:creator":"Zhang L.","prism:publicationName":"Scientific Programming","prism:issn":"10589244","prism:volume":"2021","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1155/2021/7162779","dc:description":"Polar harmonic transforms (PHTs) have been applied in pattern recognition and image analysis. But the current computational framework of PHTs has two main demerits. First, some significant color information may be lost during color image processing in conventional methods because they are based on RGB decomposition or graying. Second, PHTs are influenced by geometric errors and numerical integration errors, which can be seen from image reconstruction errors. This paper presents a novel computational framework of quaternion polar harmonic transforms (QPHTs), namely, accurate QPHTs (AQPHTs). First, to holistically handle color images, quaternion-based PHTs are introduced by using the algebra of quaternions. Second, the Gaussian numerical integration is adopted for geometric and numerical error reduction. When compared with CNNs (convolutional neural networks)-based methods (i.e., VGG16) on the Oxford5K dataset, our AQPHT achieves better performance of scaling invariant representation. Moreover, when evaluated on standard image retrieval benchmarks, our AQPHT using smaller dimension of feature vector achieves comparable results with CNNs-based methods and outperforms the hand craft-based methods by 9.6% w.r.t mAP on the Holidays dataset.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60032758","afid":"60032758","affilname":"Liaoning Technical University","affiliation-city":"Fuxin","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/117187397","afid":"117187397","affilname":"Zhejiang College of Security Technology","affiliation-city":"Wenzhou","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57222095467","authid":"57222095467","orcid":"0000-0001-6299-634X","authname":"Zhang L.","surname":"Zhang","given-name":"Lina","initials":"L.","afid": [{"@_fa": "true", "$" :"117187397"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/36824201500","authid":"36824201500","orcid":"0000-0002-4596-911X","authname":"Sang Y.","surname":"Sang","given-name":"Yu","initials":"Y.","afid": [{"@_fa": "true", "$" :"60032758"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57413511900","authid":"57413511900","orcid":"0000-0001-9861-9247","authname":"Dai D.","surname":"Dai","given-name":"Donghai","initials":"D.","afid": [{"@_fa": "true", "$" :"117187397"}]}],"article-number":"7162779","source-id":"19926","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122572776"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85122572776?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85122572776&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85122572776&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85122572776","dc:identifier":"SCOPUS_ID:85122572776","eid":"2-s2.0-85122572776","dc:title":"LQGDNet: A Local Quaternion and Global Deep Network for Facial Depression Recognition","dc:creator":"Shang Y.","prism:publicationName":"IEEE Transactions on Affective Computing","prism:eIssn":"19493045","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/TAFFC.2021.3139651","dc:description":"Recent visual-based depression recognition methods mostly use hand-crafted features with information lost in color channels, or deep network features with a limited performance from the finite data. In this paper, we propose a method called Local Quaternion and Global Deep Network (LQGDNet) which can combine advantages from hand-crafted and deep features. Specifically, the Quaternion XOR Asymmetrical Regional Local Gradient Coding (XOR-AR-LGC) is firstly designed, which encodes the facial images with local textures in the quaternion domain to keep the dependence of color channels, and integrated into the Quaternion Feature Extractor (QFE). To the best of our knowledge, it is the first attempt to use a quaternion-based method for facial depression recognition. Second, we design the Local Quaternion Representation Module (LQRM) composed of Local Deep Feature Extractor (LDFE) and QFE to output local quaternion facial features. Third, global deep facial features are encoded from the Global Deep Representation Module (GDRM) with the deep convolutional neural network. Finally, the LQGDNet integrates LQRM and GDRM with the local quaternion and global deep features and predicts the depression score. The experimental results on AVEC 2013 and AVEC 2014 show the superiority of our method compared to the state-of-the-art approaches.","citedby-count":"9","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60021143","afid":"60021143","affilname":"West Virginia University","affiliation-city":"Morgantown","affiliation-country":"United States"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60020256","afid":"60020256","affilname":"Capital Normal University","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "7", "$" :"7"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7101867863","authid":"7101867863","authname":"Shang Y.","surname":"Shang","given-name":"Yuanyuan","initials":"Y.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57406791800","authid":"57406791800","authname":"Pan Y.","surname":"Pan","given-name":"Yuchen","initials":"Y.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57169139700","authid":"57169139700","authname":"Jiang X.","surname":"Jiang","given-name":"Xiao","initials":"X.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/55849891700","authid":"55849891700","authname":"Shao Z.","surname":"Shao","given-name":"Zhhong","initials":"Z.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/57222984060","authid":"57222984060","authname":"Guo G.","surname":"Guo","given-name":"Guodong","initials":"G.","afid": [{"@_fa": "true", "$" :"60021143"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/55250686500","authid":"55250686500","authname":"Liu T.","surname":"Liu","given-name":"Tie","initials":"T.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "7", "author-url":"https://api.elsevier.com/content/author/author_id/56870698700","authid":"56870698700","authname":"Ding H.","surname":"Ding","given-name":"Hui","initials":"H.","afid": [{"@_fa": "true", "$" :"60020256"}]}],"authkeywords":"Convolutional Neural Network | Convolutional neural networks | Deep Learning | Deep learning | Depression | Depression Recognition | Face recognition | Feature extraction | Image Recognition | Mouth | Quaternion | Quaternions","source-id":"19700177034","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfree2read"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Bronze"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85121843210"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85121843210?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121843210&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85121843210&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85121843210","dc:identifier":"SCOPUS_ID:85121843210","eid":"2-s2.0-85121843210","dc:title":"Quaternion-Valued Convolutional Neural Network Applied for Acute Lymphoblastic Leukemia Diagnosis","dc:creator":"Granero M.A.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030916985"}],"prism:volume":"13074 LNAI","prism:pageRange":"280-293","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-91699-2_20","dc:description":"The field of neural networks has seen significant advances in recent years with the development of deep and convolutional neural networks. Although many of the current works address real-valued models, recent studies reveal that neural networks with hypercomplex-valued parameters can better capture, generalize, and represent the complexity of multidimensional data. This paper explores the quaternion-valued convolutional neural network application for a pattern recognition task from medicine, namely, the diagnosis of acute lymphoblastic leukemia. Precisely, we compare the performance of real-valued and quaternion-valued convolutional neural networks to classify lymphoblasts from the peripheral blood smear microscopic images. The quaternion-valued convolutional neural network achieved better or similar performance than its corresponding real-valued network but using only 34% of its parameters. This result confirms that quaternion algebra allows capturing and extracting information from a color image with fewer parameters.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60112684","afid":"60112684","affilname":"Instituto Federal de Educação, Ciência e Tecnologia de São Paulo","affiliation-city":"Sao Paulo","affiliation-country":"Brazil"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60072061","afid":"60072061","affilname":"Escuela Superior Politecnica del Litoral Ecuador","affiliation-city":"Guayaquil","affiliation-country":"Ecuador"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60029570","afid":"60029570","affilname":"Universidade Estadual de Campinas","affiliation-city":"Campinas","affiliation-country":"Brazil"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56880185200","authid":"56880185200","orcid":"0000-0002-1993-1898","authname":"Granero M.A.","surname":"Granero","given-name":"Marco Aurélio","initials":"M.A.","afid": [{"@_fa": "true", "$" :"60112684"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57383597100","authid":"57383597100","orcid":"0000-0001-8530-9880","authname":"Hernández C.X.","surname":"Hernández","given-name":"Cristhian Xavier","initials":"C.X.","afid": [{"@_fa": "true", "$" :"60072061"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/13408783400","authid":"13408783400","orcid":"0000-0003-4026-5110","authname":"Valle M.E.","surname":"Valle","given-name":"Marcos Eduardo","initials":"M.E.","afid": [{"@_fa": "true", "$" :"60029570"}]}],"authkeywords":"Classification problem | Hypercomplex number | Neural network | Quaternion algebra | Quaternion neural network","source-id":"25674","fund-acr":"CAPES","fund-no":"undefined","fund-sponsor":"Coordenação de Aperfeiçoamento de Pessoal de Nível Superior","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85121592391"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85121592391?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85121592391&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85121592391&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85121592391","dc:identifier":"SCOPUS_ID:85121592391","eid":"2-s2.0-85121592391","dc:title":"Modern Art Interactive Design Based on Artificial Intelligence Technology","dc:creator":"Zhang W.","prism:publicationName":"Scientific Programming","prism:issn":"10589244","prism:volume":"2021","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1155/2021/5223034","dc:description":"To improve the effect of modern art design, this study presents a camera pose estimation algorithm based on the least feature points of quaternion. Moreover, this study detects and matches the feature points of the camera image and establishes a system of formulas through the rigid constraints of the feature points, thereby constructing an eigenvalue problem to solve the camera pose. In addition, this study combines artificial intelligence technology to construct the modern art interactive design system and structure the system function structure. Finally, this study analyzes the logical structure and spatial structure of the system and uses the design to analyze the performance of the modern art interaction design model proposed in this study. Through experimental research, it can be known that the modern art interactive design system based on artificial intelligence technology proposed in this study can basically meet the artistic design needs of the new media era.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60069722","afid":"60069722","affilname":"Dalian Minzu University","affiliation-city":"Dalian","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57775344400","authid":"57775344400","orcid":"0000-0002-0985-3830","authname":"Zhang W.","surname":"Zhang","given-name":"Weihua","initials":"W.","afid": [{"@_fa": "true", "$" :"60069722"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57194567836","authid":"57194567836","orcid":"0000-0001-8556-6019","authname":"Jia Y.","surname":"Jia","given-name":"Yufeng","initials":"Y.","afid": [{"@_fa": "true", "$" :"60069722"}]}],"article-number":"5223034","source-id":"19926","fund-no":"undefined","fund-sponsor":"Natural Science Foundation of Liaoning Province","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120718697"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120718697?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120718697&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85120718697&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85120718697","dc:identifier":"SCOPUS_ID:85120718697","eid":"2-s2.0-85120718697","dc:title":"A quaternionic Saito–Kurokawa lift and cusp forms on G2","dc:creator":"Pollack A.","prism:publicationName":"Algebra and Number Theory","prism:issn":"19370652","prism:volume":"15","prism:issueIdentifier":"5","prism:pageRange":"1213-1244","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.2140/ant.2021.15.1213","dc:description":"We consider a special theta lift θ(f) from cuspidal Siegel modular forms f on Sp4 to “modular forms” θ(f) on SO(4, 4), in the sense of our prior work (Pollack 2020a). This lift can be considered an analogue of the Saito–Kurokawa lift, where now the image of the lift is representations of SO(4, 4) that are quaternionic at infinity. We relate the Fourier coefficients of θ(f) to those of f, and in particular prove that θ(f) is nonzero and has algebraic Fourier coefficients if f does. Restricting the θ(f) to G2 ⊆ SO(4, 4), we obtain cuspidal modular forms on G2 of arbitrarily large weight with all algebraic Fourier coefficients. In the case of level one, we obtain precise formulas for the Fourier coefficients of θ(f) in terms of those of f. In particular, we construct nonzero cuspidal modular forms on G2 of level one with all integer Fourier coefficients.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60030612","afid":"60030612","affilname":"University of California, San Diego","affiliation-city":"San Diego","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57194504870","authid":"57194504870","authname":"Pollack A.","surname":"Pollack","given-name":"Aaron","initials":"A.","afid": [{"@_fa": "true", "$" :"60030612"}]}],"authkeywords":"Cusp forms | Fourier coefficients | G2 modular forms | Saito–Kurokawa | Theta correspondence","source-id":"19700175257","fund-acr":"SF","fund-no":"585147","fund-sponsor":"Simons Foundation","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120670208"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120670208?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120670208&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85120670208&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85120670208","dc:identifier":"SCOPUS_ID:85120670208","eid":"2-s2.0-85120670208","dc:title":"Investigation of the hashing algorithm extension of depth image matching for liver surgery","dc:creator":"Numata S.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030784645"}],"prism:volume":"12763 LNCS","prism:pageRange":"615-624","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-78465-2_44","dc:description":"We have developed some liver posture estimation methods for achieving a liver surgical navigation system that can support surgeons who needs to know very precise information about vessels in the liver. Those methods use 3D liver models scanned from patients and 2D images scanned by depth cameras for estimating the liver posture as accurately as possible. Since a new posture estimation method using simple and high-speed image hashing algorithm was developed last year, we are trying to improve the method in accuracy and applicability for the real-time liver posture tracking. In this paper, we examine how deep learning methods can be used for liver posture estimation and its tracking over the 2D images scanned from depth cameras. We study how can a multi-layer perceptron neural network learn and estimate the liver rotation expressed in quaternion form. The real-time surgical navigation system should be efficiently implemented by combining multiple estimation methods including the deep learning method.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60028768","afid":"60028768","affilname":"Okayama Prefectural University","affiliation-city":"Soja","affiliation-country":"Japan"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60000886","afid":"60000886","affilname":"Osaka Electro-Communication University","affiliation-city":"Neyagawa","affiliation-country":"Japan"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/15074063800","authid":"15074063800","authname":"Numata S.","surname":"Numata","given-name":"Satoshi","initials":"S.","afid": [{"@_fa": "true", "$" :"60000886"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/14025655100","authid":"14025655100","authname":"Koeda M.","surname":"Koeda","given-name":"Masanao","initials":"M.","afid": [{"@_fa": "true", "$" :"60028768"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/43261873200","authid":"43261873200","authname":"Onishi K.","surname":"Onishi","given-name":"Katsuhiko","initials":"K.","afid": [{"@_fa": "true", "$" :"60000886"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/7408048418","authid":"7408048418","authname":"Watanabe K.","surname":"Watanabe","given-name":"Kaoru","initials":"K.","afid": [{"@_fa": "true", "$" :"60000886"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7003502107","authid":"7003502107","authname":"Noborio H.","surname":"Noborio","given-name":"Hiroshi","initials":"H.","afid": [{"@_fa": "true", "$" :"60000886"}]}],"authkeywords":"Deep learning | Liver surgery support | Navigation","source-id":"25674","fund-acr":"MEXT","fund-no":"undefined","fund-sponsor":"Ministry of Education, Culture, Sports, Science and Technology","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120407292"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120407292?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120407292&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85120407292&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85120407292","dc:identifier":"SCOPUS_ID:85120407292","eid":"2-s2.0-85120407292","dc:title":"Color image encryption scheme based on quaternion discrete multi-fractional random transform and compressive sensing","dc:creator":"Ye H.S.","prism:publicationName":"Optica Applicata","prism:issn":"00785466","prism:eIssn":"18997015","prism:volume":"51","prism:issueIdentifier":"3","prism:pageRange":"349-364","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.37190/oa210304","dc:description":"A color image compression-encryption algorithm by combining quaternion discrete multi-fractional random transform with compressive sensing is investigated, in which the chaos-based fractional orders greatly improve key sensitivity. The original color image is compressed and encrypted with the assistance of compressive sensing, in which the partial Hadamard matrix adopted as a measurement matrix is constructed by iterating Chebyshev map instead of utilizing the entire Guassian matrix as a key. The sparse images are divided into 12 sub-images and then represented as three quaternion signals, which are modulated by the quaternion discrete multi-fractional random transform. The image blocking and the quaternion representation make the proposed cryptosystem avoid additional data extension existing in many transform-based methods. To further improve the level of security, the plaintext-related key streams generated by the 2D logistic-sine-coupling map are adopted to diffuse and confuse the intermediate results simultaneously. Consequently, the final ciphertext image is attained. Simulation results reveal that the proposed cryptosystem is feasible with high security and has strong robustness against various attacks.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60008332","afid":"60008332","affilname":"Nanchang University","affiliation-city":"Nanchang","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57216847190","authid":"57216847190","authname":"Ye H.S.","surname":"Ye","given-name":"Huo Sheng","initials":"H.S.","afid": [{"@_fa": "true", "$" :"60008332"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57226311718","authid":"57226311718","authname":"Dai J.Y.","surname":"Dai","given-name":"Jing Yi","initials":"J.Y.","afid": [{"@_fa": "true", "$" :"60008332"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57359546300","authid":"57359546300","authname":"Wen S.X.","surname":"Wen","given-name":"Shun Xi","initials":"S.X.","afid": [{"@_fa": "true", "$" :"60008332"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/21739408600","authid":"21739408600","authname":"Gong L.H.","surname":"Gong","given-name":"Li Hua","initials":"L.H.","afid": [{"@_fa": "true", "$" :"60008332"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/53867555700","authid":"53867555700","authname":"Zhang W.Q.","surname":"Zhang","given-name":"Wen Quan","initials":"W.Q.","afid": [{"@_fa": "true", "$" :"60008332"}]}],"authkeywords":"Color image encryption | Compressive sensing | Confusion-diffusion strategy | Quaternion discrete multi-fractional random transform","source-id":"12289","fund-acr":"NSFC","fund-no":"20181BBE58022","fund-sponsor":"National Natural Science Foundation of China","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfree2read"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Bronze"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120075637"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85120075637?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85120075637&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85120075637&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85120075637","dc:identifier":"SCOPUS_ID:85120075637","eid":"2-s2.0-85120075637","dc:title":"A Trainable Monogenic ConvNet Layer Robust in front of Large Contrast Changes in Image Classification","dc:creator":"Moya-Sanchez E.U.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"163735-163746","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3128552","dc:description":"At present, Convolutional Neural Networks (ConvNets) achieve remarkable performance in image classification tasks. However, current ConvNets cannot guarantee the capabilities of mammalian visual systems such as invariance to contrast and illumination changes. Some ideas for overcoming the illumination and contrast variations must usually be tuned manually and tend to fail when tested with other types of data degradation. In this context, a new bio-inspired entry layer is presented in this work, M6, which detects low-level geometric features (lines, edges, and orientations) similar to those patterns detected by the V1 visual cortex. This new trainable layer is capable of dealing with image classification tasks even with large contrast variations. The explanation for this behavior is due to the use of monogenic signal geometry, which represents each pixel value in a 3D space using quaternions, a fact that confers a degree of explainability to the networks. The M6 was compared to conventional convolutional layer (C) and a deterministic quaternion local phase layer (Q9). The experimental setup is designed to evaluate the robustness of this M6 enriched ConvNet model and includes three architectures, four datasets, and three types of contrast degradation (including non-uniform haze degradations). The numerical results reveal that the models with M6 are the most robust in front of any kind of contrast variations. This amounts to a significant enhancement of the C models, which usually have reasonably good performance only when the same training and test degradation are used, except for the case of maximum degradation. Moreover, the Structural Similarity Index Measure (SSIM) and Peak Signal to Noise Ratio (PSNR) are used to analyze and explain the robustness effect of the M6 feature maps under any kind of contrast degradations.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60030794","afid":"60030794","affilname":"Centro de Investigaciones en Optica, A.C.","affiliation-city":"Leon","affiliation-country":"Mexico"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60007592","afid":"60007592","affilname":"Universitat Politécnica de Catalunya","affiliation-city":"Barcelona","affiliation-country":"Spain"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/123672783","afid":"123672783","affilname":"Gobierno de Jalisco","affiliation-city":"Guadalajara","affiliation-country":"Mexico"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/36667039500","authid":"36667039500","orcid":"0000-0002-5123-4881","authname":"Moya-Sanchez E.U.","surname":"Moya-Sanchez","given-name":"E. Ulises","initials":"E.U.","afid": [{"@_fa": "true", "$" :"123672783"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6507409675","authid":"6507409675","authname":"Xambo-Descamps S.","surname":"Xambo-Descamps","given-name":"Sebastia","initials":"S.","afid": [{"@_fa": "true", "$" :"60007592"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57390550600","authid":"57390550600","authname":"Perez A.S.","surname":"Perez","given-name":"Abraham Sanchez","initials":"A.S.","afid": [{"@_fa": "true", "$" :"123672783"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57204354992","authid":"57204354992","orcid":"0000-0002-6353-0864","authname":"Salazar-Colores S.","surname":"Salazar-Colores","given-name":"Sebastian","initials":"S.","afid": [{"@_fa": "true", "$" :"60030794"},{"@_fa": "true", "$" :"60007592"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7004065770","authid":"7004065770","orcid":"0000-0003-0192-3096","authname":"Cortes U.","surname":"Cortes","given-name":"Ulises","initials":"U.","afid": [{"@_fa": "true", "$" :"60007592"}]}],"authkeywords":"Bio-inspired models | ConvNet | monogenic signal | robust deep-learning","source-id":"21100374601","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"},{"$" :"repository"},{"$" :"repositoryvor"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85119438092"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85119438092?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119438092&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85119438092&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85119438092","dc:identifier":"SCOPUS_ID:85119438092","eid":"2-s2.0-85119438092","dc:title":"QDCT-Based Blind Color Image Watermarking with Aid of GWO and DnCNN for Performance Improvement","dc:creator":"Hsu L.Y.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"155138-155152","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3127917","dc:description":"Artificial intelligence (AI) is of great potential for improving the performance of image processing and applications. In this study, we incorporate two AI techniques, namely, the grey wolf optimizer (GWO) and denoising convolutional neural network (DnCNN), within a framework developed based on the quaternion discrete cosine transform (QDCT). Binary embedding is formulated according to the attribute of each QDCT component and the distinctive properties of available modulation schemes. The GWO is responsible for performance optimization, while the DnCNN makes the extracted binary watermark more visually recognizable. Experiment results demonstrate the efficacy of the proposed scheme for resisting a variety of image processing attacks. The proposed scheme outperforms existing ones in terms of the robustness and intelligibility of the retrieved watermarks under the same payload capacity.","citedby-count":"8","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60028478","afid":"60028478","affilname":"National Ilan University","affiliation-city":"Yilan","affiliation-country":"Taiwan"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/116277665","afid":"116277665","affilname":"St. Mary's Junior College of Medicine Nursing and Management","affiliation-city":"I-lan","affiliation-country":"Taiwan"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/26654165700","authid":"26654165700","orcid":"0000-0002-9543-6872","authname":"Hsu L.Y.","surname":"Hsu","given-name":"Ling Yuan","initials":"L.Y.","afid": [{"@_fa": "true", "$" :"116277665"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7404097894","authid":"7404097894","orcid":"0000-0001-6519-7535","authname":"Hu H.T.","surname":"Hu","given-name":"Hwai Tsu","initials":"H.T.","afid": [{"@_fa": "true", "$" :"60028478"}]}],"authkeywords":"Blind color image watermarking | denoising convolutional neural network | grey wolf optimizer | mixed modulation | quaternion discrete cosine transform","source-id":"21100374601","fund-acr":"MOST","fund-no":"MOST 109-2221-E-197-031","fund-sponsor":"Ministry of Science and Technology, Taiwan","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85119425409"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85119425409?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85119425409&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85119425409&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85119425409","dc:identifier":"SCOPUS_ID:85119425409","eid":"2-s2.0-85119425409","dc:title":"Octonion Short-Time Fourier Transform for Time-Frequency Representation and Its Applications","dc:creator":"Gao W.B.","prism:publicationName":"IEEE Transactions on Signal Processing","prism:issn":"1053587X","prism:eIssn":"19410476","prism:volume":"69","prism:pageRange":"6386-6398","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/TSP.2021.3127678","dc:description":"The octonion Fourier transform (OFT) is a useful tool for signal processing and analysis. However, due to the lack of time localization information, it is not suitable for processing signals whose frequencies vary with time. In this paper, we utilize octonion algebra to propose a new method for time-frequency representation (TFR) called the octonion short-time Fourier transform (OSTFT). The originality of the method is based on the quaternion short-time Fourier transform (QSTFT). First, we generalize the QSTFT to the OSTFT by substituting the quaternion kernel function with the octonion kernel function in the definition of the QSTFT, and the physical significance of the OSTFT is presented. Then, several essential properties of the OSTFT are derived, such as linearity, inversion formulas, time-frequency shifts and orthogonality relations. Based on the classic Fourier convolution operation, the convolution theorem for the OSTFT is derived. We apply the relationship between the OFT and OSTFT to establish Pitt's inequality and Lieb's inequality for the OSTFT. According to the logarithmic uncertainty principle of the OFT, the logarithmic uncertainty principle associated with the OSTFT is investigated. Finally, an application in which OSTFT can be used to study linear time varying (LTV) systems is proposed, and some potential applications of the OSTFT are introduced.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60016835","afid":"60016835","affilname":"Beijing Institute of Technology","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57214335899","authid":"57214335899","authname":"Gao W.B.","surname":"Gao","given-name":"Wen Biao","initials":"W.B.","afid": [{"@_fa": "true", "$" :"60016835"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/36071614100","authid":"36071614100","orcid":"0000-0002-3850-4656","authname":"Li B.Z.","surname":"Li","given-name":"Bing Zhao","initials":"B.Z.","afid": [{"@_fa": "true", "$" :"60016835"}]}],"authkeywords":"convolution theorem | linear time-varying | Octonion Fourier transform | octonion short-time Fourier transform | uncertainty principle","source-id":"17391","fund-acr":"NSFC","fund-no":"61671063","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118974860"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118974860?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118974860&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118974860&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118974860","dc:identifier":"SCOPUS_ID:85118974860","eid":"2-s2.0-85118974860","dc:title":"5th International Conference on Advances in Computing and Data Sciences, ICACDS 2021","prism:publicationName":"Communications in Computer and Information Science","prism:issn":"18650929","prism:eIssn":"18650937","prism:isbn": [{"@_fa": "true", "$" :"9783030814618"}],"prism:volume":"1440 CCIS","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","dc:description":"The proceedings contain 103 papers. The special focus in this conference is on Advances in Computing and Data Sciences. The topics include: Recognition of Isolated Gestures for Indian Sign Language Using Transfer Learning; a Study of Five Models Based on Non-clinical Data for the Prediction of Diabetes Onset in Medically Under-Served Populations; representation and Visualization of Students’ Progress Data Through Learning Dashboard; denoising of Computed Tomography Images for Improved Performance of Medical Devices in Biomedical Engineering; image Dehazing Through Dark Channel Prior and Color Attenuation Prior; predicting the Death of Road Accidents in Bangladesh Using Machine Learning Algorithms; numerical Computation of Finite Quaternion Mellin Transform Using a New Algorithm; predictive Modeling of Tandem Silicon Solar Cell for Calculating Efficiency; Text Summarization of an Article Extracted from Wikipedia Using NLTK Library; handling Class Imbalance in Electroencephalography Data Using Synthetic Minority Oversampling Technique; grapheme to Phoneme Mapping for Tamil Language; comparative Study of Physiological Signals from Empatica E4 Wristband for Stress Classification; an E-Commerce Prototype for Predicting the Product Return Phenomenon Using Optimization and Regression Techniques; crop Yield Prediction for India Using Regression Algorithms; a Novel Framework for Multimodal Twitter Sentiment Analysis Using Feature Learning; an Iterative Approach Based Reversible Data Hiding with Weight Update for Dual Stego Images; lower and Upper Bounds for ‘Useful’ Renyi Information Rate; sign Language Recognition Using Convolutional Neural Network; Prediction of Stock Price for Indian Stock Market: A Comparative Study Using LSTM and GRU; early Prediction of Cardiovascular Disease Among Young Adults Through Coronary Artery Calcium Score Technique.","citedby-count":"0","prism:aggregationType":"Book Series","subtype":"cr","subtypeDescription":"Conference Review","author-count":{"@limit": "100", "@total": "0"},"source-id":"17700155007","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118863127"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118863127?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118863127&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118863127&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118863127","dc:identifier":"SCOPUS_ID:85118863127","eid":"2-s2.0-85118863127","dc:title":"Robot navigation using modified SLAM procedure based on depth image reconstruction","dc:creator":"Zelenskii A.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:eIssn":"1996756X","prism:isbn": [{"@_fa": "true", "$" :"9781510645844"}],"prism:volume":"11870","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1117/12.2600736","dc:description":"In modern mobile robots, technologies are used to build the most optimal path for its movement. This uses simultaneous navigation and display techniques known as SLAM. A problem with all depth mapping methods is the presence of lost areas. This problem occurs due to poor lighting, mirrored surfaces of objects, or the fine-grained surface of materials, making it impossible to measure depth information. As a result, the effect of overlapping objects appears. It is impossible to distinguish one object from another, or an increase in the object's boundaries (obstacles) occurs. This problem can be solved using image reconstruction techniques. This article presents an approach based on a modified algorithm for finding similar blocks using a neural network. The proposed algorithm also uses the concept of a sparse representation of quaternions, which uses a new gradient to compute the priority function by integrating the quaternion structure with a saliency map. Compared to current technologies, the proposed algorithm provides a plausible reconstruction of the depth map from multimodal images, making it a promising tool for navigation in robot applications. Analysis of the processing results shows that the proposed method allows you to correctly restore the boundaries of objects in the image and depth map, which is a prerequisite for improving the accuracy of the robot's navigation.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60033469","afid":"60033469","affilname":"Bauman Moscow State Technical University","affiliation-city":"Moscow","affiliation-country":"Russian Federation"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60022381","afid":"60022381","affilname":"Beijing Jiaotong University","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60014196","afid":"60014196","affilname":"Moscow State Technological University Stankin","affiliation-city":"Moscow","affiliation-country":"Russian Federation"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "6", "$" :"6"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57204649388","authid":"57204649388","authname":"Zelenskii A.","surname":"Zelenskii","given-name":"A.","initials":"A.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56157301500","authid":"56157301500","authname":"Gapon N.","surname":"Gapon","given-name":"N.","initials":"N.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/55802371401","authid":"55802371401","authname":"Voronin V.","surname":"Voronin","given-name":"V.","initials":"V.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/56572834900","authid":"56572834900","authname":"Semenishchev E.","surname":"Semenishchev","given-name":"E.","initials":"E.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/57201879038","authid":"57201879038","authname":"Serebrenny V.","surname":"Serebrenny","given-name":"V.","initials":"V.","afid": [{"@_fa": "true", "$" :"60033469"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/57215374719","authid":"57215374719","authname":"Cen Y.","surname":"Cen","given-name":"Y.","initials":"Y.","afid": [{"@_fa": "true", "$" :"60022381"}]}],"authkeywords":"Anisotropic gradient | Depth image | DQFT | Image inpainting | Neural network | Quaternion | Robot navigation | Saliency map | SLAM","article-number":"118700H","source-id":"40067","fund-acr":"RSF","fund-no":"undefined","fund-sponsor":"Russian Science Foundation","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118662222"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118662222?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118662222&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118662222&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118662222","dc:identifier":"SCOPUS_ID:85118662222","eid":"2-s2.0-85118662222","dc:title":"Transform-based quality assessment for enhanced image","dc:creator":"Voronin V.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:eIssn":"1996756X","prism:isbn": [{"@_fa": "true", "$" :"9781510645226"}],"prism:volume":"11842","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1117/12.2598321","dc:description":"Images captured in bad weather suffer from low contrast and faint color. Recently, plenty of enhanced algorithms have been proposed to improve visibility and restore color. The goal of image quality assessment is to predict the perceptual quality for improving imaging systems' performance. We proposed a no-reference image quality enhancement measure using hypercomplex Fourier transform for color images. The main idea is that enhancing the contrast of an image would create more high-frequency content in the enhanced image than the original image. An increase in the magnitude of higher frequency coefficients indicates an enhancement in contrast to the image's luminance content. To test the performance of the proposed algorithm, the public database TID2013 is used. The Pearson rank-ordered correlation coefficient is utilized to measure and compare the proposed quality measure's performance with state-of-the-art approaches.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60022381","afid":"60022381","affilname":"Beijing Jiaotong University","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60018011","afid":"60018011","affilname":"Donskoj Gosudarstvennyj Tehniceskij Universitet","affiliation-city":"Rostov-on-Don","affiliation-country":"Russian Federation"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60014196","afid":"60014196","affilname":"Moscow State Technological University Stankin","affiliation-city":"Moscow","affiliation-country":"Russian Federation"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60007033","afid":"60007033","affilname":"City University of New York","affiliation-city":"New York","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "6", "$" :"6"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55802371401","authid":"55802371401","authname":"Voronin V.","surname":"Voronin","given-name":"V.","initials":"V.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56572834900","authid":"56572834900","authname":"Semenishchev E.","surname":"Semenishchev","given-name":"E.","initials":"E.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57204649388","authid":"57204649388","authname":"Zelensky A.","surname":"Zelensky","given-name":"A.","initials":"A.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57197804464","authid":"57197804464","authname":"Tokareva O.","surname":"Tokareva","given-name":"O.","initials":"O.","afid": [{"@_fa": "true", "$" :"60018011"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/57215374719","authid":"57215374719","authname":"Cen Y.","surname":"Cen","given-name":"Y.","initials":"Y.","afid": [{"@_fa": "true", "$" :"60022381"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/35321805400","authid":"35321805400","authname":"Agaian S.","surname":"Agaian","given-name":"S.","initials":"S.","afid": [{"@_fa": "true", "$" :"60007033"}]}],"authkeywords":"Enhanced image | Frequency-domain transform | No-reference (NR) image quality | Quality measure","article-number":"118422G","source-id":"40067","fund-acr":"NSFC","fund-no":"20-57-53012","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118322667"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118322667?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118322667&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118322667&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118322667","dc:identifier":"SCOPUS_ID:85118322667","eid":"2-s2.0-85118322667","dc:title":"Temporal-Consistency-Aware Video Color Transfer","dc:creator":"Liu S.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030890285"}],"prism:volume":"13002 LNCS","prism:pageRange":"464-476","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-89029-2_36","dc:description":"This paper proposes a new temporal-consistency-aware color transfer method based on quaternion distance metric. Compared with the state-of-the-art methods, our method can keep the temporal consistency and better reduce the artifacts. Firstly, keyframes are extracted from the source video and transfer the color from the reference image through soft segmentation based on Gaussian Mixture Models (GMM). Then a quaternion-based method is proposed to transfer color from keyframes to the other frames iteratively. Specifically, this method analyses the color information of each pixel along five directions to detect its best matching pixel through a quaternion-based distance metric. Additionally, considering the accumulating errors in frame sequences, an effective abnormal color correction mechanism is designed to improve the color transfer quality. A quantitative evaluation metric is further proposed to measure the temporal consistency in the output video. Various experimental results validate the effectiveness of our method.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019533","afid":"60019533","affilname":"Tianjin University","affiliation-city":"Tianjin","affiliation-country":"China"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/36621189700","authid":"36621189700","authname":"Liu S.","surname":"Liu","given-name":"Shiguang","initials":"S.","afid": [{"@_fa": "true", "$" :"60019533"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56811785500","authid":"56811785500","authname":"Zhang Y.","surname":"Zhang","given-name":"Yu","initials":"Y.","afid": [{"@_fa": "true", "$" :"60019533"}]}],"authkeywords":"Color transfer | Quaternion | Temporal consistency | Video","source-id":"25674","fund-acr":"NSFC","fund-no":"61672375","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118179500"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118179500?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118179500&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118179500&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118179500","dc:identifier":"SCOPUS_ID:85118179500","eid":"2-s2.0-85118179500","dc:title":"DWT-DQFT-Based Color Image Blind Watermark with QR Decomposition","dc:creator":"Qin L.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030891367"}],"prism:volume":"13005 LNCS","prism:pageRange":"214-224","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-89137-4_15","dc:description":"In order to improve the performance of color image digital watermarking, a watermarking algorithm based on integration of Discrete Wavelet Transform (DWT) and Discrete Quaternion Fourier Transform (DQFT) combined with QR matrix decomposition is proposed. The three channels of a color image are processed as a whole to embed watermark information. Experimental results show that the watermarking algorithm has good robustness against attacks such as JPEG compression, cropping, and median filtering.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60016835","afid":"60016835","affilname":"Beijing Institute of Technology","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57315104600","authid":"57315104600","authname":"Qin L.","surname":"Qin","given-name":"Liangcheng","initials":"L.","afid": [{"@_fa": "true", "$" :"60016835"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56349334300","authid":"56349334300","authname":"Ma L.","surname":"Ma","given-name":"Ling","initials":"L.","afid": [{"@_fa": "true", "$" :"60016835"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57314271500","authid":"57314271500","authname":"Fu X.","surname":"Fu","given-name":"Xiongjun","initials":"X.","afid": [{"@_fa": "true", "$" :"60016835"}]}],"authkeywords":"Blind watermark | DQFT transform | DWT transform | QR decomposition | Quaternion","source-id":"25674","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118137434"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85118137434?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85118137434&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118137434&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85118137434","dc:identifier":"SCOPUS_ID:85118137434","eid":"2-s2.0-85118137434","dc:title":"5th International Conference on Advances in Computing and Data Sciences, ICACDS 2021","prism:publicationName":"Communications in Computer and Information Science","prism:issn":"18650929","prism:eIssn":"18650937","prism:isbn": [{"@_fa": "true", "$" :"9783030882433"}],"prism:volume":"1441","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","dc:description":"The proceedings contain 38 papers. The special focus in this conference is on Advances in Computing and Data Sciences. The topics include: Recognition of Isolated Gestures for Indian Sign Language Using Transfer Learning; a Study of Five Models Based on Non-clinical Data for the Prediction of Diabetes Onset in Medically Under-Served Populations; representation and Visualization of Students’ Progress Data Through Learning Dashboard; denoising of Computed Tomography Images for Improved Performance of Medical Devices in Biomedical Engineering; image Dehazing Through Dark Channel Prior and Color Attenuation Prior; predicting the Death of Road Accidents in Bangladesh Using Machine Learning Algorithms; numerical Computation of Finite Quaternion Mellin Transform Using a New Algorithm; predictive Modeling of Tandem Silicon Solar Cell for Calculating Efficiency; Text Summarization of an Article Extracted from Wikipedia Using NLTK Library; handling Class Imbalance in Electroencephalography Data Using Synthetic Minority Oversampling Technique; grapheme to Phoneme Mapping for Tamil Language; comparative Study of Physiological Signals from Empatica E4 Wristband for Stress Classification; an E-Commerce Prototype for Predicting the Product Return Phenomenon Using Optimization and Regression Techniques; crop Yield Prediction for India Using Regression Algorithms; a Novel Framework for Multimodal Twitter Sentiment Analysis Using Feature Learning; an Iterative Approach Based Reversible Data Hiding with Weight Update for Dual Stego Images; lower and Upper Bounds for ‘Useful’ Renyi Information Rate; sign Language Recognition Using Convolutional Neural Network; Prediction of Stock Price for Indian Stock Market: A Comparative Study Using LSTM and GRU; early Prediction of Cardiovascular Disease Among Young Adults Through Coronary Artery Calcium Score Technique.","citedby-count":"0","prism:aggregationType":"Book Series","subtype":"cr","subtypeDescription":"Conference Review","author-count":{"@limit": "100", "@total": "0"},"source-id":"17700155007","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85117772527"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85117772527?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117772527&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85117772527&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85117772527","dc:identifier":"SCOPUS_ID:85117772527","eid":"2-s2.0-85117772527","dc:title":"Automatic Model Determination for Quaternion NMF","dc:creator":"Sanchez G.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"152243-152249","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3120656","dc:description":"Nonnegative Matrix Factorization (NMF) is a well-known method for Blind Source Separation (BSS). Recently, BSS for polarized signals in spectropolarimetric data, containing both polarization and spectral information, was introduced. This information was encoded in 4-dimensional Stokes vectors represented by quaternion numbers. In the proposed Quaternion NMF (QNMF), the common challenge of determining the (usually) unknown number of quaternion signals remained unaddressed. Estimating the number of signals (aka model determination) is important, since an underestimation of this number results in poor source separation and omission of signals, while overestimation leads to extraction of noisy signals without physical meaning. Here, we introduce a method for determining the number of polarized signals in spectropolarimetric data, named QNMF k. QNMF k integrates: (a) Quaternion Alternating Direction Method of Multipliers (QADMM) implemented for QNMF, (b) random resampling of the initial quaternion data, and (c) custom clustering of sets of QADMM solutions with same number of sources, k, needed to estimate the stability of the solutions. The appropriate latent dimension is determined based on the stability of the solutions. We demonstrate that, without any prior information, QNMF k accurately extracts the correct number of signals used to generate synthetic quaternion datasets and a benchmark spectropolarimetric data.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60015206","afid":"60015206","affilname":"Florida International University","affiliation-city":"Miami","affiliation-country":"United States"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60006164","afid":"60006164","affilname":"Los Alamos National Laboratory","affiliation-city":"Los Alamos","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57310729000","authid":"57310729000","orcid":"0000-0002-8088-1391","authname":"Sanchez G.","surname":"Sanchez","given-name":"Giancarlo","initials":"G.","afid": [{"@_fa": "true", "$" :"60015206"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57189600869","authid":"57189600869","authname":"Skau E.","surname":"Skau","given-name":"Erik","initials":"E.","afid": [{"@_fa": "true", "$" :"60006164"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/24474131000","authid":"24474131000","orcid":"0000-0001-8636-4603","authname":"Alexandrov B.","surname":"Alexandrov","given-name":"Boian","initials":"B.","afid": [{"@_fa": "true", "$" :"60006164"}]}],"authkeywords":"Blind source separation | model determination | polarization | quaternion NMF | spectropolarimetric imaging","source-id":"21100374601","fund-acr":"LDRD","fund-no":"89233218CNA000001","fund-sponsor":"Laboratory Directed Research and Development","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85117464868"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85117464868?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85117464868&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85117464868&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85117464868","dc:identifier":"SCOPUS_ID:85117464868","eid":"2-s2.0-85117464868","dc:title":"Design of virtual drum system","dc:creator":"Huang H.C.","prism:publicationName":"\"Advances in Acoustics, Noise and Vibration - 2021\" Proceedings of the 27th International Congress on Sound and Vibration, ICSV 2021","prism:issn":"23293675","prism:isbn": [{"@_fa": "true", "$" :"9788378807995"}],"prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","dc:description":"This paper proposes an innovation design of the virtual musical instruments. The philosophy of the new design is to ensure a flawless instrument playing experience as the prerequisite that requires the accuracy of inertial measurement and data processing without lagging. The design of the virtual drum (sensor), for example, includes a nine degrees of freedom device that houses 3-axis accelerometer, 3-axis gyroscope, and 3-axis magnetometer, a mobile device that functions as a “server”, and an audio output device which can be also the mobile device itself. The handheld drum sensors can be mounted on the drumsticks during the performance. The workflow starts from the sensors detect player's gesture trajectories, the data are transited via Bluetooth 5.0 to the server, data fusion through proprietary algorithms before mapping to the correspondent audio signals and finally output. The advantages of this proposed virtual instrument design have not only the accurate inertial measurement tracking with low latency, but also it is lightweight, compact, and portability. Therefore, virtual musical experience can be played anytime, anywhere.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60004879","afid":"60004879","affilname":"Feng Chia University","affiliation-city":"Taichung","affiliation-country":"Taiwan"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57302862500","authid":"57302862500","authname":"Huang H.C.","surname":"Huang","given-name":"Hsiang Chen","initials":"H.C.","afid": [{"@_fa": "true", "$" :"60004879"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/34769598800","authid":"34769598800","authname":"Huang J.H.","surname":"Huang","given-name":"Jin H.","initials":"J.H.","afid": [{"@_fa": "true", "$" :"60004879"}]}],"authkeywords":"Complementary filter | Euler Angle | IMU (Inertial Measurement Unit) | Kalman filter | Quaternion rotation","source-id":"21101134460","fund-acr":"MOST","fund-no":"107-2221-E-035 -074 -MY3","fund-sponsor":"Ministry of Science and Technology, Taiwan","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85116940957"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85116940957?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116940957&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85116940957&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85116940957","dc:identifier":"SCOPUS_ID:85116940957","eid":"2-s2.0-85116940957","dc:title":"Color Multi-focus Image Fusion Using Quaternion Morphological Gradient and Improved KNN Matting","dc:creator":"Liu W.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030873547"}],"prism:volume":"12888 LNCS","prism:pageRange":"515-527","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-87355-4_43","dc:description":"Most of existing focus measures are calculated by using the luminance information of source images while the chrominance information are ignored. In this paper, we first propose a new focus measure called quaternion morphological gradient for extracting the saliency feature of color images, which is derived based on the quaternion representation of color images and a proper ranking function. Then, the quaternion morphological gradients are used to produce initial decision maps. After that, the final decision maps are estimated by using the globally optimal weight maps obtained by the improved KNN matting algorithm. Finally, a weighted-sum strategy is used to construct the fused image. To boost the robustness of matting results, the pseudo depth information of source image is added into the feature vector of KNN matting. The experimental results validate the superiority of our method compared with the state-of-the-art algorithms both in visual perception and objective metrics.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60108797","afid":"60108797","affilname":"Tongling University","affiliation-city":"Tongling","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60026316","afid":"60026316","affilname":"Institute of Intelligent Machines Chinese Academy of Sciences","affiliation-city":"Hefei","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60019118","afid":"60019118","affilname":"University of Science and Technology of China","affiliation-city":"Hefei","affiliation-country":"China"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56127197100","authid":"56127197100","authname":"Liu W.","surname":"Liu","given-name":"Wei","initials":"W.","afid": [{"@_fa": "true", "$" :"60026316"},{"@_fa": "true", "$" :"60019118"},{"@_fa": "true", "$" :"60108797"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57015911000","authid":"57015911000","authname":"Zheng Z.","surname":"Zheng","given-name":"Zhong","initials":"Z.","afid": [{"@_fa": "true", "$" :"60026316"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/35276043300","authid":"35276043300","authname":"Wang Z.","surname":"Wang","given-name":"Zengfu","initials":"Z.","afid": [{"@_fa": "true", "$" :"60026316"},{"@_fa": "true", "$" :"60019118"}]}],"authkeywords":"Color multi-focus image fusion | KNN matting | Quaternion morphological gradient","source-id":"25674","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85116468872"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85116468872?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85116468872&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85116468872&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85116468872","dc:identifier":"SCOPUS_ID:85116468872","eid":"2-s2.0-85116468872","dc:title":"EMDQ-SLAM: Real-Time High-Resolution Reconstruction of Soft Tissue Surface from Stereo Laparoscopy Videos","dc:creator":"Zhou H.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030872014"}],"prism:volume":"12904 LNCS","prism:pageRange":"331-340","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-87202-1_32","dc:description":"We propose a novel stereo laparoscopy video-based non-rigid SLAM method called EMDQ-SLAM, which can incrementally reconstruct thee-dimensional (3D) models of soft tissue surfaces in real-time and preserve high-resolution color textures. EMDQ-SLAM uses the expectation maximization and dual quaternion (EMDQ) algorithm combined with SURF features to track the camera motion and estimate tissue deformation between video frames. To overcome the problem of accumulative errors over time, we have integrated a g2o-based graph optimization method that combines the EMDQ mismatch removal and as-rigid-as-possible (ARAP) smoothing methods. Finally, the multi-band blending (MBB) algorithm has been used to obtain high resolution color textures with real-time performance. Experimental results demonstrate that our method outperforms two state-of-the-art non-rigid SLAM methods: MISSLAM and DefSLAM. Quantitative evaluation shows an average error in the range of 0.8–2.2 mm for different cases.","citedby-count":"6","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60002746","afid":"60002746","affilname":"Harvard Medical School","affiliation-city":"Boston","affiliation-country":"United States"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55365601600","authid":"55365601600","authname":"Zhou H.","surname":"Zhou","given-name":"Haoyin","initials":"H.","afid": [{"@_fa": "true", "$" :"60002746"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/15725486800","authid":"15725486800","authname":"Jayender J.","surname":"Jayender","given-name":"Jagadeesan","initials":"J.","afid": [{"@_fa": "true", "$" :"60002746"}]}],"authkeywords":"EMDQ | g2o-based graph optimization | GPU parallel computation | High resolution texture | Multi-band blending | Non-rigid SLAM","source-id":"25674","fund-acr":"NIH","fund-no":"K99EB027177","fund-sponsor":"National Institutes of Health","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115870718"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115870718?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115870718&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85115870718&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85115870718","dc:identifier":"SCOPUS_ID:85115870718","eid":"2-s2.0-85115870718","dc:title":"Track and Field Head Posture Error Correction System Based on Deep Reinforcement Learning","dc:creator":"Er-wei L.","prism:publicationName":"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","prism:issn":"18678211","prism:eIssn":"1867822X","prism:isbn": [{"@_fa": "true", "$" :"9783030825645"}],"prism:volume":"388","prism:pageRange":"362-372","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-82565-2_30","dc:description":"The problem that track and field athletes generally have non-standard postures in their playing actions, a track and field head posture error correction system based on deep reinforcement learning is designed. By optimizing the system configuration, improving the recognition accuracy, using deep reinforcement learning technology to obtain 3D deep dynamic image data of track and field sports, converting the data into quaternion format, storing the data file in VBH format, and shaping the data through deep reinforcement learning technology a dynamic three-dimensional model is used to judge whether the track and field posture is standard using the Euclidean distance comparison method. Using the powerful learning ability of deep reinforcement learning, a series of non-linear operations are performed on the input face image, the abstract features in the image are extracted layer by layer, and then the extracted features are used for classification and recognition and error correction. Finally, through actual research, the standardization of the track and field head attitude error correction system based on deep reinforcement learning is proved. The experimental results show that this method effectively improves the accuracy of attitude estimation.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/113465098","afid":"113465098","affilname":"Chongqing Vocational College of Transportation","affiliation-city":"Chongqing","affiliation-country":"China"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57275085800","authid":"57275085800","authname":"Er-wei L.","surname":"Er-wei","given-name":"Liu","initials":"L.","afid": [{"@_fa": "true", "$" :"113465098"}]}],"authkeywords":"Deep strong chemistry | Error correction | Head posture | Track and field","source-id":"21100220348","fund-no":"undefined","openaccess":"0","openaccessFlag":false}]}}