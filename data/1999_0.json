{"search-results":{"opensearch:totalResults":"18","opensearch:startIndex":"0","opensearch:itemsPerPage":"18","opensearch:Query":{"@role": "request", "@searchTerms": "TITLE-ABS-KEY ( hypercomplex OR hyper-complex OR hipercomplex OR quaternions OR octonions OR quaternionic ) AND TITLE-ABS-KEY ( signal OR image )", "@startPage": "0"},"link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=1999&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "first", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=1999&sort=+pubyear&view=complete", "@type": "application/json"}],"entry": [{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033345782"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033345782?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033345782&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033345782&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033345782","dc:identifier":"SCOPUS_ID:0033345782","eid":"2-s2.0-0033345782","dc:title":"Hypercomplex auto- and cross-correlation of color images","dc:creator":"Sangwine S.","prism:publicationName":"IEEE International Conference on Image Processing","prism:volume":"4","prism:pageRange":"319-322","prism:coverDate":"1999-12-01","prism:coverDisplayDate":"1999","dc:description":"Autocorrelation and cross-correlation have been defined and utilized in signal and image processing for many years, but not for color or vector images. In this paper we present for the first time a definition of correlation applicable to color images, based on quaternions or hypercomplex numbers. We have devised a visualization of the result using the polar form of a quaternion in which color denotes quaternion eigenaxis and phase, and a grayscale image represents the modulus.","citedby-count":"73","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60012197","afid":"60012197","affilname":"University of Reading","affiliation-city":"Reading","affiliation-country":"United Kingdom"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7003346836","authid":"7003346836","authname":"Sangwine S.","surname":"Sangwine","given-name":"Stephen J.","initials":"S.J.","afid": [{"@_fa": "true", "$" :"60012197"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6603651371","authid":"6603651371","authname":"Ell T.","surname":"Ell","given-name":"Todd A.","initials":"T.A.","afid": [{"@_fa": "true", "$" :"60012197"}]}],"source-id":"92868","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033330497"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033330497?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033330497&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033330497&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033330497","dc:identifier":"SCOPUS_ID:0033330497","eid":"2-s2.0-0033330497","dc:title":"Properties of three dimensional vector autoregressive model","dc:creator":"Fujiki J.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:volume":"3811","prism:pageRange":"224-235","prism:coverDate":"1999-12-01","prism:coverDisplayDate":"1999","dc:description":"The invariance and covariance of extracted features from an object under certain transformation play quite important roles in the fields of pattern recognition and image understanding. For instance, in order to recognize a three dimensional (3D) object, we need specific features extracted from a given object. These features should be independent of the pose and the location of an object. To extract such feature, One of the authors has presented the 3D vector autoregressive (VAR) model. This 3D VAR model is constructed on the quaternion, which is the basis of SU (2) (the rotation group in two dimensional complex space). Then the 3D VAR model is defined by the external products of 3D sequential data and the autoregressive (AR) coefficients, unlike the conventional AR models. Therefore the 3D VAR model has some prominent features. For example, The AR coefficients of the 3D VAR model behave like vectors under any three dimensional rotation. In this paper, we present the recursive computation of 2D VAR coefficients and 3D VAR coefficients. This method reduce the cost of computation of of VAR coefficients. We also define the partial correlation (PARCOR) vectors for the 2D VAR model and 3D VAR model from the point of view of data compression and pattern recognition.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60031701","afid":"60031701","affilname":"Electrotechnical Laboratory","affiliation-city":"Tsukuba","affiliation-country":"Japan"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6602444907","authid":"6602444907","authname":"Fujiki J.","surname":"Fujiki","given-name":"Jun","initials":"J.","afid": [{"@_fa": "true", "$" :"60031701"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7408424726","authid":"7408424726","authname":"Tanaka M.","surname":"Tanaka","given-name":"Masaru","initials":"M.","afid": [{"@_fa": "true", "$" :"60031701"}]}],"source-id":"40067","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033283755"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033283755?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033283755&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033283755&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033283755","dc:identifier":"SCOPUS_ID:0033283755","eid":"2-s2.0-0033283755","dc:title":"Rigidity analysis from range image data for automatic inspection of filter components","dc:creator":"Rodrigues M.A.","prism:publicationName":"IEEE Symposium on Emerging Technologies and Factory Automation, ETFA","prism:volume":"1","prism:pageRange":"583-591","prism:coverDate":"1999-12-01","prism:coverDisplayDate":"1999","dc:description":"Recent developments in computer vision and pattern recognition have enabled the development of sophisticated vision-based quality control systems for automatic inspection. In this paper we present a new geometrical analysis of rigid body transformation parameters from properties of reflected correspondence vectors. Based on this analysis, we propose a novel algorithm to calibrate transformation parameters of 3D objects in a fast production line of filter components. The algorithm performs a rigidity analysis from range images acquired from two cameras, making full use of distance and angle information providing a closed form solution to all parameters of interest. The method is used in conjunction with a decision support system where components falling outside specified thresholds are to be rejected. For a comparative study of algorithm performance, we also implemented a well known procedure for rigidity analysis based on quaternions. Experimental results demonstrate that our novel algorithm has a number of advantages over the quaternion method and that its performance is superior or similar to the quaternion method.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60030469","afid":"60030469","affilname":"University of Hull","affiliation-city":"Hull","affiliation-country":"United Kingdom"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7202521514","authid":"7202521514","authname":"Rodrigues M.A.","surname":"Rodrigues","given-name":"M. A.","initials":"M.A.","afid": [{"@_fa": "true", "$" :"60030469"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7410228990","authid":"7410228990","authname":"Liu Y.","surname":"Liu","given-name":"Y.","initials":"Y.","afid": [{"@_fa": "true", "$" :"60030469"}]}],"source-id":"21100275520","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033076040"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033076040?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033076040&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033076040&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033076040","dc:identifier":"SCOPUS_ID:0033076040","eid":"2-s2.0-0033076040","dc:title":"Focal-plane analog vlsi cellular implementation of the boundary contour system","dc:creator":"Cauwenberghs G.","prism:publicationName":"IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications","prism:issn":"10577122","prism:volume":"46","prism:issueIdentifier":"2","prism:pageRange":"327-334","prism:coverDate":"1999-12-01","prism:coverDisplayDate":"1999","prism:doi":"10.1109/81.747215","pii":"S1057712299020735","dc:description":"We present an analog very large scale integration (VLSI) cellular architecture implementing a version of the boundary contour system (BCS) for real-time focal-plane image processing. Inspired by neuromorphic models across the retina and several layers of visual cortex, the design integrates in each pixel the functions of phototransduction and simple cells, complex cells, hypercomplex cells, and bipole cells in each of three directions interconnected on a hexagonal grid. Analog currentmode complementary metal-oxide-semiconductor (CMOS) circuits are used throughout to perform edge detection, local inhibition, directionally selective long-range diffusive kernels, and renormalizing global gain control. Experimental results from a fabricated 12 × 10 pixel prototype in a 1.2-fim CMOS process are included, demonstrating the robustness of the implemented BCS model in selecting image contours in a cluttered and noisy background. ©1999 IEEE.","citedby-count":"14","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60005248","afid":"60005248","affilname":"Johns Hopkins University","affiliation-city":"Baltimore","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7006056098","authid":"7006056098","authname":"Cauwenberghs G.","surname":"Cauwenberghs","given-name":"Gert","initials":"G.","afid": [{"@_fa": "true", "$" :"60005248"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006481919","authid":"7006481919","authname":"Waskiewicz J.","surname":"Waskiewicz","given-name":"James","initials":"J.","afid": [{"@_fa": "true", "$" :"60005248"}]}],"authkeywords":"Active pixel sensors | Analog vlsi | Boundary segmentation | Cellular neural networks (cnn's) | Focal-plane image processing | Neuromorphic engineering","source-id":"26028","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032652877"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032652877?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032652877&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032652877&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032652877","dc:identifier":"SCOPUS_ID:0032652877","eid":"2-s2.0-0032652877","dc:title":"Block-coded modulation using two-level group codes over generalized quaternion groups","dc:creator":"Selvakumaran T.V.","prism:publicationName":"IEEE Transactions on Information Theory","prism:issn":"00189448","prism:volume":"45","prism:issueIdentifier":"1","prism:pageRange":"365-372","prism:coverDate":"1999-12-01","prism:coverDisplayDate":"1999","prism:doi":"10.1109/18.746847","pii":"S0018944899000668","dc:description":"A length n group code over a group G is a subgroup of Gn under component-wise group operation. Two-level group codes over the class of generalized quaternion groups, Q2m, m > 3, are constructed using a binary code and a code over Z2m -1, the ring of integers modulo 2m-1, as compOnent codes and a mapping / from Z2 × Z2m-1 to Q2m. A set of necessary and sufficient conditions on the component codes is derived which will give group codes over Q2m- Given the generator matrices of the component codes, the computational effort involved in checking the necessary and sufficient conditions is discussed. Starting from a four-dimensional signal set matched to C2m, it is shown that the Euclidean space codes obtained from the group codes over Q2m have Euclidean distance profiles which are independent of the coset representative selection involved in J. A closed-form expression for the minimum Euclidean distance of the resulting group codes over Q2mis obtained in terms of the Euclidean distances of the component codes. Finally, it is shown that all four-dimensional signal sets matched to C2m have the same Euclidean distance profile and hence the Euclidean space codes corresponding to each signal set for a given group code over Q2m are automorphic Euclidean-distance equivalent. © 1999 IEEE.","citedby-count":"4","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60032730","afid":"60032730","affilname":"Indian Institute of Technology Delhi","affiliation-city":"New Delhi","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60014097","afid":"60014097","affilname":"Indian Institute of Science","affiliation-city":"Bengaluru","affiliation-country":"India"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6508363908","authid":"6508363908","authname":"Selvakumaran T.V.","surname":"Selvakumaran","given-name":"T. V.","initials":"T.V.","afid": [{"@_fa": "true", "$" :"60032730"},{"@_fa": "true", "$" :"60014097"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/35617116700","authid":"35617116700","authname":"Sundar Rajan B.","surname":"Sundar Rajan","given-name":"B.","initials":"B.","afid": [{"@_fa": "true", "$" :"60032730"},{"@_fa": "true", "$" :"60014097"}]}],"authkeywords":"Coded modulation | Group codes | Multilevel construction","source-id":"15107","fund-acr":"NBHM","fund-no":"undefined","fund-sponsor":"National Board for Higher Mathematics","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032785382"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032785382?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032785382&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032785382&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S001048259900013X"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032785382","dc:identifier":"SCOPUS_ID:0032785382","eid":"2-s2.0-0032785382","dc:title":"Automated registration of multimodal brain image sets using computer vision methods","dc:creator":"Secretta G.","prism:publicationName":"Computers in Biology and Medicine","prism:issn":"00104825","prism:volume":"29","prism:issueIdentifier":"5","prism:pageRange":"333-359","prism:coverDate":"1999-09-01","prism:coverDisplayDate":"September 1999","prism:doi":"10.1016/S0010-4825(99)00013-X","pii":"S001048259900013X","dc:description":"We present a new method of registering three dimensional image volumes of the brain of a given patient acquired at different times or with different imaging modalities, or both. Registration is an essential requirement for fusing the data from the two image sets so as to either increase the available information by exploiting complementary imaging modalities, or to measure small changes over time for prognostication, disease assessment, etc. The new technique exploits an external, removable, remountable reference frame which is attached to the head. Computer vision techniques are used to determine the positions of fiducial marks in every image. The transformation required to map each image of one image volume onto the other image volume is developed using the theory of quaternions. The results indicate that the new technique is robust and practical in a clinical setting.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60015913","afid":"60015913","affilname":"Dalhousie University","affiliation-city":"Halifax","affiliation-country":"Canada"}],"pubmed-id":"10463798","prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6504264913","authid":"6504264913","authname":"Secretta G.","surname":"Secretta","given-name":"Gleb","initials":"G.","afid": [{"@_fa": "true", "$" :"60015913"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7005481096","authid":"7005481096","authname":"Gregson P.H.","surname":"Gregson","given-name":"Peter H.","initials":"P.H.","afid": [{"@_fa": "true", "$" :"60015913"}]}],"authkeywords":"3D imaging | Fiducial reference | Image registration | MRI | SPECT","source-id":"17957","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84957613261"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84957613261?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84957613261&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=84957613261&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/84957613261","dc:identifier":"SCOPUS_ID:84957613261","eid":"2-s2.0-84957613261","dc:title":"A novel approach to the 2D analytic signal","dc:creator":"Bülow T.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"3540663665"},{"@_fa": "true", "$" :"9783540663669"}],"prism:volume":"1689","prism:pageRange":"25-32","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","prism:doi":"10.1007/3-540-48375-6_4","dc:description":"The analytic signal of a real signal is a standard concept in 1D signal processing. However, the definition of an analytic signal for real 2D signals is not possible by a straightforward extension of the 1D definition. There rather occur several different approaches in the literature. We review the main approaches and propose a new definition which is based on the recently introduced quaternionic Fourier transform. The approach most closely related to ours is the one by Hahn [8], which defines the analytic signal, which he calls complex signal, to have a single quadrant spectrum. This approach suffers form the fact that the original real signal is not reconstructible from its complex signal. We show that this drawback is cured by replacing the complex frequency domain by the quaternionic frequency domain defined by the quaternionic Fourier transform. It is shown how the new definition comprises all the older ones. Experimental results demonstrate that the new definition of the analytic signal in 2D is superior to the older approaches.","citedby-count":"44","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60012345","afid":"60012345","affilname":"Christian-Albrechts-Universität zu Kiel","affiliation-city":"Kiel","affiliation-country":"Germany"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55879295100","authid":"55879295100","authname":"Bülow T.","surname":"Bülow","given-name":"Thomas","initials":"T.","afid": [{"@_fa": "true", "$" :"60012345"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7102542747","authid":"7102542747","authname":"Sommer G.","surname":"Sommer","given-name":"Gerald","initials":"G.","afid": [{"@_fa": "true", "$" :"60012345"}]}],"source-id":"25674","fund-no":"So-320/2-1","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84957374800"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84957374800?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84957374800&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=84957374800&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/84957374800","dc:identifier":"SCOPUS_ID:84957374800","eid":"2-s2.0-84957374800","dc:title":"Optimized fast algorithms for the quaternionic Fourier transform","dc:creator":"Felsberg M.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"3540663665"},{"@_fa": "true", "$" :"9783540663669"}],"prism:volume":"1689","prism:pageRange":"209-216","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","prism:doi":"10.1007/3-540-48375-6_26","dc:description":"In this article, we deal with fast algorithms for the quaternionic Fourier transform (QFT). Our aim is to give a guideline for choosing algorithms in practical cases. Hence, we are not only interested in the theoretic complexity but in the real execution time of the implementation of an algorithm. This includes floating point multiplications, additions, index computations and the memory accesses. We mainly consider two cases: the QFT of a real signal and the QFT of a quaternionic signal. For both cases it follows that the row-column method yields very fast algorithms. Additionally, these algorithms are easy to implement since one can fall back on standard algorithms for the fast Fourier transform and the fast Hartley transform. The latter is the optimal choice for real signals since there is no redundancy in the transform. We take advantage of the fact that each complete transform can be converted into another complete transform. In the case of the complex Fourier transform, the Hartley transform, and the QFT, the conversions are of low complexity. Hence, the QFT of a real signal is optimally calculated using the Hartley transform.","citedby-count":"10","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60012345","afid":"60012345","affilname":"Christian-Albrechts-Universität zu Kiel","affiliation-city":"Kiel","affiliation-country":"Germany"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/12775485600","authid":"12775485600","authname":"Felsberg M.","surname":"Felsberg","given-name":"Michael","initials":"M.","afid": [{"@_fa": "true", "$" :"60012345"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7102542747","authid":"7102542747","authname":"Sommer G.","surname":"Sommer","given-name":"Gerald","initials":"G.","afid": [{"@_fa": "true", "$" :"60012345"}]}],"source-id":"25674","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/24844442371"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/24844442371?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=24844442371&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=24844442371&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/24844442371","dc:identifier":"SCOPUS_ID:24844442371","eid":"2-s2.0-24844442371","dc:title":"Analog VLSI cellular implementation of the boundary contour system","dc:creator":"Cauwenberghs G.","prism:publicationName":"Advances in Neural Information Processing Systems","prism:issn":"10495258","prism:isbn": [{"@_fa": "true", "$" :"0262112450"},{"@_fa": "true", "$" :"9780262112451"}],"prism:pageRange":"657-663","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","dc:description":"We present an analog VLSI cellular architecture implementing a simplified version of the Boundary Contour System (BCS) for real-time image processing. Inspired by neuromorphic models across several layers of visual cortex, the design integrates in each pixel the functions of simple cells, complex cells, hyper-complex cells, and bipole cells, in three orientations interconnected on a hexagonal grid. Analog current-mode CMOS circuits are used throughout to perform edge detection, local inhibition, directionally selective long-range diffusive kernels, and renormal-izing global gain control. Experimental results from a fabricated 12×10 pixel prototype in 1.2 μm CMOS technology demonstrate the robustness of the architecture in selecting image contours in a cluttered and noisy background.","citedby-count":"5","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60005248","afid":"60005248","affilname":"Johns Hopkins University","affiliation-city":"Baltimore","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7006056098","authid":"7006056098","authname":"Cauwenberghs G.","surname":"Cauwenberghs","given-name":"Gert","initials":"G.","afid": [{"@_fa": "true", "$" :"60005248"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006481919","authid":"7006481919","authname":"Waskiewicz J.","surname":"Waskiewicz","given-name":"James","initials":"J.","afid": [{"@_fa": "true", "$" :"60005248"}]}],"source-id":"23669","fund-no":"N00014-95-1-0409","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033293378"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033293378?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033293378&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033293378&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033293378","dc:identifier":"SCOPUS_ID:0033293378","eid":"2-s2.0-0033293378","dc:title":"Algebraic motion approximation with nurbs iviotions and its application to spherical mechanism synthesis","dc:creator":"Jeffrey Ge Q.","prism:publicationName":"Journal of Mechanical Design, Transactions of the ASME","prism:issn":"10500472","prism:volume":"121","prism:issueIdentifier":"4","prism:pageRange":"529-532","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"December 1999","prism:doi":"10.1115/1.2829493","dc:description":"In this work we bring together classical mechanism theory with recent works in the area of Computer Aided Geometric Design (CAGD) of rational motions as well as curve approximation techniques in CAGD to study the problem of mechanism motion approximation from a computational geometric viewpoint. We present a framework for approximating algebraic motions of spherical mechanisms with rational B-Spline spherical motions. Algebraic spherical motions and rational B-spline spherical motions are represented as algebraic curves and rational B-Spline curves in the space of quaternions (or the image space). Thus the problem of motion approximation is transformed into a curve approximation problem, where concepts and techniques in the field of Computer Aided Geometric Design and Computational Geometry may be applied. An example is included at the end to show how a NURBS motion can be used for synthesizing spherical four-bar linkages. © 1999 by ASME.","citedby-count":"41","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60026415","afid":"60026415","affilname":"Stony Brook University","affiliation-city":"Stony Brook","affiliation-country":"United States"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60010339","afid":"60010339","affilname":"Florida Institute of Technology","affiliation-city":"Melbourne","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7101922139","authid":"7101922139","authname":"Jeffrey Ge Q.","surname":"Jeffrey Ge","given-name":"Q.","initials":"Q.","afid": [{"@_fa": "true", "$" :"60026415"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006350634","authid":"7006350634","authname":"Larochelle P.M.","surname":"Larochelle","given-name":"P. M.","initials":"P.M.","afid": [{"@_fa": "true", "$" :"60010339"}]}],"source-id":"20979","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033285896"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033285896?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033285896&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033285896&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033285896","dc:identifier":"SCOPUS_ID:0033285896","eid":"2-s2.0-0033285896","dc:title":"Representation issues in the ML estimation of camera motion","dc:creator":"Hornegger J.","prism:publicationName":"Proceedings of the IEEE International Conference on Computer Vision","prism:issn":"15505499","prism:volume":"1","prism:pageRange":"640-647","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","prism:doi":"10.1109/iccv.1999.791285","dc:description":"The computation of camera motion from image measurements is a parameter estimation problem. We show that for the analysis of the problem's sensitivity, the parametrization must enjoy the property of fairness, which makes sensitivity results invariant to changes of coordinates. We prove that Cartesian unit norm vectors and quaternions are fair parametrizations of rotations and translations, respectively, and that spherical coordinates and Euler angles are not. We extend the Gauss-Markov theorem to implicit formulations with constrained parameters, a necessary step in order to take advantage of fair parametrizations. We show how estimation problems whose sensitivity depends on a large number of parameters, such as coordinates of points in the scene, can be partitioned into equivalence classes, with problems in the same class exhibiting the same sensitivity.","citedby-count":"21","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60012708","afid":"60012708","affilname":"Stanford University","affiliation-city":"Palo Alto","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6603448080","authid":"6603448080","authname":"Hornegger J.","surname":"Hornegger","given-name":"J.","initials":"J.","afid": [{"@_fa": "true", "$" :"60012708"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7005719698","authid":"7005719698","authname":"Tomasi C.","surname":"Tomasi","given-name":"C.","initials":"C.","afid": [{"@_fa": "true", "$" :"60012708"}]}],"source-id":"110561","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033172459"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033172459?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033172459&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033172459&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S0921889099000147"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033172459","dc:identifier":"SCOPUS_ID:0033172459","eid":"2-s2.0-0033172459","dc:title":"Filtering in a unit quaternion space for model-based object tracking","dc:creator":"Ude A.","prism:publicationName":"Robotics and Autonomous Systems","prism:issn":"09218890","prism:volume":"28","prism:issueIdentifier":"2","prism:pageRange":"163-172","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"August 1999","prism:doi":"10.1016/S0921-8890(99)00014-7","pii":"S0921889099000147","dc:description":"The main idea in object tracking is to support the processing of incoming images by predicting future object's poses and features using the knowledge about the object's previous motion. In this paper we present a new method for the prediction and adjustment of motion parameters to the current measurements using the quaternion representation for the orientation and the Gauss-Newton iteration on the unit sphere S3. Unlike other trackers, our tracker searches for estimates directly in the space of rigid body motions (represented by R3 × S3) and takes into account the differential and geometric properties of this space. We present some results showing the successful tracking in synthetic and real image sequences.","citedby-count":"36","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60015682","afid":"60015682","affilname":"Japan Science and Technology Agency","affiliation-city":"Kawaguchi","affiliation-country":"Japan"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6701345288","authid":"6701345288","authname":"Ude A.","surname":"Ude","given-name":"Aleš","initials":"A.","afid": [{"@_fa": "true", "$" :"60015682"}]}],"source-id":"18079","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033055131"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0033055131?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0033055131&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0033055131&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0033055131","dc:identifier":"SCOPUS_ID:0033055131","eid":"2-s2.0-0033055131","dc:title":"Three-dimensional eye-head coordination during gaze saccades in the primate","dc:creator":"Crawford J.D.","prism:publicationName":"Journal of Neurophysiology","prism:issn":"00223077","prism:volume":"81","prism:issueIdentifier":"4","prism:pageRange":"1760-1782","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","prism:doi":"10.1152/jn.1999.81.4.1760","dc:description":"The purpose of this investigation was to describe the neural constraints on three-dimensional (3-D) orientations of the eye in space (Es), head in space (Hs), and eye in head (Eh) during visual fixations in the monkey and the control strategies used to implement these constraints during head-free gaze saccades. Dual scleral search coil signals were used to compute 3-D orientation quaternions, two-dimensional (2-D) direction vectors, and 3-D angular velocity vectors for both the eye and head in three monkeys during the following visual tasks: radial to/from center, repetitive horizontal, nonrepetitive oblique, random (wide 2-D range), and random with pin-hole goggles. Although 2-D gaze direction (of Es) was controlled more tightly than the contributing 2-D Hs and Eh components, the torsional standard deviation of Es was greater (mean 3.55°) than Hs (3.10°), which in turn was greater than Eh (1.87°) during random fixations. Thus the 3-D Es range appeared to be the byproduct of Hs and Eh constraints, resulting in a pseudoplanar Es range that was twisted (in orthogonal coordinates) like the zero torsion range of Fick coordinates. The Hs fixation range was similarly Fick-like, whereas the Eh fixation range was quasiplanar. The latter Eh range was maintained through exquisite saccade/slow phase coordination, i.e., during each head movement, multiple anticipatory saccades drove the eye torsionally out of the planar range such that subsequent slow phases drove the eye back toward the fixation range. The Fick-like Hs constraint was maintained by the following strategies: first, during purely vertical/horizontal movements, the head rotated about constantly oriented axes that closely resembled physical Fick gimbals, i.e., about head-fixed horizontal axes and space-fixed vertical axes, respectively (although in 1 animal, the latter constraint was relaxed during repetitive horizontal movements, allowing for trajectory optimization). However, during large oblique movements, head orientation made transient but dramatic departures from the zero-torsion Fick surface, taking the shortest path between two torsionally eccentric fixation points on the surface. Moreover, in the pin-hole goggle task, the head-orientation range flattened significantly, suggesting a task-dependent default strategy similar to Listing's law. These and previous observations suggest two quasi- independent brain stem circuits: an oculomotor 2-D to 3-D transformation that coordinates anticipatory saccades with slow phases to uphold Listing's law, and a flexible 'Fick operator' that selects head motor error; both nested within a dynamic gaze feedback loop.","citedby-count":"70","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60118086","afid":"60118086","affilname":"York University, Centre for Vision Research","affiliation-city":"Toronto","affiliation-country":"Canada"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60033420","afid":"60033420","affilname":"York University","affiliation-city":"Toronto","affiliation-country":"Canada"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60030616","afid":"60030616","affilname":"Institut-Hôpital Neurologique de Montréal","affiliation-city":"Montreal","affiliation-country":"Canada"}],"pubmed-id":"10200211","prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7401850867","authid":"7401850867","authname":"Crawford J.D.","surname":"Crawford","given-name":"J. Douglas","initials":"J.D.","afid": [{"@_fa": "true", "$" :"60118086"},{"@_fa": "true", "$" :"60033420"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7003726195","authid":"7003726195","authname":"Ceylan M.Z.","surname":"Ceylan","given-name":"Melike Z.","initials":"M.Z.","afid": [{"@_fa": "true", "$" :"60118086"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/7004258907","authid":"7004258907","authname":"Klier E.M.","surname":"Klier","given-name":"Eliana M.","initials":"E.M.","afid": [{"@_fa": "true", "$" :"60118086"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/7003372328","authid":"7003372328","authname":"Guitton D.","surname":"Guitton","given-name":"Daniel","initials":"D.","afid": [{"@_fa": "true", "$" :"60030616"}]}],"source-id":"23475","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032683102"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032683102?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032683102&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032683102&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032683102","dc:identifier":"SCOPUS_ID:0032683102","eid":"2-s2.0-0032683102","dc:title":"Computer-generated fractal kinetic images and its applications in security holograms","dc:creator":"Cao H.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:volume":"3637","prism:pageRange":"176-180","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","dc:description":"In this paper, we propose a new kind of fractal kinetic holograms using fractal hypercomplex model for security purpose. The proposed fractal kinetic holograms are consist of a sequence of computer-generated fractal images between which there are self-similarity. And on that basis, we can make `fractal animated holograms'. The results indicate that the model is quite efficient for synthesis of fractal kinetic images. The generated fractal kinetic holograms can be used in laser anti-counterfeiting field.","citedby-count":"9","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025761","afid":"60025761","affilname":"Huazhong University of Science and Technology","affiliation-city":"Wuhan","affiliation-country":"China"}],"prism:aggregationType":"Conference Proceeding","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "6", "$" :"6"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7403345988","authid":"7403345988","authname":"Cao H.","surname":"Cao","given-name":"Hanqiang","initials":"H.","afid": [{"@_fa": "true", "$" :"60025761"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7402633110","authid":"7402633110","authname":"Zhu G.","surname":"Zhu","given-name":"Guangxi","initials":"G.","afid": [{"@_fa": "true", "$" :"60025761"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/7406074423","authid":"7406074423","authname":"Zhu Y.","surname":"Zhu","given-name":"Yaoting","initials":"Y.","afid": [{"@_fa": "true", "$" :"60025761"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/15041331900","authid":"15041331900","authname":"Zhang Z.","surname":"Zhang","given-name":"Zhaoqun","initials":"Z.","afid": [{"@_fa": "true", "$" :"60025761"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/36795949000","authid":"36795949000","authname":"Ge H.","surname":"Ge","given-name":"Hongwei","initials":"H.","afid": [{"@_fa": "true", "$" :"60025761"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/7501712932","authid":"7501712932","authname":"Li X.","surname":"Li","given-name":"Xuan","initials":"X.","afid": [{"@_fa": "true", "$" :"60025761"}]}],"source-id":"40067","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032675794"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032675794?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032675794&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032675794&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032675794","dc:identifier":"SCOPUS_ID:0032675794","eid":"2-s2.0-0032675794","dc:title":"Optimal rigid motion estimation and performance evaluation with bootstrap","dc:creator":"Matei B.","prism:publicationName":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","prism:issn":"10636919","prism:volume":"1","prism:pageRange":"339-345","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","dc:description":"A new method for 3D rigid motion estimation is derived under the most general assumption that the measurements are corrupted by inhomogeneous and anisotropic, i.e., heteroscedastic noise. This is the case, for example, when the motion of a calibrated stereo-head is to be determined from image pairs. Linearization in the quaternion space transforms the problem into a multivariate, heteroscedastic errors-in-variables (HEIV) regression, from which the rotation and translation estimates are obtained simultaneously. The significant performance improvement is illustrated, for real data, by comparison with the results of quaternion, subspace and renormalization based approaches described in the literature. Extensive use is made of bootstrap, an advanced numerical tool from statistics, both to estimate the covariances of the 3D data points and to obtain confidence regions for the rotation and translation estimates. Bootstrap enables an accurate recovery of these information using only the two image pairs serving as input.","citedby-count":"46","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60119141","afid":"60119141","affilname":"Rutgers University–New Brunswick","affiliation-city":"New Brunswick","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/12544556200","authid":"12544556200","authname":"Matei B.","surname":"Matei","given-name":"Bogdan","initials":"B.","afid": [{"@_fa": "true", "$" :"60119141"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006570387","authid":"7006570387","authname":"Meer P.","surname":"Meer","given-name":"Peter","initials":"P.","afid": [{"@_fa": "true", "$" :"60119141"}]}],"source-id":"24212","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032667629"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032667629?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032667629&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032667629&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032667629","dc:identifier":"SCOPUS_ID:0032667629","eid":"2-s2.0-0032667629","dc:title":"Sensor system for the navigation of an underwater vehicle","dc:creator":"Smith R.","prism:publicationName":"International Journal of Robotics Research","prism:issn":"02783649","prism:volume":"18","prism:issueIdentifier":"7","prism:pageRange":"697-710","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"July 1999","prism:doi":"10.1177/02783649922066510","dc:description":"A sensor system for an underwater vehicle is described. The vehicle is equipped with inclinometers, gyroscopes, a magnetometer, a pressure gauge, and a sonar system. The sensor models used for the inclinometers and gyroscopes are straightforward; however, the magnetometer can be corrupted by variations in the earth's field caused by: external objects and internal magnetic fields. We show how to use inclinometer data to adjust for a limited set of external field variation. We also show how to calibrate the magnetometer to compensate for static and thruster-dependent internal fields. The sonar unit uses range differentials between cheap time-of-flight sonar to follow a target. This reduces signal processing since data association is only required on target acquisition, and removes the need to scan an entire landscape, which is usually slow. The gyroscopes are fused via a second indirect filter system. The vehicle attitude is represented as a quaternion; these have a low computational burden, and lack discontinuities and singularities. The simplicity of the indirect filter permits very fast update rates, so that the system may follow rapid vehicle rotations.","citedby-count":"17","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60026851","afid":"60026851","affilname":"University of Oxford","affiliation-city":"Oxford","affiliation-country":"United Kingdom"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60000695","afid":"60000695","affilname":"Silsoe Research Institute","affiliation-city":"Silsoe","affiliation-country":"United Kingdom"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57197526053","authid":"57197526053","authname":"Smith R.","surname":"Smith","given-name":"Robert","initials":"R.","afid": [{"@_fa": "true", "$" :"60026851"},{"@_fa": "true", "$" :"60000695"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7101612313","authid":"7101612313","authname":"Frost A.","surname":"Frost","given-name":"Andy","initials":"A.","afid": [{"@_fa": "true", "$" :"60026851"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56233155400","authid":"56233155400","authname":"Probert P.","surname":"Probert","given-name":"Penny","initials":"P.","afid": [{"@_fa": "true", "$" :"60000695"}]}],"source-id":"18050","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032636157"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032636157?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032636157&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032636157&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032636157","dc:identifier":"SCOPUS_ID:0032636157","eid":"2-s2.0-0032636157","dc:title":"Nonlinear neurons and higher-order statistics: new approaches to biological vision and digital image processing","dc:creator":"Zetzsche C.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:volume":"3644","prism:pageRange":"2-33","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","dc:description":"The classical approach in vision research - the derivation of basically linear filter models from experiments with simple artificial test stimuli - is currently undergoing a major revision. Instead of trying to keep the dirty environment out of our clean labs we put it now right into the focus of scientific exploration. The new approach has a close relation to basic engineering strategies for electronic image processing since its major concept is the exploitation of the statistical redundancies of the environment by appropriate neural transformations. The standard engineering methods are not sufficient, however. Even a basic biological feature like orientation selectivity requires the consideration of higher-order statistics, like cumulants or polyspectra. Furthermore, there exists an abundance of nonlinear phenomena in biological vision, for example the phase-invariance of complex cells, cortical gain control, or end-stopping, which make it necessary to consider unconventional modeling approaches like differential geometry or Volterra-Wiener systems. By use of such methods we cannot only gain a deeper understanding of the adaptation of the visual system to the complex natural environment, but we can also make the biological system an inspiring source for the design of novel strategies in electronic image processing.","citedby-count":"19","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60028717","afid":"60028717","affilname":"Ludwig-Maximilians-Universität München","affiliation-city":"Munich","affiliation-country":"Germany"}],"prism:aggregationType":"Conference Proceeding","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56021568900","authid":"56021568900","authname":"Zetzsche C.","surname":"Zetzsche","given-name":"Christoph","initials":"C.","afid": [{"@_fa": "true", "$" :"60028717"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7006079333","authid":"7006079333","authname":"Krieger G.","surname":"Krieger","given-name":"Gerhard","initials":"G.","afid": [{"@_fa": "true", "$" :"60028717"}]}],"source-id":"40067","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032635565"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/0032635565?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=0032635565&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=0032635565&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/0032635565","dc:identifier":"SCOPUS_ID:0032635565","eid":"2-s2.0-0032635565","dc:title":"Color image processing by using binary quaternion-moment-preserving thresholding technique","dc:creator":"Pei S.","prism:publicationName":"IEEE Transactions on Image Processing","prism:issn":"10577149","prism:volume":"8","prism:issueIdentifier":"5","prism:pageRange":"614-628","prism:coverDate":"1999-01-01","prism:coverDisplayDate":"1999","prism:doi":"10.1109/83.760310","dc:description":"This paper presents a new moment-preserving thresholding technique, called the binary quaternion-moment-preserving (BQMP) thresholding, for color image data. Based on representing color data by the quaternions, the statistical parameters of color data can be expressed through the definition of quaternion moments. Analytical formulas of the BQMP thresholding can thus be determined by using the algebra of the quaternions. The computation time for the BQMP thresholding is of order of data size. By using the BQMP thresholding, quaternion-moment-based operators are designed for the applications of color image processing, such as color image compression, multiclass clustering of color data, and subpixel color edge detection. The experimental results show that the proposed operator for color image compression can have output picture quality acceptable to human eyes. In addition, the proposed edge operator can detect color edge in subpixel level. Therefore, the proposed BQMP thresholding can be used as a tool for color image processing.","citedby-count":"143","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60072375","afid":"60072375","affilname":"Chunghwa Telecom Co. Ltd.","affiliation-city":"Yangmei","affiliation-country":"Taiwan"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60005429","afid":"60005429","affilname":"National Taiwan University","affiliation-city":"Taipei","affiliation-country":"Taiwan"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60000251","afid":"60000251","affilname":"IEEE","affiliation-city":"New York","affiliation-country":"United States"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/7202463605","authid":"7202463605","authname":"Pei S.","surname":"Pei","given-name":"Soo Chang","initials":"S.C.","afid": [{"@_fa": "true", "$" :"60005429"},{"@_fa": "true", "$" :"60000251"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57198497260","authid":"57198497260","authname":"Cheng C.","surname":"Cheng","given-name":"Ching Min","initials":"C.M.","afid": [{"@_fa": "true", "$" :"60072375"}]}],"source-id":"25534","fund-acr":"NSC","fund-no":"86-2221-E-002-018","fund-sponsor":"National Science Council","openaccess":"0","openaccessFlag":false}]}}