{"abstracts-retrieval-response": {
    "item": {
        "ait:process-info": {
            "ait:status": {
                "@state": "update",
                "@type": "core",
                "@stage": "S300"
            },
            "ait:date-delivered": {
                "@day": "01",
                "@timestamp": "2023-04-01T04:53:44.000044-04:00",
                "@year": "2023",
                "@month": "04"
            },
            "ait:date-sort": {
                "@day": "01",
                "@year": "2023",
                "@month": "01"
            }
        },
        "xocs:meta": {"xocs:funding-list": {
            "@pui-match": "primary",
            "@has-funding-info": "1",
            "xocs:funding": {
                "xocs:funding-agency-matched-string": "Sapienza University of Rome",
                "xocs:funding-agency": "Sapienza Università di Roma",
                "xocs:funding-id": "RG11916B88E1942F",
                "xocs:funding-agency-id": "http://data.elsevier.com/vocabulary/SciValFunders/501100004271",
                "xocs:funding-agency-country": "http://sws.geonames.org/3175395/"
            },
            "xocs:funding-addon-generated-timestamp": "2023-04-03T10:41:38.675189Z",
            "xocs:funding-text": "This work was supported by the \"Progetti di Ricerca Grandi\"of Sapienza University of Rome under Grant RG11916B88E1942F.",
            "xocs:funding-addon-type": "http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/aggregated-refined"
        }},
        "bibrecord": {
            "head": {
                "author-group": [
                    {
                        "affiliation": {
                            "country": "United Kingdom",
                            "address-part": "Northampton Square",
                            "postal-code": "EC1V 0HB",
                            "@afid": "60033387",
                            "@country": "gbr",
                            "city": "London",
                            "organization": [
                                {"$": "University of London"},
                                {"$": "Department of Computer Science"}
                            ],
                            "affiliation-id": {
                                "@afid": "60033387",
                                "@dptid": "104696623"
                            },
                            "@affiliation-instance-id": "OB2BibRecID-946469326-11bd795642827cff9c8e6e1b84ba82c4-1",
                            "ce:source-text": "University of London, Northampton Square, Department of Computer Science, City, London, U.K., EC1V 0HB",
                            "@dptid": "104696623"
                        },
                        "author": [
                            {
                                "ce:given-name": "Eric",
                                "preferred-name": {
                                    "ce:given-name": "Eric",
                                    "ce:initials": "E.",
                                    "ce:surname": "Guizzo",
                                    "ce:indexed-name": "Guizzo E."
                                },
                                "@author-instance-id": "OB2BibRecID-946469326-1c2536198c8edf4109a5563756241ee3-1",
                                "@seq": "1",
                                "ce:initials": "E.",
                                "@_fa": "true",
                                "@type": "auth",
                                "ce:surname": "Guizzo",
                                "@auid": "57210359479",
                                "@orcid": "0000-0002-2857-4140",
                                "ce:indexed-name": "Guizzo E."
                            },
                            {
                                "ce:given-name": "Tillman",
                                "preferred-name": {
                                    "ce:given-name": "Tillman",
                                    "ce:initials": "T.",
                                    "ce:surname": "Weyde",
                                    "ce:indexed-name": "Weyde T."
                                },
                                "@author-instance-id": "OB2BibRecID-946469326-0131555d838e121e8506cd69744c61c8-1",
                                "@seq": "2",
                                "ce:initials": "T.",
                                "@_fa": "true",
                                "@type": "auth",
                                "ce:surname": "Weyde",
                                "@auid": "24476899500",
                                "@orcid": "0000-0001-8028-9905",
                                "ce:indexed-name": "Weyde T."
                            }
                        ]
                    },
                    {
                        "affiliation": {
                            "country": "Italy",
                            "postal-code": "00184",
                            "@afid": "60032350",
                            "@country": "ita",
                            "city": "Rome",
                            "organization": [
                                {"$": "Sapienza University of Rome"},
                                {"$": "Department of Information Engineering"},
                                {"$": "Electronics and Telecommunications (DIET)"}
                            ],
                            "affiliation-id": {
                                "@afid": "60032350",
                                "@dptid": "113784148"
                            },
                            "@affiliation-instance-id": "OB2BibRecID-946469326-c0a15791d56ed07d48a5d3e481ca5bdb-1",
                            "ce:source-text": "Sapienza University of Rome, Department of Information Engineering, Electronics and Telecommunications (DIET), Rome, Italy, 00184",
                            "@dptid": "113784148"
                        },
                        "author": [
                            {
                                "ce:given-name": "Simone",
                                "preferred-name": {
                                    "ce:given-name": "Simone",
                                    "ce:initials": "S.",
                                    "ce:surname": "Scardapane",
                                    "ce:indexed-name": "Scardapane S."
                                },
                                "@author-instance-id": "OB2BibRecID-946469326-6b503356d434086f13782907c69fb0c0-1",
                                "@seq": "3",
                                "ce:initials": "S.",
                                "@_fa": "true",
                                "@type": "auth",
                                "ce:surname": "Scardapane",
                                "@auid": "55772102700",
                                "@orcid": "0000-0003-0881-8344",
                                "ce:indexed-name": "Scardapane S."
                            },
                            {
                                "ce:given-name": "Danilo",
                                "preferred-name": {
                                    "ce:given-name": "Danilo",
                                    "ce:initials": "D.",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                },
                                "@author-instance-id": "OB2BibRecID-946469326-fea6313056440d2001c1ab58ddb309fd-1",
                                "@seq": "4",
                                "ce:initials": "D.",
                                "@_fa": "true",
                                "@type": "auth",
                                "ce:surname": "Comminiello",
                                "@auid": "36444807900",
                                "@orcid": "0000-0003-4067-4504",
                                "ce:indexed-name": "Comminiello D."
                            }
                        ]
                    }
                ],
                "citation-title": "Learning Speech Emotion Representations in the Quaternion Domain",
                "abstracts": "© 2014 IEEE.The modeling of human emotion expression in speech signals is an important, yet challenging task. The high resource demand of speech emotion recognition models, combined with the general scarcity of emotion-labelled data are obstacles to the development and application of effective solutions in this field. In this paper, we present an approach to jointly circumvent these difficulties. Our method, named RH-emo, is a novel semi-supervised architecture aimed at extracting quaternion embeddings from real-valued monoaural spectrograms, enabling the use of quaternion-valued networks for speech emotion recognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that consists of a real-valued encoder in parallel to a real-valued emotion classifier and a quaternion-valued decoder. On the one hand, the classifier permits to optimization of each latent axis of the embeddings for the classification of a specific emotion-related characteristic: valence, arousal, dominance, and overall emotion. On the other hand, quaternion reconstruction enables the latent dimension to develop intra-channel correlations that are required for an effective representation as a quaternion entity. We test our approach on speech emotion recognition tasks using four popular datasets: IEMOCAP, RAVDESS, EmoDB, and TESS, comparing the performance of three well-established real-valued CNN architectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent fed with the embeddings created with RH-emo. We obtain a consistent improvement in the test accuracy for all datasets, while drastically reducing the resources' demand of models. Moreover, we performed additional experiments and ablation studies that confirm the effectiveness of our approach.",
                "correspondence": {
                    "affiliation": {
                        "country": "United Kingdom",
                        "address-part": "Northampton Square",
                        "postal-code": "EC1V 0HB",
                        "@country": "gbr",
                        "city": "London",
                        "organization": [
                            {"$": "University of London"},
                            {"$": "Department of Computer Science"}
                        ],
                        "@affiliation-instance-id": "OB2BibRecID-946469326-11bd795642827cff9c8e6e1b84ba82c4-1",
                        "ce:source-text": "University of London, Northampton Square, Department of Computer Science, City, London, U.K., EC1V 0HB"
                    },
                    "person": {
                        "ce:given-name": "Eric",
                        "@author-instance-id": "OB2BibRecID-946469326-d70d6e10f438215673e2505c82157321-1",
                        "ce:initials": "E.",
                        "ce:surname": "Guizzo",
                        "ce:indexed-name": "Guizzo E."
                    }
                },
                "citation-info": {
                    "author-keywords": {"author-keyword": [
                        {
                            "$": "quaternion algebra",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "quaternion neural networks",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Speech emotion recognition",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "transferable embeddings",
                            "@xml:lang": "eng",
                            "@original": "y"
                        }
                    ]},
                    "citation-type": {"@code": "ar"},
                    "citation-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    },
                    "abstract-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    }
                },
                "source": {
                    "sourcetitle-abbrev": "IEEE ACM Trans. Audio Speech Lang. Process.",
                    "website": {"ce:e-address": {
                        "$": "http://ieeexplore.ieee.org/servlet/opac?punumber=6570655",
                        "@type": "email"
                    }},
                    "@country": "usa",
                    "translated-sourcetitle": {"@xml:lang": "eng"},
                    "issn": [
                        {
                            "$": "23299304",
                            "@type": "electronic"
                        },
                        {
                            "$": "23299290",
                            "@type": "print"
                        }
                    ],
                    "volisspag": {
                        "voliss": {"@volume": "31"},
                        "pagerange": {
                            "@first": "1200",
                            "@last": "1212"
                        }
                    },
                    "@type": "j",
                    "publicationyear": {"@first": "2023"},
                    "publisher": {"publishername": "Institute of Electrical and Electronics Engineers Inc."},
                    "sourcetitle": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                    "@srcid": "21100368801",
                    "publicationdate": {
                        "year": "2023",
                        "date-text": {
                            "@xfab-added": "true",
                            "$": "2023"
                        }
                    }
                },
                "enhancement": {"classificationgroup": {"classifications": [
                    {
                        "@type": "CPXCLASS",
                        "classification": [
                            {
                                "classification-code": "723.2",
                                "classification-description": "Data Processing and Image Processing"
                            },
                            {
                                "classification-code": "751.5",
                                "classification-description": "Speech"
                            }
                        ]
                    },
                    {
                        "@type": "FLXCLASS",
                        "classification": {
                            "classification-code": "902",
                            "classification-description": "FLUIDEX; Related Topics"
                        }
                    },
                    {
                        "@type": "ASJC",
                        "classification": [
                            {"$": "1701"},
                            {"$": "3102"},
                            {"$": "2605"},
                            {"$": "2208"}
                        ]
                    },
                    {
                        "@type": "SUBJABBR",
                        "classification": [
                            {"$": "COMP"},
                            {"$": "PHYS"},
                            {"$": "MATH"},
                            {"$": "ENGI"}
                        ]
                    }
                ]}},
                "grantlist": {
                    "@complete": "y",
                    "grant-text": {
                        "$": "This work was supported by the \"Progetti di Ricerca Grandi\"of Sapienza University of Rome under Grant RG11916B88E1942F.",
                        "@xml:lang": "eng"
                    },
                    "grant": {
                        "grant-id": "RG11916B88E1942F",
                        "grant-agency": {
                            "@iso-code": "ita",
                            "$": "Sapienza Università di Roma"
                        },
                        "grant-agency-id": "501100004271"
                    }
                }
            },
            "item-info": {
                "copyright": {
                    "$": "Copyright 2023 Elsevier B.V., All rights reserved.",
                    "@type": "Elsevier"
                },
                "dbcollection": [
                    {"$": "CPX"},
                    {"$": "REAXYSCAR"},
                    {"$": "SCOPUS"},
                    {"$": "Scopusbase"}
                ],
                "history": {"date-created": {
                    "@day": "31",
                    "@timestamp": "BST 12:25:12",
                    "@year": "2023",
                    "@month": "03"
                }},
                "itemidlist": {
                    "itemid": [
                        {
                            "$": "2023205244",
                            "@idtype": "PUI"
                        },
                        {
                            "$": "946469326",
                            "@idtype": "CAR-ID"
                        },
                        {
                            "$": "20231013688290",
                            "@idtype": "CPX"
                        },
                        {
                            "$": "20230616560",
                            "@idtype": "REAXYSCAR"
                        },
                        {
                            "$": "20230847278",
                            "@idtype": "SCOPUS"
                        },
                        {
                            "$": "20210017751083",
                            "@idtype": "TPA-ID"
                        },
                        {
                            "$": "85149423370",
                            "@idtype": "SCP"
                        },
                        {
                            "$": "85149423370",
                            "@idtype": "SGR"
                        }
                    ],
                    "ce:doi": "10.1109/TASLP.2023.3250840"
                }
            },
            "tail": {"bibliography": {
                "@refcount": "75",
                "reference": [
                    {
                        "ref-fulltext": "J. Rybka and A. Janicki, \u201cComparison of speaker dependent and speaker independent emotion recognition,\u201d Int. J. Appl. Math. Comput. Sci., vol. 23, no. 4, pp. 797\u2013808, 2013.",
                        "@reference-instance-id": "OB2BibRecID-946469326-ad9ae70c7598c4903b96cba555a8542d-1",
                        "@id": "1",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2013"},
                            "ref-title": {"ref-titletext": "Comparison of speaker dependent and speaker independent emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84893804735",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "23",
                                    "@issue": "4"
                                },
                                "pagerange": {
                                    "@first": "797",
                                    "@last": "808"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Rybka",
                                    "ce:indexed-name": "Rybka J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Janicki",
                                    "ce:indexed-name": "Janicki A."
                                }
                            ]},
                            "ref-sourcetitle": "Int. J. Appl. Math. Comput. Sci."
                        },
                        "ce:source-text": "J. Rybka and A. Janicki, \u201cComparison of speaker dependent and speaker independent emotion recognition,\u201d Int. J. Appl. Math. Comput. Sci., vol. 23, no. 4, pp. 797\u2013808, 2013."
                    },
                    {
                        "ref-fulltext": "V. Hozjan and Z. Kačič, \u201cContext-independent multilingual emotion recognition from speech signals,\u201d Int. J. Speech Techn., vol. 6, no. 3, pp. 311\u2013320, 2003.",
                        "@reference-instance-id": "OB2BibRecID-946469326-76a0f52de3065207d918146bbc899fd8-2",
                        "@id": "2",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2003"},
                            "ref-title": {"ref-titletext": "Context-independent multilingual emotion recognition from speech signals"},
                            "refd-itemidlist": {"itemid": {
                                "$": "0038277026",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "6",
                                    "@issue": "3"
                                },
                                "pagerange": {
                                    "@first": "311",
                                    "@last": "320"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "V.",
                                    "@_fa": "true",
                                    "ce:surname": "Hozjan",
                                    "ce:indexed-name": "Hozjan V."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Kačič",
                                    "ce:indexed-name": "Kacic Z."
                                }
                            ]},
                            "ref-sourcetitle": "Int. J. Speech Techn."
                        },
                        "ce:source-text": "V. Hozjan and Z. Kačič, \u201cContext-independent multilingual emotion recognition from speech signals,\u201d Int. J. Speech Techn., vol. 6, no. 3, pp. 311\u2013320, 2003."
                    },
                    {
                        "ref-fulltext": "S. Rigoulot, E. Wassiliwizky, and M. D. Pell, \u201cFeeling backwards? How temporal order in speech affects the time course of vocal emotion recognition,\u201d Front. Psychol., vol. 4, no. 367, pp. 1\u201314, 2013.",
                        "@reference-instance-id": "OB2BibRecID-946469326-7934e187031bbec15f768a4a4cec14c4-3",
                        "@id": "3",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2013"},
                            "ref-title": {"ref-titletext": "Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84904543081",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "4",
                                    "@issue": "367"
                                },
                                "pagerange": {
                                    "@first": "1",
                                    "@last": "14"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Rigoulot",
                                    "ce:indexed-name": "Rigoulot S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Wassiliwizky",
                                    "ce:indexed-name": "Wassiliwizky E."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "M.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Pell",
                                    "ce:indexed-name": "Pell M.D."
                                }
                            ]},
                            "ref-sourcetitle": "Front. Psychol."
                        },
                        "ce:source-text": "S. Rigoulot, E. Wassiliwizky, and M. D. Pell, \u201cFeeling backwards? How temporal order in speech affects the time course of vocal emotion recognition,\u201d Front. Psychol., vol. 4, no. 367, pp. 1\u201314, 2013."
                    },
                    {
                        "ref-fulltext": "S. Khorram, Z. Aldeneh, D. Dimitriadis, M. McInnis, and E. M. Provost, \u201cCapturing long-term temporal dependencies with convolutional networks for continuous emotion recognition,\u201d in Proc. INTERSPEECH, 2017, pp. 1253\u20131257.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6a3f1116e84f121cf51570eaa383130a-4",
                        "@id": "4",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85039171854",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1253",
                                "@last": "1257"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Khorram",
                                    "ce:indexed-name": "Khorram S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Aldeneh",
                                    "ce:indexed-name": "Aldeneh Z."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Dimitriadis",
                                    "ce:indexed-name": "Dimitriadis D."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "McInnis",
                                    "ce:indexed-name": "McInnis M."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "E.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Provost",
                                    "ce:indexed-name": "Provost E.M."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "S. Khorram, Z. Aldeneh, D. Dimitriadis, M. McInnis, and E. M. Provost, \u201cCapturing long-term temporal dependencies with convolutional networks for continuous emotion recognition,\u201d in Proc. INTERSPEECH, 2017, pp. 1253\u20131257."
                    },
                    {
                        "ref-fulltext": "Z. Lian, J. Tao, B. Liu, and J. Huang, \u201cUnsupervised representation learning with future observation prediction for speech emotion recognition,\u201d in Proc. INTERSPEECH, 2019, pp. 3840\u20133844.",
                        "@reference-instance-id": "OB2BibRecID-946469326-5a6bc9fe12c57b5d8d27ee59c267e87d-5",
                        "@id": "5",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2019"},
                            "ref-title": {"ref-titletext": "Unsupervised representation learning with future observation prediction for speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85074721600",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "3840",
                                "@last": "3844"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Lian",
                                    "ce:indexed-name": "Lian Z."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Tao",
                                    "ce:indexed-name": "Tao J."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Liu",
                                    "ce:indexed-name": "Liu B."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Huang",
                                    "ce:indexed-name": "Huang J."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "Z. Lian, J. Tao, B. Liu, and J. Huang, \u201cUnsupervised representation learning with future observation prediction for speech emotion recognition,\u201d in Proc. INTERSPEECH, 2019, pp. 3840\u20133844."
                    },
                    {
                        "ref-fulltext": "G. K. Verma and U. S. Tiwary, \u201cAffect representation and recognition in 3 D continuous valence\u2013arousal\u2013dominance space,\u201d Multimedia Tools Appl., vol. 76, no. 2, pp. 2159\u20132183, 2017.",
                        "@reference-instance-id": "OB2BibRecID-946469326-45b6fa061c47528c9a376abc74761c79-6",
                        "@id": "6",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "Affect representation and recognition in 3D continuous valence\u2013arousal\u2013dominance space"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85043756240",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "76",
                                    "@issue": "2"
                                },
                                "pagerange": {
                                    "@first": "2159",
                                    "@last": "2183"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "G.K.",
                                    "@_fa": "true",
                                    "ce:surname": "Verma",
                                    "ce:indexed-name": "Verma G.K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "U.S.",
                                    "@_fa": "true",
                                    "ce:surname": "Tiwary",
                                    "ce:indexed-name": "Tiwary U.S."
                                }
                            ]},
                            "ref-sourcetitle": "Multimedia Tools Appl"
                        },
                        "ce:source-text": "G. K. Verma and U. S. Tiwary, \u201cAffect representation and recognition in 3 D continuous valence\u2013arousal\u2013dominance space,\u201d Multimedia Tools Appl., vol. 76, no. 2, pp. 2159\u20132183, 2017."
                    },
                    {
                        "ref-fulltext": "M. Gaertner, D. Sauter, H. Baumgartl, T. Rieg, and R. Buettner, \u201cMulti-class emotion recognition within the valence-arousal-dominance space using EEG,\u201d in Proc. AMCIS, 2021.",
                        "@reference-instance-id": "OB2BibRecID-946469326-142a895f619ca2aa6e5a64c3c27e3601-7",
                        "@id": "7",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Multi-class emotion recognition within the valence-arousal-dominance space using EEG"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85115869425",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {"@first": "2021"}},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Gaertner",
                                    "ce:indexed-name": "Gaertner M."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Sauter",
                                    "ce:indexed-name": "Sauter D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Baumgartl",
                                    "ce:indexed-name": "Baumgartl H."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Rieg",
                                    "ce:indexed-name": "Rieg T."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Buettner",
                                    "ce:indexed-name": "Buettner R."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. AMCIS"
                        },
                        "ce:source-text": "M. Gaertner, D. Sauter, H. Baumgartl, T. Rieg, and R. Buettner, \u201cMulti-class emotion recognition within the valence-arousal-dominance space using EEG,\u201d in Proc. AMCIS, 2021."
                    },
                    {
                        "ref-fulltext": "S. Buechel and U. Hahn, \u201cEmobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis,\u201d in Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguistics, Valencia, Spain, vol. 2, Short Papers, pp. 578\u2013585, Apr. 2017.",
                        "@reference-instance-id": "OB2BibRecID-946469326-f3bbd86601793fce3eca7be2ecfb0fb5-8",
                        "@id": "8",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "EmoBank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85021646310",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2"},
                                "pagerange": {
                                    "@first": "578",
                                    "@last": "585"
                                }
                            },
                            "ref-text": "Valencia, Spain,. Short Papers,. Apr",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Buechel",
                                    "ce:indexed-name": "Buechel S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "U.",
                                    "@_fa": "true",
                                    "ce:surname": "Hahn",
                                    "ce:indexed-name": "Hahn U."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguistics,"
                        },
                        "ce:source-text": "S. Buechel and U. Hahn, \u201cEmobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis,\u201d in Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguistics, Valencia, Spain, vol. 2, Short Papers, pp. 578\u2013585, Apr. 2017."
                    },
                    {
                        "ref-fulltext": "M. W. Bhatti, Y. Wang, and L. Guan, \u201cA neural network approach for human emotion recognition in speech,\u201d in Proc. IEEE Int. Symp. Circuits Syst., Vancouver, BC, vol. 2, 2004, pp. 181\u2013184.",
                        "@reference-instance-id": "OB2BibRecID-946469326-7ea9ad38a22aba7023c31f065ae7bf51-9",
                        "@id": "9",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2004"},
                            "ref-title": {"ref-titletext": "A neural network approach for human emotion recognition in speech"},
                            "refd-itemidlist": {"itemid": {
                                "$": "4344572159",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2"},
                                "pagerange": {
                                    "@first": "181",
                                    "@last": "184"
                                }
                            },
                            "ref-text": "Vancouver, BC",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.W.",
                                    "@_fa": "true",
                                    "ce:surname": "Bhatti",
                                    "ce:indexed-name": "Bhatti M.W."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Wang",
                                    "ce:indexed-name": "Wang Y."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Guan",
                                    "ce:indexed-name": "Guan L."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Symp. Circuits Syst."
                        },
                        "ce:source-text": "M. W. Bhatti, Y. Wang, and L. Guan, \u201cA neural network approach for human emotion recognition in speech,\u201d in Proc. IEEE Int. Symp. Circuits Syst., Vancouver, BC, vol. 2, 2004, pp. 181\u2013184."
                    },
                    {
                        "ref-fulltext": "R. Cowie et al., \u201cEmotion recognition in human-computer interaction,\u201d IEEE Signal Process. Mag., vol. 18, no. 1, pp. 32\u201380, Jan. 2001.",
                        "@reference-instance-id": "OB2BibRecID-946469326-3fffe3cb7b048c17f982f46d075ab0d2-10",
                        "@id": "10",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2001"},
                            "ref-title": {"ref-titletext": "Emotion recognition in human-computer interaction"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85032751766",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "18",
                                    "@issue": "1"
                                },
                                "pagerange": {
                                    "@first": "32",
                                    "@last": "80"
                                }
                            },
                            "ref-text": "Jan",
                            "ref-authors": {
                                "author": [{
                                    "@seq": "1",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Cowie",
                                    "ce:indexed-name": "Cowie R."
                                }],
                                "et-al": null
                            },
                            "ref-sourcetitle": "IEEE Signal Process. Mag."
                        },
                        "ce:source-text": "R. Cowie et al., \u201cEmotion recognition in human-computer interaction,\u201d IEEE Signal Process. Mag., vol. 18, no. 1, pp. 32\u201380, Jan. 2001."
                    },
                    {
                        "ref-fulltext": "J. Nicholson, K. Takahashi, and R. Nakatsu, \u201cEmotion recognition in speech using neural networks,\u201d Neural Comput. Appl., vol. 9, no. 4, pp. 290\u2013296, 2000.",
                        "@reference-instance-id": "OB2BibRecID-946469326-2fad9f73302cd52ebe10d8deb1d7ffa3-11",
                        "@id": "11",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2000"},
                            "ref-title": {"ref-titletext": "Emotion recognition in speech using neural networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "0034346176",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "9",
                                    "@issue": "4"
                                },
                                "pagerange": {
                                    "@first": "290",
                                    "@last": "296"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Nicholson",
                                    "ce:indexed-name": "Nicholson J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Takahashi",
                                    "ce:indexed-name": "Takahashi K."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Nakatsu",
                                    "ce:indexed-name": "Nakatsu R."
                                }
                            ]},
                            "ref-sourcetitle": "Neural Comput. Appl."
                        },
                        "ce:source-text": "J. Nicholson, K. Takahashi, and R. Nakatsu, \u201cEmotion recognition in speech using neural networks,\u201d Neural Comput. Appl., vol. 9, no. 4, pp. 290\u2013296, 2000."
                    },
                    {
                        "ref-fulltext": "D. Ververidis and C. Kotropoulos, \u201cFast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition,\u201d Signal Process., vol. 88, no. 12, pp. 2956\u20132970, 2008.",
                        "@reference-instance-id": "OB2BibRecID-946469326-493f82919148a0c9798130531c6cd642-12",
                        "@id": "12",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2008"},
                            "ref-title": {"ref-titletext": "Fast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "50049092345",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "88",
                                    "@issue": "12"
                                },
                                "pagerange": {
                                    "@first": "2956",
                                    "@last": "2970"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Ververidis",
                                    "ce:indexed-name": "Ververidis D."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Kotropoulos",
                                    "ce:indexed-name": "Kotropoulos C."
                                }
                            ]},
                            "ref-sourcetitle": "Signal Process"
                        },
                        "ce:source-text": "D. Ververidis and C. Kotropoulos, \u201cFast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition,\u201d Signal Process., vol. 88, no. 12, pp. 2956\u20132970, 2008."
                    },
                    {
                        "ref-fulltext": "X. Mao, L. Chen, and L. Fu, \u201cMulti-level speech emotion recognition based on HMM and ANN,\u201d in Proc. WRI World Congr. Comput. Sci. Inf. Eng., 2009, vol. 7, pp. 225\u2013229.",
                        "@reference-instance-id": "OB2BibRecID-946469326-0ad9c0e0027a32b05dafc6dff28b2d3e-13",
                        "@id": "13",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2009"},
                            "ref-title": {"ref-titletext": "Multi-level speech emotion recognition based on HMM and ANN"},
                            "refd-itemidlist": {"itemid": {
                                "$": "71049148049",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "7"},
                                "pagerange": {
                                    "@first": "225",
                                    "@last": "229"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "X.",
                                    "@_fa": "true",
                                    "ce:surname": "Mao",
                                    "ce:indexed-name": "Mao X."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Chen",
                                    "ce:indexed-name": "Chen L."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Fu",
                                    "ce:indexed-name": "Fu L."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. WRI World Congr. Comput. Sci. Inf. Eng."
                        },
                        "ce:source-text": "X. Mao, L. Chen, and L. Fu, \u201cMulti-level speech emotion recognition based on HMM and ANN,\u201d in Proc. WRI World Congr. Comput. Sci. Inf. Eng., 2009, vol. 7, pp. 225\u2013229."
                    },
                    {
                        "ref-fulltext": "T. L. Nwe, S. W. Foo, and L. C. D. Silva, \u201cSpeech emotion recognition using hidden Markov models,\u201d Speech Commun., vol. 41, no. 4, pp. 603\u2013623, 2003.",
                        "@reference-instance-id": "OB2BibRecID-946469326-47e3de40fccdfc04d2df56da937ae96a-14",
                        "@id": "14",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2003"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition using hidden Markov models"},
                            "refd-itemidlist": {"itemid": {
                                "$": "0242721417",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "41",
                                    "@issue": "4"
                                },
                                "pagerange": {
                                    "@first": "603",
                                    "@last": "623"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "T.L.",
                                    "@_fa": "true",
                                    "ce:surname": "Nwe",
                                    "ce:indexed-name": "Nwe T.L."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.W.",
                                    "@_fa": "true",
                                    "ce:surname": "Foo",
                                    "ce:indexed-name": "Foo S.W."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.C.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Silva",
                                    "ce:indexed-name": "Silva L.C.D."
                                }
                            ]},
                            "ref-sourcetitle": "Speech Commun"
                        },
                        "ce:source-text": "T. L. Nwe, S. W. Foo, and L. C. D. Silva, \u201cSpeech emotion recognition using hidden Markov models,\u201d Speech Commun., vol. 41, no. 4, pp. 603\u2013623, 2003."
                    },
                    {
                        "ref-fulltext": "J. Zhou, G. Wang, Y. Yang, and P. Chen, \u201cSpeech emotion recognition based on rough set and SVM,\u201d in Proc. IEEE Int. Conf. Cogn. Inform., 2006, vol. 1, pp. 53\u201361.",
                        "@reference-instance-id": "OB2BibRecID-946469326-e3dd91315d5cef9cac29eab3eacc9a00-15",
                        "@id": "15",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2006"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition based on rough set and SVM"},
                            "refd-itemidlist": {"itemid": {
                                "$": "38148998673",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "1"},
                                "pagerange": {
                                    "@first": "53",
                                    "@last": "61"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhou",
                                    "ce:indexed-name": "Zhou J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Wang",
                                    "ce:indexed-name": "Wang G."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Yang",
                                    "ce:indexed-name": "Yang Y."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Chen",
                                    "ce:indexed-name": "Chen P."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Cogn. Inform."
                        },
                        "ce:source-text": "J. Zhou, G. Wang, Y. Yang, and P. Chen, \u201cSpeech emotion recognition based on rough set and SVM,\u201d in Proc. IEEE Int. Conf. Cogn. Inform., 2006, vol. 1, pp. 53\u201361."
                    },
                    {
                        "ref-fulltext": "H. Hu, M.-X. Xu, and W. Wu, \u201cGMM supervector based SVM with spectral features for speech emotion recognition,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., Honolulu, HI, USA, vol. 4, 2007, pp. 413\u2013416.",
                        "@reference-instance-id": "OB2BibRecID-946469326-283fc388bdff1e9fe87691b3bc4e0f94-16",
                        "@id": "16",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2007"},
                            "ref-title": {"ref-titletext": "GMM supervector based SVM with spectral features for speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "34547507232",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "4"},
                                "pagerange": {
                                    "@first": "413",
                                    "@last": "416"
                                }
                            },
                            "ref-text": "Honolulu, HI, USA",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Hu",
                                    "ce:indexed-name": "Hu H."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.-X.",
                                    "@_fa": "true",
                                    "ce:surname": "Xu",
                                    "ce:indexed-name": "Xu M.-X."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "W.",
                                    "@_fa": "true",
                                    "ce:surname": "Wu",
                                    "ce:indexed-name": "Wu W."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "H. Hu, M.-X. Xu, and W. Wu, \u201cGMM supervector based SVM with spectral features for speech emotion recognition,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., Honolulu, HI, USA, vol. 4, 2007, pp. 413\u2013416."
                    },
                    {
                        "ref-fulltext": "D. Neiberg, K. Elenius, and K. Laskowski, \u201cEmotion recognition in spontaneous speech using GMMs,\u201d in Interspeech., Pittsburgh, PA, Sep. 2006, pp. 809\u2013812.",
                        "@reference-instance-id": "OB2BibRecID-946469326-d7b9b3001f5b25a17827d2921dd78e70-17",
                        "@id": "17",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2006"},
                            "ref-title": {"ref-titletext": "Emotion recognition in spontaneous speech using GMMs"},
                            "refd-itemidlist": {"itemid": {
                                "$": "38749103707",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "809",
                                "@last": "812"
                            }},
                            "ref-text": "Pittsburgh, PA, Sep",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Neiberg",
                                    "ce:indexed-name": "Neiberg D."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Elenius",
                                    "ce:indexed-name": "Elenius K."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Laskowski",
                                    "ce:indexed-name": "Laskowski K."
                                }
                            ]},
                            "ref-sourcetitle": "Interspeech"
                        },
                        "ce:source-text": "D. Neiberg, K. Elenius, and K. Laskowski, \u201cEmotion recognition in spontaneous speech using GMMs,\u201d in Interspeech., Pittsburgh, PA, Sep. 2006, pp. 809\u2013812."
                    },
                    {
                        "ref-fulltext": "M. El Ayadi, M. S. Kamel, and F. Karray, \u201cSurvey on speech emotion recognition: Features, classification schemes, and databases,\u201d Pattern Recognit., vol. 44, no. 3, pp. 572\u2013587, 2011.",
                        "@reference-instance-id": "OB2BibRecID-946469326-fb4439f85e400586dae3e7e6fac3303b-18",
                        "@id": "18",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2011"},
                            "ref-title": {"ref-titletext": "Survey on speech emotion recognition: Features, classification schemes, and databases"},
                            "refd-itemidlist": {"itemid": {
                                "$": "78649328053",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "44",
                                    "@issue": "3"
                                },
                                "pagerange": {
                                    "@first": "572",
                                    "@last": "587"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.S.",
                                    "@_fa": "true",
                                    "ce:surname": "Kamel",
                                    "ce:indexed-name": "Kamel M.S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Karray",
                                    "ce:indexed-name": "Karray F."
                                }
                            ]},
                            "ref-sourcetitle": "Pattern Recognit"
                        },
                        "ce:source-text": "M. El Ayadi, M. S. Kamel, and F. Karray, \u201cSurvey on speech emotion recognition: Features, classification schemes, and databases,\u201d Pattern Recognit., vol. 44, no. 3, pp. 572\u2013587, 2011."
                    },
                    {
                        "ref-fulltext": "A. M. Badshah, J. Ahmad, N. Rahim, and S. W. Baik, \u201cSpeech emotion recognition from spectrograms with deep convolutional neural network,\u201d in Proc. IEEE Int. Conf. Platform Technol. Serv., 2017, pp. 1\u20135.",
                        "@reference-instance-id": "OB2BibRecID-946469326-44749fdad91677451499bd18bf7e6c23-19",
                        "@id": "19",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition from spectrograms with deep convolutional neural network"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85018171150",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1",
                                "@last": "5"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Badshah",
                                    "ce:indexed-name": "Badshah A.M."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Ahmad",
                                    "ce:indexed-name": "Ahmad J."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "N.",
                                    "@_fa": "true",
                                    "ce:surname": "Rahim",
                                    "ce:indexed-name": "Rahim N."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "S.W.",
                                    "@_fa": "true",
                                    "ce:surname": "Baik",
                                    "ce:indexed-name": "Baik S.W."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Platform Technol. Serv."
                        },
                        "ce:source-text": "A. M. Badshah, J. Ahmad, N. Rahim, and S. W. Baik, \u201cSpeech emotion recognition from spectrograms with deep convolutional neural network,\u201d in Proc. IEEE Int. Conf. Platform Technol. Serv., 2017, pp. 1\u20135."
                    },
                    {
                        "ref-fulltext": "T.-W. Sun, \u201cEnd-to-end speech emotion recognition with gender information,\u201d IEEE Access, vol. 8, pp. 152423\u2013152438, 2020.",
                        "@reference-instance-id": "OB2BibRecID-946469326-315a52cfec94100cc23c0818e73b0778-20",
                        "@id": "20",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2020"},
                            "ref-title": {"ref-titletext": "End-to-end speech emotion recognition with gender information"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85090770377",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "8"},
                                "pagerange": {
                                    "@first": "152423",
                                    "@last": "152438"
                                }
                            },
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "T.-W.",
                                "@_fa": "true",
                                "ce:surname": "Sun",
                                "ce:indexed-name": "Sun T.-W."
                            }]},
                            "ref-sourcetitle": "IEEE Access"
                        },
                        "ce:source-text": "T.-W. Sun, \u201cEnd-to-end speech emotion recognition with gender information,\u201d IEEE Access, vol. 8, pp. 152423\u2013152438, 2020."
                    },
                    {
                        "ref-fulltext": "D. Issa, M. F. Demirci, and A. Yazici, \u201cSpeech emotion recognition with deep convolutional neural networks,\u201d Biomed. Signal Process. Control, vol. 59, pp. 1\u201311, 2020.",
                        "@reference-instance-id": "OB2BibRecID-946469326-1d411914c0d2a9316338963554e8eb28-21",
                        "@id": "21",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2020"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition with deep convolutional neural networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85079857593",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "59"},
                                "pagerange": {
                                    "@first": "1",
                                    "@last": "11"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Issa",
                                    "ce:indexed-name": "Issa D."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.F.",
                                    "@_fa": "true",
                                    "ce:surname": "Demirci",
                                    "ce:indexed-name": "Demirci M.F."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Yazici",
                                    "ce:indexed-name": "Yazici A."
                                }
                            ]},
                            "ref-sourcetitle": "Biomed. Signal Process. Control"
                        },
                        "ce:source-text": "D. Issa, M. F. Demirci, and A. Yazici, \u201cSpeech emotion recognition with deep convolutional neural networks,\u201d Biomed. Signal Process. Control, vol. 59, pp. 1\u201311, 2020."
                    },
                    {
                        "ref-fulltext": "J. Lee and I. Tashev, \u201cHigh-level feature representation using recurrent neural network for speech emotion recognition,\u201d in Proc. INTERSPEECH, 2015, pp. 1537\u20131540.",
                        "@reference-instance-id": "OB2BibRecID-946469326-71ecf46e139938aa6f54b59e4b194e1f-22",
                        "@id": "22",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "High-level feature representation using recurrent neural network for speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84959149459",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1537",
                                "@last": "1540"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Lee",
                                    "ce:indexed-name": "Lee J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "I.",
                                    "@_fa": "true",
                                    "ce:surname": "Tashev",
                                    "ce:indexed-name": "Tashev I."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "J. Lee and I. Tashev, \u201cHigh-level feature representation using recurrent neural network for speech emotion recognition,\u201d in Proc. INTERSPEECH, 2015, pp. 1537\u20131540."
                    },
                    {
                        "ref-fulltext": "V. Chernykh and P. Prikhodko, \u201cEmotion recognition from speech with recurrent neural networks,\u201d Jul. 2018, arXiv:1701.08071v2.",
                        "@reference-instance-id": "OB2BibRecID-946469326-65e8b6165d25af669f9aa8131cc76545-23",
                        "@id": "23",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "1701.08071v2",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85039174225",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-text": "Jul",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "V.",
                                    "@_fa": "true",
                                    "ce:surname": "Chernykh",
                                    "ce:indexed-name": "Chernykh V."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Prikhodko",
                                    "ce:indexed-name": "Prikhodko P."
                                }
                            ]},
                            "ref-sourcetitle": "Emotion Recognition from Speech with Recurrent Neural Networks"
                        },
                        "ce:source-text": "V. Chernykh and P. Prikhodko, \u201cEmotion recognition from speech with recurrent neural networks,\u201d Jul. 2018, arXiv:1701.08071v2."
                    },
                    {
                        "ref-fulltext": "G. Trigeorgis et al., \u201cAdieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2016, pp. 5200\u20135204.",
                        "@reference-instance-id": "OB2BibRecID-946469326-4a55323853e7bc12491b8233a37523ae-24",
                        "@id": "24",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84973293291",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "5200",
                                "@last": "5204"
                            }},
                            "ref-authors": {
                                "author": [{
                                    "@seq": "1",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Trigeorgis",
                                    "ce:indexed-name": "Trigeorgis G."
                                }],
                                "et-al": null
                            },
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "G. Trigeorgis et al., \u201cAdieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2016, pp. 5200\u20135204."
                    },
                    {
                        "ref-fulltext": "P. Meyer, Z. Xu, and T. Fingscheidt, \u201cImproving convolutional recurrent neural networks for speech emotion recognition,\u201d in Proc. IEEE Spoken Lang. Technnol. Workshop, 2021, pp. 365\u2013372.",
                        "@reference-instance-id": "OB2BibRecID-946469326-93a1dd3dabf2870ddfe44f978e6a8063-25",
                        "@id": "25",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Improving convolutional recurrent neural networks for speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85103961498",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "365",
                                    "@last": "372"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Meyer",
                                    "ce:indexed-name": "Meyer P."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Xu",
                                    "ce:indexed-name": "Xu Z."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Fingscheidt",
                                    "ce:indexed-name": "Fingscheidt T."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Spoken Lang. Technnol. Workshop"
                        },
                        "ce:source-text": "P. Meyer, Z. Xu, and T. Fingscheidt, \u201cImproving convolutional recurrent neural networks for speech emotion recognition,\u201d in Proc. IEEE Spoken Lang. Technnol. Workshop, 2021, pp. 365\u2013372."
                    },
                    {
                        "ref-fulltext": "M. A. Qamhan, A. H. Meftah, S.-A. Selouani, Y. A. Alotaibi, M. Zakariah, and Y. M. Seddiq, \u201cSpeech emotion recognition using convolutional recurrent neural networks and spectrograms,\u201d in Proc. IEEE Can. Conf. Elect. Comput. Eng., 2020, pp. 1\u20135.",
                        "@reference-instance-id": "OB2BibRecID-946469326-c5918ba23ff44395b80456f2a681fd4e-26",
                        "@id": "26",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Speech emotion recognition using convolutional recurrent neural networks and spectrograms"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85097830608",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2020"},
                                "pagerange": {
                                    "@first": "1",
                                    "@last": "5"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.A.",
                                    "@_fa": "true",
                                    "ce:surname": "Qamhan",
                                    "ce:indexed-name": "Qamhan M.A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.H.",
                                    "@_fa": "true",
                                    "ce:surname": "Meftah",
                                    "ce:indexed-name": "Meftah A.H."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.-A.",
                                    "@_fa": "true",
                                    "ce:surname": "Selouani",
                                    "ce:indexed-name": "Selouani S.-A."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "Y.A.",
                                    "@_fa": "true",
                                    "ce:surname": "Alotaibi",
                                    "ce:indexed-name": "Alotaibi Y.A."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Zakariah",
                                    "ce:indexed-name": "Zakariah M."
                                },
                                {
                                    "@seq": "6",
                                    "ce:initials": "Y.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Seddiq",
                                    "ce:indexed-name": "Seddiq Y.M."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Can. Conf. Elect. Comput. Eng."
                        },
                        "ce:source-text": "M. A. Qamhan, A. H. Meftah, S.-A. Selouani, Y. A. Alotaibi, M. Zakariah, and Y. M. Seddiq, \u201cSpeech emotion recognition using convolutional recurrent neural networks and spectrograms,\u201d in Proc. IEEE Can. Conf. Elect. Comput. Eng., 2020, pp. 1\u20135."
                    },
                    {
                        "ref-fulltext": "Y. Kim, H. Lee, and E. M. Provost, \u201cDeep learning for robust feature generation in audiovisual emotion recognition,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2013, pp. 3687\u20133691.",
                        "@reference-instance-id": "OB2BibRecID-946469326-cbdffcf5245dcae736ab68b49038ebaa-27",
                        "@id": "27",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2013"},
                            "ref-title": {"ref-titletext": "Deep learning for robust feature generation in audiovisual emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84890526379",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "3687",
                                "@last": "3691"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Kim",
                                    "ce:indexed-name": "Kim Y."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Lee",
                                    "ce:indexed-name": "Lee H."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "E.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Provost",
                                    "ce:indexed-name": "Provost E.M."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "Y. Kim, H. Lee, and E. M. Provost, \u201cDeep learning for robust feature generation in audiovisual emotion recognition,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2013, pp. 3687\u20133691."
                    },
                    {
                        "ref-fulltext": "Q. Mao, M. Dong, Z. Huang, and Y. Zhan, \u201cLearning salient features for speech emotion recognition using convolutional neural networks,\u201d IEEE Trans. Multimedia, vol. 16, pp. 2203\u20132213, 2014.",
                        "@reference-instance-id": "OB2BibRecID-946469326-ac0e00a08774138e0b603c3c89dc745d-28",
                        "@id": "28",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2014"},
                            "ref-title": {"ref-titletext": "Learning salient features for speech emotion recognition using convolutional neural networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84913548678",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "16"},
                                "pagerange": {
                                    "@first": "2203",
                                    "@last": "2213"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "Q.",
                                    "@_fa": "true",
                                    "ce:surname": "Mao",
                                    "ce:indexed-name": "Mao Q."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Dong",
                                    "ce:indexed-name": "Dong M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Huang",
                                    "ce:indexed-name": "Huang Z."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhan",
                                    "ce:indexed-name": "Zhan Y."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans. Multimedia"
                        },
                        "ce:source-text": "Q. Mao, M. Dong, Z. Huang, and Y. Zhan, \u201cLearning salient features for speech emotion recognition using convolutional neural networks,\u201d IEEE Trans. Multimedia, vol. 16, pp. 2203\u20132213, 2014."
                    },
                    {
                        "ref-fulltext": "Z. Huang, M. Dong, Q. Mao, and Y. Zhan, \u201cSpeech emotion recognition using CNN,\u201d in Proc. ACM Int. Conf. Multimedia, 2014, pp. 801\u2013804.",
                        "@reference-instance-id": "OB2BibRecID-946469326-3479b740f3fce1c4811ea4d14e84df22-29",
                        "@id": "29",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2014"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition using CNN"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84913537962",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "801",
                                "@last": "804"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "Z.",
                                    "@_fa": "true",
                                    "ce:surname": "Huang",
                                    "ce:indexed-name": "Huang Z."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Dong",
                                    "ce:indexed-name": "Dong M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "Q.",
                                    "@_fa": "true",
                                    "ce:surname": "Mao",
                                    "ce:indexed-name": "Mao Q."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhan",
                                    "ce:indexed-name": "Zhan Y."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. ACM Int. Conf. Multimedia"
                        },
                        "ce:source-text": "Z. Huang, M. Dong, Q. Mao, and Y. Zhan, \u201cSpeech emotion recognition using CNN,\u201d in Proc. ACM Int. Conf. Multimedia, 2014, pp. 801\u2013804."
                    },
                    {
                        "ref-fulltext": "K. Han, D. Yu, and I. Tashev, \u201cSpeech emotion recognition using deep neural network and extreme learning machine,\u201d in Proc. INTERSPEECH, 2014, pp. 223\u2013227.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6d3093397ccd0ceb065e3ba4c95de1c9-30",
                        "@id": "30",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2014"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition using deep neural network and extreme learning machine"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84910060363",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "223",
                                "@last": "227"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Han",
                                    "ce:indexed-name": "Han K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Yu",
                                    "ce:indexed-name": "Yu D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "I.",
                                    "@_fa": "true",
                                    "ce:surname": "Tashev",
                                    "ce:indexed-name": "Tashev I."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "K. Han, D. Yu, and I. Tashev, \u201cSpeech emotion recognition using deep neural network and extreme learning machine,\u201d in Proc. INTERSPEECH, 2014, pp. 223\u2013227."
                    },
                    {
                        "ref-fulltext": "N. Rossenbach, A. Zeyer, R. Schlüter, and H. Ney, \u201cGenerating synthetic audio data for attention-based speech recognition systems,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2020, pp. 7069\u20137073.",
                        "@reference-instance-id": "OB2BibRecID-946469326-d388de8b3af24a6bcc47993512433ecd-31",
                        "@id": "31",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Generating synthetic audio data for attention-based speech recognition systems"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85089243050",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2020"},
                                "pagerange": {
                                    "@first": "7069",
                                    "@last": "7073"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "N.",
                                    "@_fa": "true",
                                    "ce:surname": "Rossenbach",
                                    "ce:indexed-name": "Rossenbach N."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Zeyer",
                                    "ce:indexed-name": "Zeyer A."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Schlüter",
                                    "ce:indexed-name": "Schluter R."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Ney",
                                    "ce:indexed-name": "Ney H."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "N. Rossenbach, A. Zeyer, R. Schlüter, and H. Ney, \u201cGenerating synthetic audio data for attention-based speech recognition systems,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2020, pp. 7069\u20137073."
                    },
                    {
                        "ref-fulltext": "A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, and S. Rybin, \u201cYou do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation,\u201d in Proc. Int. Congr. Image Signal Process., Biomed. Eng. Informat., 2020, pp. 439\u2013444.",
                        "@reference-instance-id": "OB2BibRecID-946469326-f3db6d45ef952ddea7c0c16a5af0bc11-32",
                        "@id": "32",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85099566768",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2020"},
                                "pagerange": {
                                    "@first": "439",
                                    "@last": "444"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Laptev",
                                    "ce:indexed-name": "Laptev A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Korostik",
                                    "ce:indexed-name": "Korostik R."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Svischev",
                                    "ce:indexed-name": "Svischev A."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Andrusenko",
                                    "ce:indexed-name": "Andrusenko A."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "I.",
                                    "@_fa": "true",
                                    "ce:surname": "Medennikov",
                                    "ce:indexed-name": "Medennikov I."
                                },
                                {
                                    "@seq": "6",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Rybin",
                                    "ce:indexed-name": "Rybin S."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. Int. Congr. Image Signal Process., Biomed. Eng. Informat."
                        },
                        "ce:source-text": "A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, and S. Rybin, \u201cYou do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation,\u201d in Proc. Int. Congr. Image Signal Process., Biomed. Eng. Informat., 2020, pp. 439\u2013444."
                    },
                    {
                        "ref-fulltext": "M. Macary, M. Tahon, Y. Estève, and A. Rousseau, \u201cOn the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition,\u201d in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 373\u2013380.",
                        "@reference-instance-id": "OB2BibRecID-946469326-32ba8a85a142998bb4b325f224b41c0c-33",
                        "@id": "33",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85103961035",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "373",
                                    "@last": "380"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Macary",
                                    "ce:indexed-name": "Macary M."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Tahon",
                                    "ce:indexed-name": "Tahon M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Estève",
                                    "ce:indexed-name": "Esteve Y."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Rousseau",
                                    "ce:indexed-name": "Rousseau A."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Spoken Lang. Technol. Workshop"
                        },
                        "ce:source-text": "M. Macary, M. Tahon, Y. Estève, and A. Rousseau, \u201cOn the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition,\u201d in Proc. IEEE Spoken Lang. Technol. Workshop, 2021, pp. 373\u2013380."
                    },
                    {
                        "ref-fulltext": "L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion recognition from speech using Wav2vec 2.0 embeddings,\u201d in Proc. INTERSPEECH, 2021, pp. 3400\u20133404.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6118571ddc482afd7b11dd278ba92af5-34",
                        "@id": "34",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Emotion recognition from speech using Wav2vec 2.0 embeddings"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85117558382",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "3400",
                                    "@last": "3404"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Pepino",
                                    "ce:indexed-name": "Pepino L."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Riera",
                                    "ce:indexed-name": "Riera P."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Ferrer",
                                    "ce:indexed-name": "Ferrer L."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion recognition from speech using Wav2vec 2.0 embeddings,\u201d in Proc. INTERSPEECH, 2021, pp. 3400\u20133404."
                    },
                    {
                        "ref-fulltext": "E. Guizzo, T. Weyde, and G. Tarroni, \u201cAnti-transfer learning for task invariance in convolutional neural networks for speech processing,\u201d Neural Netw., vol. 142, pp. 238\u2013251, 2021.",
                        "@reference-instance-id": "OB2BibRecID-946469326-b14fdf40f32d5db24c8c11a7a09d75f1-35",
                        "@id": "35",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2021"},
                            "ref-title": {"ref-titletext": "Anti-transfer learning for task invariance in convolutional neural networks for speech processing"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85106349267",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "142"},
                                "pagerange": {
                                    "@first": "238",
                                    "@last": "251"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Guizzo",
                                    "ce:indexed-name": "Guizzo E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Weyde",
                                    "ce:indexed-name": "Weyde T."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Tarroni",
                                    "ce:indexed-name": "Tarroni G."
                                }
                            ]},
                            "ref-sourcetitle": "Neural Netw"
                        },
                        "ce:source-text": "E. Guizzo, T. Weyde, and G. Tarroni, \u201cAnti-transfer learning for task invariance in convolutional neural networks for speech processing,\u201d Neural Netw., vol. 142, pp. 238\u2013251, 2021."
                    },
                    {
                        "ref-fulltext": "S. Padi, D. Manocha, and R. D. Sriram, \u201cMulti-window data augmentation approach for speech emotion recognition,\u201d Feb. 2022, arXiv:2010.09895v4.",
                        "@reference-instance-id": "OB2BibRecID-946469326-303c7e88898cbd2b10bd7010aa536e91-36",
                        "@id": "36",
                        "ref-info": {
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "2010.09895v4",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85112718919",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-text": "Feb. 2022",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Padi",
                                    "ce:indexed-name": "Padi S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Manocha",
                                    "ce:indexed-name": "Manocha D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "R.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Sriram",
                                    "ce:indexed-name": "Sriram R.D."
                                }
                            ]},
                            "ref-sourcetitle": "Multi-Window Data Augmentation Approach for Speech Emotion Recognition"
                        },
                        "ce:source-text": "S. Padi, D. Manocha, and R. D. Sriram, \u201cMulti-window data augmentation approach for speech emotion recognition,\u201d Feb. 2022, arXiv:2010.09895v4."
                    },
                    {
                        "ref-fulltext": "A. Shilandari, H. Marvi, and H. Khosravi, \u201cSpeech emotion recognition using data augmentation method by cycle-generative adversarial networks,\u201d Signal, Image Video Process., vol. 16, no. 7, pp. 1955\u20131962, 2022.",
                        "@reference-instance-id": "OB2BibRecID-946469326-7c318802636d4eac2067ddeb6ce97d46-37",
                        "@id": "37",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2022"},
                            "ref-title": {"ref-titletext": "Speech emotion recognition using data augmentation method by cycle-generative adversarial networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85124570961",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "16",
                                    "@issue": "7"
                                },
                                "pagerange": {
                                    "@first": "1955",
                                    "@last": "1962"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Shilandari",
                                    "ce:indexed-name": "Shilandari A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Marvi",
                                    "ce:indexed-name": "Marvi H."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "H.",
                                    "@_fa": "true",
                                    "ce:surname": "Khosravi",
                                    "ce:indexed-name": "Khosravi H."
                                }
                            ]},
                            "ref-sourcetitle": "Signal, Image Video Process"
                        },
                        "ce:source-text": "A. Shilandari, H. Marvi, and H. Khosravi, \u201cSpeech emotion recognition using data augmentation method by cycle-generative adversarial networks,\u201d Signal, Image Video Process., vol. 16, no. 7, pp. 1955\u20131962, 2022."
                    },
                    {
                        "ref-fulltext": "P. Fewzee and F. Karray, \u201cDimensionality reduction for emotional speech recognition,\u201d in Proc. IEEE Int. Conf. Privacy, Secur., Risk Trust Int. Conf. Social Comput., 2012, pp. 532\u2013537.",
                        "@reference-instance-id": "OB2BibRecID-946469326-5e1aab54a8acbb358bd7874cf826c5b6-38",
                        "@id": "38",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2012"},
                            "ref-title": {"ref-titletext": "Dimensionality reduction for emotional speech recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84873677251",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "532",
                                "@last": "537"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Fewzee",
                                    "ce:indexed-name": "Fewzee P."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Karray",
                                    "ce:indexed-name": "Karray F."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Privacy, Secur., Risk Trust Int. Conf. Social Comput."
                        },
                        "ce:source-text": "P. Fewzee and F. Karray, \u201cDimensionality reduction for emotional speech recognition,\u201d in Proc. IEEE Int. Conf. Privacy, Secur., Risk Trust Int. Conf. Social Comput., 2012, pp. 532\u2013537."
                    },
                    {
                        "ref-fulltext": "N. Patel, S. Patel, and S. H. Mankad, \u201cImpact of autoencoder based compact representation on emotion detection from audio,\u201d J. Ambient Intell. Humanized Comput., vol. 13, no. 2, pp. 867\u2013885, 2021.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6a085255777b29ff0f41c5147b01f45f-39",
                        "@id": "39",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2021"},
                            "ref-title": {"ref-titletext": "Impact of autoencoder based compact representation on emotion detection from audio"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85108125006",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "13",
                                    "@issue": "2"
                                },
                                "pagerange": {
                                    "@first": "867",
                                    "@last": "885"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "N.",
                                    "@_fa": "true",
                                    "ce:surname": "Patel",
                                    "ce:indexed-name": "Patel N."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Patel",
                                    "ce:indexed-name": "Patel S."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.H.",
                                    "@_fa": "true",
                                    "ce:surname": "Mankad",
                                    "ce:indexed-name": "Mankad S.H."
                                }
                            ]},
                            "ref-sourcetitle": "J. Ambient Intell. Humanized Comput."
                        },
                        "ce:source-text": "N. Patel, S. Patel, and S. H. Mankad, \u201cImpact of autoencoder based compact representation on emotion detection from audio,\u201d J. Ambient Intell. Humanized Comput., vol. 13, no. 2, pp. 867\u2013885, 2021."
                    },
                    {
                        "ref-fulltext": "Y. Tay et al., \u201cLightweight and efficient neural natural language processing with quaternion networks,\u201d in Proc. 57th Ann. Meeting the Assoc. Comput. Linguistics, 2019, pp. 1494\u20131503.",
                        "@reference-instance-id": "OB2BibRecID-946469326-956163c30f5888aeac8edaffddfdcf55-40",
                        "@id": "40",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2019"},
                            "ref-title": {"ref-titletext": "Lightweight and efficient neural natural language processing with quaternion networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85078527600",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1494",
                                "@last": "1503"
                            }},
                            "ref-authors": {
                                "author": [{
                                    "@seq": "1",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Tay",
                                    "ce:indexed-name": "Tay Y."
                                }],
                                "et-al": null
                            },
                            "ref-sourcetitle": "Proc. 57th Ann. Meeting The Assoc. Comput. Linguistics"
                        },
                        "ce:source-text": "Y. Tay et al., \u201cLightweight and efficient neural natural language processing with quaternion networks,\u201d in Proc. 57th Ann. Meeting the Assoc. Comput. Linguistics, 2019, pp. 1494\u20131503."
                    },
                    {
                        "ref-fulltext": "E. Grassucci, D. Comminiello, and A. Uncini, \u201cA quaternion-valued variational autoencoder,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 3310\u20133314.",
                        "@reference-instance-id": "OB2BibRecID-946469326-c00764db962ab55a07ef29d0e1bb01f4-41",
                        "@id": "41",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "A quaternion-valued variational autoencoder"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85106110193",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "3310",
                                    "@last": "3314"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Grassucci",
                                    "ce:indexed-name": "Grassucci E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Uncini",
                                    "ce:indexed-name": "Uncini A."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "E. Grassucci, D. Comminiello, and A. Uncini, \u201cA quaternion-valued variational autoencoder,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 3310\u20133314."
                    },
                    {
                        "ref-fulltext": "E. Grassucci, E. Cicero, and D. Comminiello, \u201cQuaternion generative adversarial networks,\u201d in Generative Adversarial Learning: Architectures and Applications, R. Razavi-Far, A. Ruiz-Garcia, V. Palade, and J. Schmidhuber, Eds. Cham, Switzerland:Springer, 2022, pp. 57\u201386.",
                        "@reference-instance-id": "OB2BibRecID-946469326-7e75c39db7c4ba33ecc9dd7493481ec2-42",
                        "@id": "42",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2022"},
                            "ref-title": {"ref-titletext": "Quaternion generative adversarial networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85124536243",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "57",
                                "@last": "86"
                            }},
                            "ref-text": "R. Razavi-Far, A. Ruiz-Garcia, Palade, and J. Schmidhuber, Eds. Cham, Switzerland:Springer",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Grassucci",
                                    "ce:indexed-name": "Grassucci E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Cicero",
                                    "ce:indexed-name": "Cicero E."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                }
                            ]},
                            "ref-sourcetitle": "Generative Adversarial Learning: Architectures and Applications"
                        },
                        "ce:source-text": "E. Grassucci, E. Cicero, and D. Comminiello, \u201cQuaternion generative adversarial networks,\u201d in Generative Adversarial Learning: Architectures and Applications, R. Razavi-Far, A. Ruiz-Garcia, V. Palade, and J. Schmidhuber, Eds. Cham, Switzerland:Springer, 2022, pp. 57\u201386."
                    },
                    {
                        "ref-fulltext": "E. Grassucci, A. Zhang, and D. Comminiello, \u201cPHNNs: Lightweight neural networks via parameterized hypercomplex convolutions,\u201d IEEE Trans. Neural Netw. Learn. Syst., early access, Dec. 2022, doi: 10.1109/TNNLS.2022.3226772.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6c820863977a2958b6fc8d768e050515-43",
                        "@id": "43",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2022"},
                            "ref-title": {"ref-titletext": "PHNNS: Lightweight neural networks via parameterized hypercomplex convolutions"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "10.1109/TNNLS.2022.3226772",
                                    "@idtype": "DOI"
                                },
                                {
                                    "$": "85144770762",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-text": "early access, Dec",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Grassucci",
                                    "ce:indexed-name": "Grassucci E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhang",
                                    "ce:indexed-name": "Zhang A."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans. Neural Netw. Learn. Syst."
                        },
                        "ce:source-text": "E. Grassucci, A. Zhang, and D. Comminiello, \u201cPHNNs: Lightweight neural networks via parameterized hypercomplex convolutions,\u201d IEEE Trans. Neural Netw. Learn. Syst., early access, Dec. 2022, doi: 10.1109/TNNLS.2022.3226772."
                    },
                    {
                        "ref-fulltext": "A. B. Greenblatt and S. S. Agaian, \u201cIntroducing quaternion multi-valued neural networks with numerical examples,\u201d Inf. Sci., vol. 423, pp. 326\u2013342, 2018.",
                        "@reference-instance-id": "OB2BibRecID-946469326-cf9359b514a97c7831dc4cb94d0e56ee-44",
                        "@id": "44",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "Introducing quaternion multi-valued neural networks with numerical examples"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85030089829",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "423"},
                                "pagerange": {
                                    "@first": "326",
                                    "@last": "342"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.B.",
                                    "@_fa": "true",
                                    "ce:surname": "Greenblatt",
                                    "ce:indexed-name": "Greenblatt A.B."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.S.",
                                    "@_fa": "true",
                                    "ce:surname": "Agaian",
                                    "ce:indexed-name": "Agaian S.S."
                                }
                            ]},
                            "ref-sourcetitle": "Inf. Sci."
                        },
                        "ce:source-text": "A. B. Greenblatt and S. S. Agaian, \u201cIntroducing quaternion multi-valued neural networks with numerical examples,\u201d Inf. Sci., vol. 423, pp. 326\u2013342, 2018."
                    },
                    {
                        "ref-fulltext": "T. Parcollet et al., \u201cQuaternion convolutional neural networks for end-to-end automatic speech recognition,\u201d in Proc. INTERSPEECH, 2018, pp. 22\u201326.",
                        "@reference-instance-id": "OB2BibRecID-946469326-60e5a50986f1488fe289f71d4335cd82-45",
                        "@id": "45",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "Quaternion convolutional neural networks for end-to-end automatic speech recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85054958663",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "22",
                                "@last": "26"
                            }},
                            "ref-authors": {
                                "author": [{
                                    "@seq": "1",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Parcollet",
                                    "ce:indexed-name": "Parcollet T."
                                }],
                                "et-al": null
                            },
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "T. Parcollet et al., \u201cQuaternion convolutional neural networks for end-to-end automatic speech recognition,\u201d in Proc. INTERSPEECH, 2018, pp. 22\u201326."
                    },
                    {
                        "ref-fulltext": "A. Muppidi and M. Radfar, \u201cSpeech emotion recognition using quaternion convolutional neural networks,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 6309\u20136313.",
                        "@reference-instance-id": "OB2BibRecID-946469326-2a5513b49bc3bc7842ee58e959de0dec-46",
                        "@id": "46",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Speech emotion recognition using quaternion convolutional neural networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85114965941",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "6309",
                                    "@last": "6313"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Muppidi",
                                    "ce:indexed-name": "Muppidi A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Radfar",
                                    "ce:indexed-name": "Radfar M."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "A. Muppidi and M. Radfar, \u201cSpeech emotion recognition using quaternion convolutional neural networks,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 6309\u20136313."
                    },
                    {
                        "ref-fulltext": "T. Bulow and G. Sommer, \u201cHypercomplex signals-a novel extension of the analytic signal to the multidimensional case,\u201d IEEE Trans. Signal Process., vol. 49, no. 11, pp. 2844\u20132852, Nov. 2001.",
                        "@reference-instance-id": "OB2BibRecID-946469326-57fcaab20bab217a1fde81eec52d5162-47",
                        "@id": "47",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2001"},
                            "ref-title": {"ref-titletext": "Hypercomplex signals-a novel extension of the analytic signal to the multidimensional case"},
                            "refd-itemidlist": {"itemid": {
                                "$": "0035503560",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "49",
                                    "@issue": "11"
                                },
                                "pagerange": {
                                    "@first": "2844",
                                    "@last": "2852"
                                }
                            },
                            "ref-text": "Nov",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Bulow",
                                    "ce:indexed-name": "Bulow T."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Sommer",
                                    "ce:indexed-name": "Sommer G."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans. Signal Process."
                        },
                        "ce:source-text": "T. Bulow and G. Sommer, \u201cHypercomplex signals-a novel extension of the analytic signal to the multidimensional case,\u201d IEEE Trans. Signal Process., vol. 49, no. 11, pp. 2844\u20132852, Nov. 2001."
                    },
                    {
                        "ref-fulltext": "D. P. Mandic, C. Jahanchahi, and C. C. Took, \u201cA quaternion gradient operator and its applications,\u201d IEEE Signal Process. Lett., vol. 18, no. 1, pp. 47\u201350, Jan. 2011.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6326b5459f61a4157f7125f8288bbd5d-48",
                        "@id": "48",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2011"},
                            "ref-title": {"ref-titletext": "A quaternion gradient operator and its applications"},
                            "refd-itemidlist": {"itemid": {
                                "$": "78650000279",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "18",
                                    "@issue": "1"
                                },
                                "pagerange": {
                                    "@first": "47",
                                    "@last": "50"
                                }
                            },
                            "ref-text": "Jan",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.P.",
                                    "@_fa": "true",
                                    "ce:surname": "Mandic",
                                    "ce:indexed-name": "Mandic D.P."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Jahanchahi",
                                    "ce:indexed-name": "Jahanchahi C."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "C.C.",
                                    "@_fa": "true",
                                    "ce:surname": "Took",
                                    "ce:indexed-name": "Took C.C."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Signal Process. Lett."
                        },
                        "ce:source-text": "D. P. Mandic, C. Jahanchahi, and C. C. Took, \u201cA quaternion gradient operator and its applications,\u201d IEEE Signal Process. Lett., vol. 18, no. 1, pp. 47\u201350, Jan. 2011."
                    },
                    {
                        "ref-fulltext": "D. Comminiello, M. Scarpiniti, R. Parisi, and A. Uncini, \u201cFrequency-domain adaptive filtering: From real to hypercomplex signal processing,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2019, pp. 7745\u20137749.",
                        "@reference-instance-id": "OB2BibRecID-946469326-bdfbe488cb32d24054b12d353d932046-49",
                        "@id": "49",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2019"},
                            "ref-title": {"ref-titletext": "Frequency-domain adaptive filtering: From real to hypercomplex signal processing"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85068979289",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "7745",
                                "@last": "7749"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Scarpiniti",
                                    "ce:indexed-name": "Scarpiniti M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Parisi",
                                    "ce:indexed-name": "Parisi R."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Uncini",
                                    "ce:indexed-name": "Uncini A."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "D. Comminiello, M. Scarpiniti, R. Parisi, and A. Uncini, \u201cFrequency-domain adaptive filtering: From real to hypercomplex signal processing,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2019, pp. 7745\u20137749."
                    },
                    {
                        "ref-fulltext": "D. Comminiello, M. Lella, S. Scardapane, and A. Uncini, \u201cQuaternion convolutional neural networks for detection and localization of 3D sound events,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2019, pp. 8533\u20138537.",
                        "@reference-instance-id": "OB2BibRecID-946469326-98e08900122da0615f8b14d93af4d3d5-50",
                        "@id": "50",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2019"},
                            "ref-title": {"ref-titletext": "Quaternion convolutional neural networks for detection and localization of 3D sound events"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85068998795",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "8533",
                                "@last": "8537"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Lella",
                                    "ce:indexed-name": "Lella M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Scardapane",
                                    "ce:indexed-name": "Scardapane S."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Uncini",
                                    "ce:indexed-name": "Uncini A."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "D. Comminiello, M. Lella, S. Scardapane, and A. Uncini, \u201cQuaternion convolutional neural networks for detection and localization of 3D sound events,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2019, pp. 8533\u20138537."
                    },
                    {
                        "ref-fulltext": "R. K. Furness, \u201cAmbisonics\u2013an overview,\u201d in Proc. AES 8th Int. Conf. Sound Audio. Audio Eng. Soc., pp. 181\u2013190, Washington, DC, May 1990.",
                        "@reference-instance-id": "OB2BibRecID-946469326-20efa0c0012b3ef0ffc968790564b30e-51",
                        "@id": "51",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "1990"},
                            "ref-title": {"ref-titletext": "Ambisonics\u2013an overview"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85128556752",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "181",
                                "@last": "190"
                            }},
                            "ref-text": "Washington, DC, May",
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "R.K.",
                                "@_fa": "true",
                                "ce:surname": "Furness",
                                "ce:indexed-name": "Furness R.K."
                            }]},
                            "ref-sourcetitle": "Proc. AES 8th Int. Conf. Sound Audio. Audio Eng. Soc."
                        },
                        "ce:source-text": "R. K. Furness, \u201cAmbisonics\u2013an overview,\u201d in Proc. AES 8th Int. Conf. Sound Audio. Audio Eng. Soc., pp. 181\u2013190, Washington, DC, May 1990."
                    },
                    {
                        "ref-fulltext": "X. Qiu, T. Parcollet, M. Ravanelli, N. Lane, and M. Morchid, \u201cQuaternion neural networks for multi-channel distant speech recognition,\u201d in Proc. INTERSPEECH, 2020, pp. 329\u2013333.",
                        "@reference-instance-id": "OB2BibRecID-946469326-9f3cc41c7e918d8b1915d9024c61465c-52",
                        "@id": "52",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Quaternion neural networks for multi-channel distant speech recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85098122685",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2020"},
                                "pagerange": {
                                    "@first": "329",
                                    "@last": "333"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "X.",
                                    "@_fa": "true",
                                    "ce:surname": "Qiu",
                                    "ce:indexed-name": "Qiu X."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Parcollet",
                                    "ce:indexed-name": "Parcollet T."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Ravanelli",
                                    "ce:indexed-name": "Ravanelli M."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "N.",
                                    "@_fa": "true",
                                    "ce:surname": "Lane",
                                    "ce:indexed-name": "Lane N."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Morchid",
                                    "ce:indexed-name": "Morchid M."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. INTERSPEECH"
                        },
                        "ce:source-text": "X. Qiu, T. Parcollet, M. Ravanelli, N. Lane, and M. Morchid, \u201cQuaternion neural networks for multi-channel distant speech recognition,\u201d in Proc. INTERSPEECH, 2020, pp. 329\u2013333."
                    },
                    {
                        "ref-fulltext": "C. Brignone, G. Mancini, E. Grassucci, A. Uncini, and D. Comminiello, \u201cEfficient sound event localization and detection in the quaternion domain,\u201d IEEE Trans. Circuits Syst. II: Exp. Briefs, vol. 69, no. 5, pp. 2453\u20132457, May 2022.",
                        "@reference-instance-id": "OB2BibRecID-946469326-45eaaec9e0a15bfe91d70ca43ecfad4a-53",
                        "@id": "53",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2022"},
                            "ref-title": {"ref-titletext": "Efficient sound event localization and detection in the quaternion domain"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85126716404",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "69",
                                    "@issue": "5"
                                },
                                "pagerange": {
                                    "@first": "2453",
                                    "@last": "2457"
                                }
                            },
                            "ref-text": "May",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Brignone",
                                    "ce:indexed-name": "Brignone C."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Mancini",
                                    "ce:indexed-name": "Mancini G."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Grassucci",
                                    "ce:indexed-name": "Grassucci E."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Uncini",
                                    "ce:indexed-name": "Uncini A."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans. Circuits Syst. II: Exp. Briefs"
                        },
                        "ce:source-text": "C. Brignone, G. Mancini, E. Grassucci, A. Uncini, and D. Comminiello, \u201cEfficient sound event localization and detection in the quaternion domain,\u201d IEEE Trans. Circuits Syst. II: Exp. Briefs, vol. 69, no. 5, pp. 2453\u20132457, May 2022."
                    },
                    {
                        "ref-fulltext": "E. Grassucci, G. Mancini, C. Brignone, A. Uncini, and D. Comminiello, \u201cDual quaternion ambisonics array for six-degree-of-freedom acoustic representation,\u201d Pattern Recognit. Lett., vol. 166, pp. 24\u201330, 2023.",
                        "@reference-instance-id": "OB2BibRecID-946469326-0e04e0964e4772721e731aae8fd1deb8-54",
                        "@id": "54",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Dual quaternion ambisonics array for six-degree-of-freedom acoustic representation"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85145774706",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "166"},
                                "pagerange": {
                                    "@first": "24",
                                    "@last": "30"
                                }
                            },
                            "ref-text": "2023",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "E.",
                                    "@_fa": "true",
                                    "ce:surname": "Grassucci",
                                    "ce:indexed-name": "Grassucci E."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Mancini",
                                    "ce:indexed-name": "Mancini G."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Brignone",
                                    "ce:indexed-name": "Brignone C."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Uncini",
                                    "ce:indexed-name": "Uncini A."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Comminiello",
                                    "ce:indexed-name": "Comminiello D."
                                }
                            ]},
                            "ref-sourcetitle": "Pattern Recognit. Lett."
                        },
                        "ce:source-text": "E. Grassucci, G. Mancini, C. Brignone, A. Uncini, and D. Comminiello, \u201cDual quaternion ambisonics array for six-degree-of-freedom acoustic representation,\u201d Pattern Recognit. Lett., vol. 166, pp. 24\u201330, 2023."
                    },
                    {
                        "ref-fulltext": "T. Parcollet, M. Morchid, X. Bost, G. Linarès, and R. D. Mori, \u201cReal to H-space autoencoders for theme identification in telephone conversations,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 198\u2013210, 2020.",
                        "@reference-instance-id": "OB2BibRecID-946469326-4d22e38268a0d8b11afeeb48a23cdaea-55",
                        "@id": "55",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Real to H-space autoencoders for theme identification in telephone conversations"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85077218181",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "28"},
                                "pagerange": {
                                    "@first": "198",
                                    "@last": "210"
                                }
                            },
                            "ref-text": "2020",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Parcollet",
                                    "ce:indexed-name": "Parcollet T."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Morchid",
                                    "ce:indexed-name": "Morchid M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "X.",
                                    "@_fa": "true",
                                    "ce:surname": "Bost",
                                    "ce:indexed-name": "Bost X."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Linarès",
                                    "ce:indexed-name": "Linares G."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "R.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Mori",
                                    "ce:indexed-name": "Mori R.D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE/ACM Trans. Audio, Speech, Lang. Process."
                        },
                        "ce:source-text": "T. Parcollet, M. Morchid, X. Bost, G. Linarès, and R. D. Mori, \u201cReal to H-space autoencoders for theme identification in telephone conversations,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 28, pp. 198\u2013210, 2020."
                    },
                    {
                        "ref-fulltext": "C. J. Gaudet and A. S. Maida, \u201cDeep quaternion networks,\u201d in Proc. IEEE Int. Joint Conf. Neural Netw., 2018, pp. 1\u20138.",
                        "@reference-instance-id": "OB2BibRecID-946469326-6e5bf4113ec3388123b7c8292fdd77a6-56",
                        "@id": "56",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "Deep quaternion networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85056554293",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1",
                                "@last": "8"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "C.J.",
                                    "@_fa": "true",
                                    "ce:surname": "Gaudet",
                                    "ce:indexed-name": "Gaudet C.J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.S.",
                                    "@_fa": "true",
                                    "ce:surname": "Maida",
                                    "ce:indexed-name": "Maida A.S."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Joint Conf. Neural Netw."
                        },
                        "ce:source-text": "C. J. Gaudet and A. S. Maida, \u201cDeep quaternion networks,\u201d in Proc. IEEE Int. Joint Conf. Neural Netw., 2018, pp. 1\u20138."
                    },
                    {
                        "ref-fulltext": "B. C. Ujang, C. C. Took, and D. P. Mandic, \u201cQuaternion-valued nonlinear adaptive filtering,\u201d IEEE Trans. Neural Netw., vol. 22, no. 8, pp. 1193\u20131206, Aug. 2011.",
                        "@reference-instance-id": "OB2BibRecID-946469326-4a46839de126ff5e769489a0968a62a3-57",
                        "@id": "57",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2011"},
                            "ref-title": {"ref-titletext": "Quaternion-valued nonlinear adaptive filtering"},
                            "refd-itemidlist": {"itemid": {
                                "$": "80051550143",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "22",
                                    "@issue": "8"
                                },
                                "pagerange": {
                                    "@first": "1193",
                                    "@last": "1206"
                                }
                            },
                            "ref-text": "Aug",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "B.C.",
                                    "@_fa": "true",
                                    "ce:surname": "Ujang",
                                    "ce:indexed-name": "Ujang B.C."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "C.C.",
                                    "@_fa": "true",
                                    "ce:surname": "Took",
                                    "ce:indexed-name": "Took C.C."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "D.P.",
                                    "@_fa": "true",
                                    "ce:surname": "Mandic",
                                    "ce:indexed-name": "Mandic D.P."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans. Neural Netw."
                        },
                        "ce:source-text": "B. C. Ujang, C. C. Took, and D. P. Mandic, \u201cQuaternion-valued nonlinear adaptive filtering,\u201d IEEE Trans. Neural Netw., vol. 22, no. 8, pp. 1193\u20131206, Aug. 2011."
                    },
                    {
                        "ref-fulltext": "C. Busso et al., \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Lang. Resour. Eval., vol. 42, no. 4, pp. 335\u2013359, 2008.",
                        "@reference-instance-id": "OB2BibRecID-946469326-dec57f0b3b10b3b14f94414a70f7fee3-58",
                        "@id": "58",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2008"},
                            "ref-title": {"ref-titletext": "IEMoCAp: Interactive emotional dyadic motion capture database"},
                            "refd-itemidlist": {"itemid": {
                                "$": "59849093076",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "42",
                                    "@issue": "4"
                                },
                                "pagerange": {
                                    "@first": "335",
                                    "@last": "359"
                                }
                            },
                            "ref-authors": {
                                "author": [{
                                    "@seq": "1",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Busso",
                                    "ce:indexed-name": "Busso C."
                                }],
                                "et-al": null
                            },
                            "ref-sourcetitle": "Lang. Resour. Eval."
                        },
                        "ce:source-text": "C. Busso et al., \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Lang. Resour. Eval., vol. 42, no. 4, pp. 335\u2013359, 2008."
                    },
                    {
                        "ref-fulltext": "D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. Int. Conf. Learn. Representations, San Diego, CA, May 2015, pp. 1\u201313.",
                        "@reference-instance-id": "OB2BibRecID-946469326-1a35d501f539c96d345391d25a2b2a37-59",
                        "@id": "59",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "ADaM: A method for stochastic optimization"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85083951076",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1",
                                "@last": "13"
                            }},
                            "ref-text": "San Diego, CA, May",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "D.P.",
                                    "@_fa": "true",
                                    "ce:surname": "Kingma",
                                    "ce:indexed-name": "Kingma D.P."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Ba",
                                    "ce:indexed-name": "Ba J."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. Int. Conf. Learn. Representations"
                        },
                        "ce:source-text": "D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. Int. Conf. Learn. Representations, San Diego, CA, May 2015, pp. 1\u201313."
                    },
                    {
                        "ref-fulltext": "S. R. Livingstone and F. A. Russo, \u201cThe Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English,\u201d PLoS One, vol. 13, no. 5, 2018, Art. no. e0196391.",
                        "@reference-instance-id": "OB2BibRecID-946469326-da21330c5587cb082de672f96712ea04-60",
                        "@id": "60",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "e0196391",
                                    "@idtype": "ARTNUM"
                                },
                                {
                                    "$": "85047203203",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"voliss": {
                                "@volume": "13",
                                "@issue": "5"
                            }},
                            "ref-text": "Art.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.R.",
                                    "@_fa": "true",
                                    "ce:surname": "Livingstone",
                                    "ce:indexed-name": "Livingstone S.R."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "F.A.",
                                    "@_fa": "true",
                                    "ce:surname": "Russo",
                                    "ce:indexed-name": "Russo F.A."
                                }
                            ]},
                            "ref-sourcetitle": "PLoS One"
                        },
                        "ce:source-text": "S. R. Livingstone and F. A. Russo, \u201cThe Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English,\u201d PLoS One, vol. 13, no. 5, 2018, Art. no. e0196391."
                    },
                    {
                        "ref-fulltext": "F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss, \u201cA database of German emotional speech,\u201d in Proc. Eur. Conf. Speech Commun. Technol., 2005, pp. 1517\u20131520.",
                        "@reference-instance-id": "OB2BibRecID-946469326-f467e8ae1daec7effe50d98b83b504f2-61",
                        "@id": "61",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2005"},
                            "ref-title": {"ref-titletext": "A database of German emotional speech"},
                            "refd-itemidlist": {"itemid": {
                                "$": "33745202280",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1517",
                                "@last": "1520"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Burkhardt",
                                    "ce:indexed-name": "Burkhardt F."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Paeschke",
                                    "ce:indexed-name": "Paeschke A."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Rolfes",
                                    "ce:indexed-name": "Rolfes M."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "W.F.",
                                    "@_fa": "true",
                                    "ce:surname": "Sendlmeier",
                                    "ce:indexed-name": "Sendlmeier W.F."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Weiss",
                                    "ce:indexed-name": "Weiss B."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. Eur. Conf. Speech Commun. Technol."
                        },
                        "ce:source-text": "F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss, \u201cA database of German emotional speech,\u201d in Proc. Eur. Conf. Speech Commun. Technol., 2005, pp. 1517\u20131520."
                    },
                    {
                        "ref-fulltext": "K. Dupuis and M. K. Pichora-Fuller, \u201cRecognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set,\u201d Can. Acoust., vol. 39, no. 3, pp. 182\u2013183, 2011.",
                        "@reference-instance-id": "OB2BibRecID-946469326-a4fba7933d5fa6edffbed1b070a7d703-62",
                        "@id": "62",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2011"},
                            "ref-title": {"ref-titletext": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84859497773",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {
                                    "@volume": "39",
                                    "@issue": "3"
                                },
                                "pagerange": {
                                    "@first": "182",
                                    "@last": "183"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Dupuis",
                                    "ce:indexed-name": "Dupuis K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.K.",
                                    "@_fa": "true",
                                    "ce:surname": "Pichora-Fuller",
                                    "ce:indexed-name": "Pichora-Fuller M.K."
                                }
                            ]},
                            "ref-sourcetitle": "Can. Acoust."
                        },
                        "ce:source-text": "K. Dupuis and M. K. Pichora-Fuller, \u201cRecognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set,\u201d Can. Acoust., vol. 39, no. 3, pp. 182\u2013183, 2011."
                    },
                    {
                        "ref-fulltext": "K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d in Proc. Int. Conf. Learn. Representations, San Diego, CA, May 2015, pp. 1\u201314.",
                        "@reference-instance-id": "OB2BibRecID-946469326-f400186a9f433f43195f58a369e3d00d-63",
                        "@id": "63",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Very deep convolutional networks for large-scale image recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85083953063",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "1",
                                "@last": "14"
                            }},
                            "ref-text": "San Diego, CA, May",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Simonyan",
                                    "ce:indexed-name": "Simonyan K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Zisserman",
                                    "ce:indexed-name": "Zisserman A."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. Int. Conf. Learn. Representations"
                        },
                        "ce:source-text": "K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d in Proc. Int. Conf. Learn. Representations, San Diego, CA, May 2015, pp. 1\u201314."
                    },
                    {
                        "ref-fulltext": "A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d Adv. Neural Inf. Process. Syst., vol. 25, pp. 1097\u20131105, 2012.",
                        "@reference-instance-id": "OB2BibRecID-946469326-b36621e22ff6d21ac5aeeb1949bdd50e-64",
                        "@id": "64",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2012"},
                            "ref-title": {"ref-titletext": "ImageNet classification with deep convolutional neural networks"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84876231242",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "25"},
                                "pagerange": {
                                    "@first": "1097",
                                    "@last": "1105"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Krizhevsky",
                                    "ce:indexed-name": "Krizhevsky A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "I.",
                                    "@_fa": "true",
                                    "ce:surname": "Sutskever",
                                    "ce:indexed-name": "Sutskever I."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "G.E.",
                                    "@_fa": "true",
                                    "ce:surname": "Hinton",
                                    "ce:indexed-name": "Hinton G.E."
                                }
                            ]},
                            "ref-sourcetitle": "Adv. Neural Inf. Process. Syst."
                        },
                        "ce:source-text": "A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d Adv. Neural Inf. Process. Syst., vol. 25, pp. 1097\u20131105, 2012."
                    },
                    {
                        "ref-fulltext": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770\u2013778.",
                        "@reference-instance-id": "OB2BibRecID-946469326-00b7179a25f8c4792083813e423081ba-65",
                        "@id": "65",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "Deep residual learning for image recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84986274465",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {"pagerange": {
                                "@first": "770",
                                "@last": "778"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "He",
                                    "ce:indexed-name": "He K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "X.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhang",
                                    "ce:indexed-name": "Zhang X."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Ren",
                                    "ce:indexed-name": "Ren S."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Sun",
                                    "ce:indexed-name": "Sun J."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit."
                        },
                        "ce:source-text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770\u2013778."
                    },
                    {
                        "ref-fulltext": "N. T. Pham, D. N. M. Dang, and S. D. Nguyen, \u201cHybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition,\u201d 2021, arXiv:2109.09026.",
                        "@reference-instance-id": "OB2BibRecID-946469326-e4c3861fc51be7b0ac695968b19bec5d-66",
                        "@id": "66",
                        "ref-info": {
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "2109.09026",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85150987961",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"voliss": {"@volume": "2021"}},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "N.T.",
                                    "@_fa": "true",
                                    "ce:surname": "Pham",
                                    "ce:indexed-name": "Pham N.T."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.N.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Dang",
                                    "ce:indexed-name": "Dang D.N.M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Nguyen",
                                    "ce:indexed-name": "Nguyen S.D."
                                }
                            ]},
                            "ref-sourcetitle": "Hybrid Data Augmentation and Deep Attention-Based Dilated Convolutional-Recurrent Neural Networks for Speech Emotion Recognition"
                        },
                        "ce:source-text": "N. T. Pham, D. N. M. Dang, and S. D. Nguyen, \u201cHybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition,\u201d 2021, arXiv:2109.09026."
                    },
                    {
                        "ref-fulltext": "S. Jothimani and K. Premalatha, \u201cMFF-SAUG: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network,\u201d Chaos, Solitons Fractals, vol. 162, 2022, Art. no. 112512.",
                        "@reference-instance-id": "OB2BibRecID-946469326-0dfc979c073bd2f01a67c9db2aa4320a-67",
                        "@id": "67",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "MFF-SAUG: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "112512",
                                    "@idtype": "ARTNUM"
                                },
                                {
                                    "$": "85135315781",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {
                                "voliss": {"@volume": "162"},
                                "pagerange": {"@first": "2022"}
                            },
                            "ref-text": "Art.",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Jothimani",
                                    "ce:indexed-name": "Jothimani S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Premalatha",
                                    "ce:indexed-name": "Premalatha K."
                                }
                            ]},
                            "ref-sourcetitle": "Chaos, Solitons Fractals"
                        },
                        "ce:source-text": "S. Jothimani and K. Premalatha, \u201cMFF-SAUG: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network,\u201d Chaos, Solitons Fractals, vol. 162, 2022, Art. no. 112512."
                    },
                    {
                        "ref-fulltext": "C. Etienne, G. Fidanza, A. Petrovskii, L. Devillers, and B. Schmauch, \u201cSpeech emotion recognition with data augmentation and layer-wise learning rate adjustment,\u201d Feb. 2018 arXiv:1802.05630.",
                        "@reference-instance-id": "OB2BibRecID-946469326-9c2623c5ef8bc16ca899c4edacb603cd-68",
                        "@id": "68",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "1802.05630",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85063111753",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-text": "Feb",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Etienne",
                                    "ce:indexed-name": "Etienne C."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Fidanza",
                                    "ce:indexed-name": "Fidanza G."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Petrovskii",
                                    "ce:indexed-name": "Petrovskii A."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Devillers",
                                    "ce:indexed-name": "Devillers L."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "B.",
                                    "@_fa": "true",
                                    "ce:surname": "Schmauch",
                                    "ce:indexed-name": "Schmauch B."
                                }
                            ]},
                            "ref-sourcetitle": "Speech Emotion Recognition with Data Augmentation and Layer-Wise Learning Rate Adjustment"
                        },
                        "ce:source-text": "C. Etienne, G. Fidanza, A. Petrovskii, L. Devillers, and B. Schmauch, \u201cSpeech emotion recognition with data augmentation and layer-wise learning rate adjustment,\u201d Feb. 2018 arXiv:1802.05630."
                    },
                    {
                        "ref-fulltext": "M. Xu, F. Zhang, X. Cui, and W. Zhang, \u201cSpeech emotion recognition with multiscale area attention and data augmentation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 6319\u20136323.",
                        "@reference-instance-id": "OB2BibRecID-946469326-1a369da3f899dafeaead2a293526f406-69",
                        "@id": "69",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Speech emotion recognition with multiscale area attention and data augmentation"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85115134779",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2021"},
                                "pagerange": {
                                    "@first": "6319",
                                    "@last": "6323"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Xu",
                                    "ce:indexed-name": "Xu M."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhang",
                                    "ce:indexed-name": "Zhang F."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "X.",
                                    "@_fa": "true",
                                    "ce:surname": "Cui",
                                    "ce:indexed-name": "Cui X."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "W.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhang",
                                    "ce:indexed-name": "Zhang W."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process."
                        },
                        "ce:source-text": "M. Xu, F. Zhang, X. Cui, and W. Zhang, \u201cSpeech emotion recognition with multiscale area attention and data augmentation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2021, pp. 6319\u20136323."
                    },
                    {
                        "ref-fulltext": "N.-H. Ho, H.-J. Yang, S.-H. Kim, and G. Lee, \u201cMultimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network,\u201d IEEE Access, vol. 8, pp. 61672\u201361686, 2020.",
                        "@reference-instance-id": "OB2BibRecID-946469326-3b0a28962aa321e8383280b53fff98d9-70",
                        "@id": "70",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2020"},
                            "ref-title": {"ref-titletext": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85083424488",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "8"},
                                "pagerange": {
                                    "@first": "61672",
                                    "@last": "61686"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "N.-H.",
                                    "@_fa": "true",
                                    "ce:surname": "Ho",
                                    "ce:indexed-name": "Ho N.-H."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "H.-J.",
                                    "@_fa": "true",
                                    "ce:surname": "Yang",
                                    "ce:indexed-name": "Yang H.-J."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.-H.",
                                    "@_fa": "true",
                                    "ce:surname": "Kim",
                                    "ce:indexed-name": "Kim S.-H."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "G.",
                                    "@_fa": "true",
                                    "ce:surname": "Lee",
                                    "ce:indexed-name": "Lee G."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Access"
                        },
                        "ce:source-text": "N.-H. Ho, H.-J. Yang, S.-H. Kim, and G. Lee, \u201cMultimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network,\u201d IEEE Access, vol. 8, pp. 61672\u201361686, 2020."
                    },
                    {
                        "ref-fulltext": "S. Kakouros, T. Stafylakis, L. Mosner, and L. Burget, \u201cSpeech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing,\u201d in Proc. 48th IEEE Int. Conf. Acoust., Speech, Signal Process., Rhodes, Greece, Jun. 2023.",
                        "@reference-instance-id": "OB2BibRecID-946469326-d6dd81a5670a5fa18afdc7111242e8ce-71",
                        "@id": "71",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2023"},
                            "ref-title": {"ref-titletext": "Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85150955542",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "Rhodes, Greece, Jun",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Kakouros",
                                    "ce:indexed-name": "Kakouros S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Stafylakis",
                                    "ce:indexed-name": "Stafylakis T."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Mosner",
                                    "ce:indexed-name": "Mosner L."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Burget",
                                    "ce:indexed-name": "Burget L."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. 48th IEEE Int. Conf. Acoust., Speech, Signal Process."
                        },
                        "ce:source-text": "S. Kakouros, T. Stafylakis, L. Mosner, and L. Burget, \u201cSpeech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing,\u201d in Proc. 48th IEEE Int. Conf. Acoust., Speech, Signal Process., Rhodes, Greece, Jun. 2023."
                    },
                    {
                        "ref-fulltext": "S. Latif, R. Rana, and J. Qadir, \u201cAdversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness,\u201d 2018, arXiv:1811.11402.",
                        "@reference-instance-id": "OB2BibRecID-946469326-40a195cff8d1380577cbba7840b86c33-72",
                        "@id": "72",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "1811.11402",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85086636023",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Latif",
                                    "ce:indexed-name": "Latif S."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Rana",
                                    "ce:indexed-name": "Rana R."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Qadir",
                                    "ce:indexed-name": "Qadir J."
                                }
                            ]},
                            "ref-sourcetitle": "Adversarial Machine Learning and Speech Emotion Recognition: Utilizing Generative Adversarial Networks for Robustness"
                        },
                        "ce:source-text": "S. Latif, R. Rana, and J. Qadir, \u201cAdversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness,\u201d 2018, arXiv:1811.11402."
                    },
                    {
                        "ref-fulltext": "Y. L. Bouali, O. B. Ahmed, and S. Mazouzi, \u201cCross-modal learning for audio-visual emotion recognition in acted speech,\u201d in Proc. IEEE 6th Int. Conf. Adv. Technol. Signal Image Process., 2022, pp. 1\u20136.",
                        "@reference-instance-id": "OB2BibRecID-946469326-df2e26941f238109d26cce7c58ac7dcc-73",
                        "@id": "73",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Cross-modal learning for audio-visual emotion recognition in acted speech"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85134253015",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2022"},
                                "pagerange": {
                                    "@first": "1",
                                    "@last": "6"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "Y.L.",
                                    "@_fa": "true",
                                    "ce:surname": "Bouali",
                                    "ce:indexed-name": "Bouali Y.L."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "O.B.",
                                    "@_fa": "true",
                                    "ce:surname": "Ahmed",
                                    "ce:indexed-name": "Ahmed O.B."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Mazouzi",
                                    "ce:indexed-name": "Mazouzi S."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. IEEE 6th Int. Conf. Adv. Technol. Signal Image Process."
                        },
                        "ce:source-text": "Y. L. Bouali, O. B. Ahmed, and S. Mazouzi, \u201cCross-modal learning for audio-visual emotion recognition in acted speech,\u201d in Proc. IEEE 6th Int. Conf. Adv. Technol. Signal Image Process., 2022, pp. 1\u20136."
                    },
                    {
                        "ref-fulltext": "T. Kim and P. Vossen, \u201cEmoberta: Speaker-aware emotion recognition in conversation with RoBERTa,\u201d 2021, arXiv:2108.12009.",
                        "@reference-instance-id": "OB2BibRecID-946469326-b4af8118f30ac95270c5394c870200e2-74",
                        "@id": "74",
                        "ref-info": {
                            "refd-itemidlist": {"itemid": [
                                {
                                    "$": "2108.12009",
                                    "@idtype": "ARXIV"
                                },
                                {
                                    "$": "85150999865",
                                    "@idtype": "SGR"
                                }
                            ]},
                            "ref-volisspag": {"pagerange": {"@first": "2021"}},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Kim",
                                    "ce:indexed-name": "Kim T."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Vossen",
                                    "ce:indexed-name": "Vossen P."
                                }
                            ]},
                            "ref-sourcetitle": "Emoberta: Speaker-Aware Emotion Recognition in Conversation with RoBERTa"
                        },
                        "ce:source-text": "T. Kim and P. Vossen, \u201cEmoberta: Speaker-aware emotion recognition in conversation with RoBERTa,\u201d 2021, arXiv:2108.12009."
                    },
                    {
                        "ref-fulltext": "J. Li, D. Ji, F. Li, M. Zhang, and Y. Liu, \u201cHitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations,\u201d in Proc. 28th Int. Conf. Comput. Linguistics, 2020, pp. 4190\u20134200.",
                        "@reference-instance-id": "OB2BibRecID-946469326-7b978b39553db87107b310eecf39c72c-75",
                        "@id": "75",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "HITrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85107672845",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "2020"},
                                "pagerange": {
                                    "@first": "4190",
                                    "@last": "4200"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Li",
                                    "ce:indexed-name": "Li J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "D.",
                                    "@_fa": "true",
                                    "ce:surname": "Ji",
                                    "ce:indexed-name": "Ji D."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "F.",
                                    "@_fa": "true",
                                    "ce:surname": "Li",
                                    "ce:indexed-name": "Li F."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Zhang",
                                    "ce:indexed-name": "Zhang M."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Liu",
                                    "ce:indexed-name": "Liu Y."
                                }
                            ]},
                            "ref-sourcetitle": "Proc. 28th Int. Conf. Comput. Linguistics"
                        },
                        "ce:source-text": "J. Li, D. Ji, F. Li, M. Zhang, and Y. Liu, \u201cHitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations,\u201d in Proc. 28th Int. Conf. Comput. Linguistics, 2020, pp. 4190\u20134200."
                    }
                ]
            }}
        }
    },
    "affiliation": [
        {
            "affiliation-city": "London",
            "@id": "60033387",
            "affilname": "University of London",
            "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60033387",
            "affiliation-country": "United Kingdom"
        },
        {
            "affiliation-city": "Rome",
            "@id": "60032350",
            "affilname": "Sapienza Università di Roma",
            "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60032350",
            "affiliation-country": "Italy"
        }
    ],
    "coredata": {
        "srctype": "j",
        "eid": "2-s2.0-85149423370",
        "dc:description": "The modeling of human emotion expression in speech signals is an important, yet challenging task. The high resource demand of speech emotion recognition models, combined with the general scarcity of emotion-labelled data are obstacles to the development and application of effective solutions in this field. In this paper, we present an approach to jointly circumvent these difficulties. Our method, named RH-emo, is a novel semi-supervised architecture aimed at extracting quaternion embeddings from real-valued monoaural spectrograms, enabling the use of quaternion-valued networks for speech emotion recognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that consists of a real-valued encoder in parallel to a real-valued emotion classifier and a quaternion-valued decoder. On the one hand, the classifier permits to optimization of each latent axis of the embeddings for the classification of a specific emotion-related characteristic: valence, arousal, dominance, and overall emotion. On the other hand, quaternion reconstruction enables the latent dimension to develop intra-channel correlations that are required for an effective representation as a quaternion entity. We test our approach on speech emotion recognition tasks using four popular datasets: IEMOCAP, RAVDESS, EmoDB, and TESS, comparing the performance of three well-established real-valued CNN architectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent fed with the embeddings created with RH-emo. We obtain a consistent improvement in the test accuracy for all datasets, while drastically reducing the resources' demand of models. Moreover, we performed additional experiments and ablation studies that confirm the effectiveness of our approach.",
        "prism:coverDate": "2023-01-01",
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/abstract/scopus_id/85149423370",
        "dc:creator": {"author": [{
            "ce:given-name": "Eric",
            "preferred-name": {
                "ce:given-name": "Eric",
                "ce:initials": "E.",
                "ce:surname": "Guizzo",
                "ce:indexed-name": "Guizzo E."
            },
            "@seq": "1",
            "ce:initials": "E.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60033387",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60033387"
            },
            "ce:surname": "Guizzo",
            "@auid": "57210359479",
            "author-url": "https://api.elsevier.com/content/author/author_id/57210359479",
            "ce:indexed-name": "Guizzo E."
        }]},
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/abstract/scopus_id/85149423370"
            },
            {
                "@_fa": "true",
                "@rel": "scopus",
                "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85149423370&origin=inward"
            },
            {
                "@_fa": "true",
                "@rel": "scopus-citedby",
                "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85149423370&origin=inward"
            }
        ],
        "source-id": "21100368801",
        "citedby-count": "2",
        "prism:volume": "31",
        "subtype": "ar",
        "dc:title": "Learning Speech Emotion Representations in the Quaternion Domain",
        "openaccess": "1",
        "prism:issn": "23299304 23299290",
        "publishercopyright": "© 2014 IEEE.",
        "subtypeDescription": "Article",
        "prism:publicationName": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "prism:pageRange": "1200-1212",
        "prism:endingPage": "1212",
        "openaccessFlag": "true",
        "prism:doi": "10.1109/TASLP.2023.3250840",
        "prism:startingPage": "1200",
        "dc:identifier": "SCOPUS_ID:85149423370",
        "dc:publisher": "Institute of Electrical and Electronics Engineers Inc."
    },
    "idxterms": {"mainterm": [
        {
            "$": "Embeddings",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Emotion recognition",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Features extraction",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Neural-networks",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Quaternion",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Quaternion algebra",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Quaternion neural network",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Speech emotion recognition",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Task analysis",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Transferable embedding",
            "@weight": "b",
            "@candidate": "n"
        }
    ]},
    "language": {"@xml:lang": "eng"},
    "authkeywords": {"author-keyword": [
        {
            "@_fa": "true",
            "$": "quaternion algebra"
        },
        {
            "@_fa": "true",
            "$": "quaternion neural networks"
        },
        {
            "@_fa": "true",
            "$": "Speech emotion recognition"
        },
        {
            "@_fa": "true",
            "$": "transferable embeddings"
        }
    ]},
    "subject-areas": {"subject-area": [
        {
            "@_fa": "true",
            "$": "Computer Science (miscellaneous)",
            "@code": "1701",
            "@abbrev": "COMP"
        },
        {
            "@_fa": "true",
            "$": "Acoustics and Ultrasonics",
            "@code": "3102",
            "@abbrev": "PHYS"
        },
        {
            "@_fa": "true",
            "$": "Computational Mathematics",
            "@code": "2605",
            "@abbrev": "MATH"
        },
        {
            "@_fa": "true",
            "$": "Electrical and Electronic Engineering",
            "@code": "2208",
            "@abbrev": "ENGI"
        }
    ]},
    "authors": {"author": [
        {
            "ce:given-name": "Eric",
            "preferred-name": {
                "ce:given-name": "Eric",
                "ce:initials": "E.",
                "ce:surname": "Guizzo",
                "ce:indexed-name": "Guizzo E."
            },
            "@seq": "1",
            "ce:initials": "E.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60033387",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60033387"
            },
            "ce:surname": "Guizzo",
            "@auid": "57210359479",
            "author-url": "https://api.elsevier.com/content/author/author_id/57210359479",
            "ce:indexed-name": "Guizzo E."
        },
        {
            "ce:given-name": "Tillman",
            "preferred-name": {
                "ce:given-name": "Tillman",
                "ce:initials": "T.",
                "ce:surname": "Weyde",
                "ce:indexed-name": "Weyde T."
            },
            "@seq": "2",
            "ce:initials": "T.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60033387",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60033387"
            },
            "ce:surname": "Weyde",
            "@auid": "24476899500",
            "author-url": "https://api.elsevier.com/content/author/author_id/24476899500",
            "ce:indexed-name": "Weyde T."
        },
        {
            "ce:given-name": "Simone",
            "preferred-name": {
                "ce:given-name": "Simone",
                "ce:initials": "S.",
                "ce:surname": "Scardapane",
                "ce:indexed-name": "Scardapane S."
            },
            "@seq": "3",
            "ce:initials": "S.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60032350",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60032350"
            },
            "ce:surname": "Scardapane",
            "@auid": "55772102700",
            "author-url": "https://api.elsevier.com/content/author/author_id/55772102700",
            "ce:indexed-name": "Scardapane S."
        },
        {
            "ce:given-name": "Danilo",
            "preferred-name": {
                "ce:given-name": "Danilo",
                "ce:initials": "D.",
                "ce:surname": "Comminiello",
                "ce:indexed-name": "Comminiello D."
            },
            "@seq": "4",
            "ce:initials": "D.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60032350",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60032350"
            },
            "ce:surname": "Comminiello",
            "@auid": "36444807900",
            "author-url": "https://api.elsevier.com/content/author/author_id/36444807900",
            "ce:indexed-name": "Comminiello D."
        }
    ]}
}}