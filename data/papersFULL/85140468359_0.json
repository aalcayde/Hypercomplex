{"abstracts-retrieval-response": {
    "item": {
        "ait:process-info": {
            "ait:status": {
                "@state": "update",
                "@type": "core",
                "@stage": "S300"
            },
            "ait:date-delivered": {
                "@day": "30",
                "@timestamp": "2022-10-30T19:08:08.000008-04:00",
                "@year": "2022",
                "@month": "10"
            },
            "ait:date-sort": {
                "@day": "01",
                "@year": "2023",
                "@month": "01"
            }
        },
        "bibrecord": {
            "head": {
                "author-group": [
                    {
                        "affiliation": {
                            "country": "Japan",
                            "@afid": "60025272",
                            "@country": "jpn",
                            "city": "Tokyo",
                            "organization": [
                                {"$": "Graduate School of Engineering Aeronautics and Astronautics"},
                                {"$": "The University of Tokyo"}
                            ],
                            "affiliation-id": {
                                "@afid": "60025272",
                                "@dptid": "104281083"
                            },
                            "ce:source-text": "Graduate School of Engineering Aeronautics and Astronautics, The University of Tokyo, Tokyo, Japan",
                            "@dptid": "104281083"
                        },
                        "author": [{
                            "ce:given-name": "Park",
                            "preferred-name": {
                                "ce:given-name": "Park",
                                "ce:initials": "P.",
                                "ce:surname": "Kunbum",
                                "ce:indexed-name": "Kunbum P."
                            },
                            "@seq": "1",
                            "ce:initials": "P.",
                            "@_fa": "true",
                            "@type": "auth",
                            "ce:surname": "Kunbum",
                            "@auid": "57939221500",
                            "ce:indexed-name": "Kunbum P."
                        }]
                    },
                    {
                        "affiliation": {
                            "country": "Japan",
                            "address-part": "Hongo, Bunkyou-ku",
                            "postal-code": "113-8656",
                            "@afid": "60025272",
                            "@country": "jpn",
                            "city": "Tokyo",
                            "organization": {"$": "University of Tokyo"},
                            "affiliation-id": {"@afid": "60025272"},
                            "ce:source-text": "University of Tokyo, Hongo, Bunkyou-ku, Tokyo, 113-8656, Japan"
                        },
                        "author": [{
                            "ce:given-name": "Takeshi",
                            "preferred-name": {
                                "ce:given-name": "Takeshi",
                                "ce:initials": "T.",
                                "ce:surname": "Tsuchiya",
                                "ce:indexed-name": "Tsuchiya T."
                            },
                            "@seq": "2",
                            "ce:initials": "T.",
                            "@_fa": "true",
                            "@type": "auth",
                            "ce:surname": "Tsuchiya",
                            "@auid": "55754198600",
                            "ce:indexed-name": "Tsuchiya T."
                        }]
                    }
                ],
                "citation-title": "3D Reconstruction by Pretrained Features and Visual-Inertial Odometry",
                "abstracts": "Â© 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.The goal of my paper is to create a new framework which provides sufficient semantic information for decision makings as a core component of applications such as SLAM (Simultaneous Localization And Mapping), robotics, AR (Augmented Reality), autonomous driving, etc. This framework does not provide dense point clouds. Rather, a scene is described with several features which the agent has been trained to recognize. Specifically, scenes are generated by extracting features from the space\u2019s occupancy, the location of the light source, shape of the object, colors, and textures in the images. The extraction of the features is conducted by ensemble with deep-learning based feature extractors, traditional machine learning algorithms and image processing techniques. The deep-learning based feature extractors are trained by a supervised learning with data augmentation over 3D models; and the results of inferences are evaluated by a depth camera and retrained by unsupervised learning. Using existed methods it is difficult to utilize semantic information for unknown objects. But the agent in this methodology tries to describe them as much as possible by utilizing information trained in advance. For an odometry module, which estimates attitudes and positions, is implemented by a typical feature-based visual odometry methodology. The camera coordinate frame\u2019s depth camera points and the pixel plane\u2019s points are optimized by Levenberg\u2013Marquardt algorithm after extracting a typical corner detection algorithm and tracking by an optical flow algorithm. Using several key-frames, sliding-windowed PnP (Perspective-n-Point), algorithms can be constructed. The visual odometry of the camera and the attitude estimation from the IMU (Inertial Measurement Unit) are loosely coupled. An ARS (Attitude Reference System) is built with a quaternion based linear Kalman filter, and mainly compensates the rotation error of the sliding-windowed PnP algorithm for each frame. The positions of the recognized objects are also included in the PnP algorithm to cover up the lack of features due to the lack of light or motion blur, which are a major problem in feature-based odometry. Since the re-recognized objects\u2019 positions anchor the odometry, the drift problem which commonly occurs can also be solved. This framework performs a rough 3D reconstruction by interpreting the scene with minimal computing resources, and obtains the location of the agent. Therefore, it offers a simple 3D map and a graph structure, as a by-product for applications using this framework, and provides the attitudes and positions of agents for each frame. For applications that do not necessarily require dense geometric results, I propose that this framework can be utilized as a flexible and versatile component with fewer computer resources.",
                "correspondence": {
                    "affiliation": {
                        "country": "Japan",
                        "address-part": "Hongo, Bunkyou-ku",
                        "postal-code": "113-8656",
                        "@country": "jpn",
                        "city": "Tokyo",
                        "organization": {"$": "University of Tokyo"},
                        "ce:source-text": "University of Tokyo, Hongo, Bunkyou-ku, Tokyo, 113-8656, Japan"
                    },
                    "person": {
                        "ce:given-name": "Takeshi",
                        "ce:initials": "T.",
                        "ce:surname": "Tsuchiya",
                        "ce:indexed-name": "Tsuchiya T."
                    }
                },
                "citation-info": {
                    "author-keywords": {"author-keyword": [
                        {
                            "$": "Ensemble vision machine learning",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Scene reconstruction",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Scene understanding",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "SLAM",
                            "@xml:lang": "eng",
                            "@original": "y"
                        },
                        {
                            "$": "Visual-inertial odometry",
                            "@xml:lang": "eng",
                            "@original": "y"
                        }
                    ]},
                    "citation-type": {"@code": "cp"},
                    "citation-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    },
                    "abstract-language": {
                        "@language": "English",
                        "@xml:lang": "eng"
                    }
                },
                "source": {
                    "website": {"ce:e-address": {
                        "$": "https://www.springer.com/series/7818",
                        "@type": "email"
                    }},
                    "translated-sourcetitle": {
                        "$": "Lecture Notes in Electrical Engineering",
                        "@xml:lang": "eng"
                    },
                    "volisspag": {
                        "voliss": {"@volume": "913"},
                        "pagerange": {
                            "@first": "241",
                            "@last": "252"
                        }
                    },
                    "@type": "k",
                    "isbn": {
                        "@level": "volume",
                        "$": "9789811926341",
                        "@type": "print",
                        "@length": "13"
                    },
                    "additional-srcinfo": {"conferenceinfo": {
                        "confpublication": {"procpartno": "2 of 2"},
                        "confevent": {
                            "confname": "Asia-Pacific International Symposium on Aerospace Technology, APISAT 2021",
                            "confseriestitle": "Asia-Pacific International Symposium on Aerospace Technology",
                            "conflocation": {"city": "Virtual, Online"},
                            "confcode": "282569",
                            "confdate": {
                                "enddate": {
                                    "@day": "17",
                                    "@year": "2021",
                                    "@month": "11"
                                },
                                "startdate": {
                                    "@day": "15",
                                    "@year": "2021",
                                    "@month": "11"
                                }
                            }
                        }
                    }},
                    "sourcetitle": "Lecture Notes in Electrical Engineering",
                    "contributor-group": [
                        {"contributor": {
                            "ce:given-name": "Sangchul",
                            "@seq": "1",
                            "ce:initials": "S.",
                            "ce:surname": "Lee",
                            "@role": "edit",
                            "ce:indexed-name": "Lee S."
                        }},
                        {"contributor": {
                            "ce:given-name": "Cheolheui",
                            "@seq": "1",
                            "ce:initials": "C.",
                            "ce:surname": "Han",
                            "@role": "edit",
                            "ce:indexed-name": "Han C."
                        }},
                        {"contributor": {
                            "ce:given-name": "Jeong-Yeol",
                            "@seq": "1",
                            "ce:initials": "J.-Y.",
                            "ce:surname": "Choi",
                            "@role": "edit",
                            "ce:indexed-name": "Choi J.-Y."
                        }},
                        {"contributor": {
                            "ce:given-name": "Seungkeun",
                            "@seq": "1",
                            "ce:initials": "S.",
                            "ce:surname": "Kim",
                            "@role": "edit",
                            "ce:indexed-name": "Kim S."
                        }},
                        {"contributor": {
                            "ce:given-name": "Jeong Ho",
                            "@seq": "1",
                            "ce:initials": "J.H.",
                            "ce:surname": "Kim",
                            "@role": "edit",
                            "ce:indexed-name": "Kim J.H."
                        }}
                    ],
                    "publicationdate": {
                        "year": "2023",
                        "date-text": {
                            "@xfab-added": "true",
                            "$": "2023"
                        }
                    },
                    "sourcetitle-abbrev": "Lect. Notes Electr. Eng.",
                    "@country": "deu",
                    "issuetitle": "The Proceedings of the 2021 Asia-Pacific International Symposium on Aerospace Technology APISAT 2021, Volume 2",
                    "issn": [
                        {
                            "$": "18761119",
                            "@type": "electronic"
                        },
                        {
                            "$": "18761100",
                            "@type": "print"
                        }
                    ],
                    "publicationyear": {"@first": "2023"},
                    "publisher": {"publishername": "Springer Science and Business Media Deutschland GmbH"},
                    "@srcid": "19700186822"
                },
                "enhancement": {"classificationgroup": {"classifications": [
                    {
                        "@type": "CPXCLASS",
                        "classification": [
                            {
                                "classification-code": "461.4",
                                "classification-description": "Ergonomics and Human Factors Engineering"
                            },
                            {
                                "classification-code": "723",
                                "classification-description": "Computer Software, Data Handling and Applications"
                            },
                            {
                                "classification-code": "723.4.2",
                                "classification-description": "Machine Learning"
                            },
                            {
                                "classification-code": "741.2",
                                "classification-description": "Vision"
                            },
                            {
                                "classification-code": "742.2",
                                "classification-description": "Photographic and Video Equipment"
                            },
                            {
                                "classification-code": "912.2",
                                "classification-description": "Management"
                            }
                        ]
                    },
                    {
                        "@type": "FLXCLASS",
                        "classification": {
                            "classification-code": "902",
                            "classification-description": "FLUIDEX; Related Topics"
                        }
                    },
                    {
                        "@type": "ASJC",
                        "classification": "2209"
                    },
                    {
                        "@type": "SUBJABBR",
                        "classification": "ENGI"
                    }
                ]}}
            },
            "item-info": {
                "copyright": {
                    "$": "Copyright 2022 Elsevier B.V., All rights reserved.",
                    "@type": "Elsevier"
                },
                "dbcollection": [
                    {"$": "CPX"},
                    {"$": "REAXYSCAR"},
                    {"$": "SCOPUS"},
                    {"$": "Scopusbase"}
                ],
                "history": {"date-created": {
                    "@day": "29",
                    "@timestamp": "BST 07:06:41",
                    "@year": "2022",
                    "@month": "10"
                }},
                "itemidlist": {
                    "itemid": [
                        {
                            "$": "639340678",
                            "@idtype": "PUI"
                        },
                        {
                            "$": "943765781",
                            "@idtype": "CAR-ID"
                        },
                        {
                            "$": "20224413014960",
                            "@idtype": "CPX"
                        },
                        {
                            "$": "20223135573",
                            "@idtype": "REAXYSCAR"
                        },
                        {
                            "$": "20223738925",
                            "@idtype": "SCOPUS"
                        },
                        {
                            "$": "85140468359",
                            "@idtype": "SCP"
                        },
                        {
                            "$": "85140468359",
                            "@idtype": "SGR"
                        }
                    ],
                    "ce:doi": "10.1007/978-981-19-2635-8_18"
                }
            },
            "tail": {"bibliography": {
                "@refcount": "11",
                "reference": [
                    {
                        "ref-fulltext": "Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the ISMAR 2009",
                        "@id": "1",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2009"},
                            "ref-title": {"ref-titletext": "Murray D (2009) Parallel tracking and mapping on a camera phone"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85140471519",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [{
                                "@seq": "1",
                                "ce:initials": "G.",
                                "@_fa": "true",
                                "ce:surname": "Klein",
                                "ce:indexed-name": "Klein G."
                            }]},
                            "ref-sourcetitle": "Proceedings of the ISMAR"
                        },
                        "ce:source-text": "Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the ISMAR 2009"
                    },
                    {
                        "ref-fulltext": "Mur-Artal R, Montiel JMM, Tardos JD (2015) ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Trans Robot 31:1147\u20131163",
                        "@id": "2",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "ORB-SLAM: A versatile and accurate monocular SLAM system"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84988339174",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "31"},
                                "pagerange": {
                                    "@first": "1147",
                                    "@last": "1163"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Mur-Artal",
                                    "ce:indexed-name": "Mur-Artal R."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.M.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Montiel",
                                    "ce:indexed-name": "Montiel J.M.M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "J.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Tardos",
                                    "ce:indexed-name": "Tardos J.D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans Robot"
                        },
                        "ce:source-text": "Mur-Artal R, Montiel JMM, Tardos JD (2015) ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Trans Robot 31:1147\u20131163"
                    },
                    {
                        "ref-fulltext": "Mur-Artal R, Tardos JD (2017) ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Trans Robot 33:1255\u20131262",
                        "@id": "3",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "ORB-SLAM2: An open-source SLAM system for monocular, stereo and RGB-D cameras"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85020739341",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "33"},
                                "pagerange": {
                                    "@first": "1255",
                                    "@last": "1262"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Mur-Artal",
                                    "ce:indexed-name": "Mur-Artal R."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "J.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Tardos",
                                    "ce:indexed-name": "Tardos J.D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans Robot"
                        },
                        "ce:source-text": "Mur-Artal R, Tardos JD (2017) ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Trans Robot 33:1255\u20131262"
                    },
                    {
                        "ref-fulltext": "Campos C, Elvira R, GÃ³mez RodrÃ­guez JJ, Montiel JMM, TardÃ³s JD (2021) ORB-SLAM3: an accurate open-source library for visual, visual-inertial and multi-map SLAM. IEEE Trans Robot 37:1874\u20131890",
                        "@id": "4",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2021"},
                            "ref-title": {"ref-titletext": "ORB-SLAM3: An accurate open-source library for visual, visual-inertial and multi-map SLAM"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85107189435",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "37"},
                                "pagerange": {
                                    "@first": "1874",
                                    "@last": "1890"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "C.",
                                    "@_fa": "true",
                                    "ce:surname": "Campos",
                                    "ce:indexed-name": "Campos C."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Elvira",
                                    "ce:indexed-name": "Elvira R."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "J.J.",
                                    "@_fa": "true",
                                    "ce:surname": "GÃ³mez RodrÃ­guez",
                                    "ce:indexed-name": "Gomez Rodriguez J.J."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "J.M.M.",
                                    "@_fa": "true",
                                    "ce:surname": "Montiel",
                                    "ce:indexed-name": "Montiel J.M.M."
                                },
                                {
                                    "@seq": "5",
                                    "ce:initials": "J.D.",
                                    "@_fa": "true",
                                    "ce:surname": "TardÃ³s",
                                    "ce:indexed-name": "Tardos J.D."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans Robot"
                        },
                        "ce:source-text": "Campos C, Elvira R, GÃ³mez RodrÃ­guez JJ, Montiel JMM, TardÃ³s JD (2021) ORB-SLAM3: an accurate open-source library for visual, visual-inertial and multi-map SLAM. IEEE Trans Robot 37:1874\u20131890"
                    },
                    {
                        "ref-fulltext": "Bachrach A Presentation: robust visual navigation in the real world. In: ICRA21 VINS workshop",
                        "@id": "5",
                        "ref-info": {
                            "ref-title": {"ref-titletext": "Bachrach A Presentation: Robust visual navigation in the real world"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85140456812",
                                "@idtype": "SGR"
                            }},
                            "ref-sourcetitle": "ICRA21 VINS Workshop"
                        },
                        "ce:source-text": "Bachrach A Presentation: robust visual navigation in the real world. In: ICRA21 VINS workshop"
                    },
                    {
                        "ref-fulltext": "Rosinol A, Abate M, Chang Y, Carlone L (2020) Kimera: an open-source library for real-time metric-semantic localization and mapping. In: IEEE International Conference on Robotics and Automation (ICRA)",
                        "@id": "6",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2020"},
                            "ref-title": {"ref-titletext": "Kimera: An open-source library for real-time metric-semantic localization and mapping"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85092713405",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "ICRA",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Rosinol",
                                    "ce:indexed-name": "Rosinol A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "M.",
                                    "@_fa": "true",
                                    "ce:surname": "Abate",
                                    "ce:indexed-name": "Abate M."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "Y.",
                                    "@_fa": "true",
                                    "ce:surname": "Chang",
                                    "ce:indexed-name": "Chang Y."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "L.",
                                    "@_fa": "true",
                                    "ce:surname": "Carlone",
                                    "ce:indexed-name": "Carlone L."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE International Conference on Robotics and Automation"
                        },
                        "ce:source-text": "Rosinol A, Abate M, Chang Y, Carlone L (2020) Kimera: an open-source library for real-time metric-semantic localization and mapping. In: IEEE International Conference on Robotics and Automation (ICRA)"
                    },
                    {
                        "ref-fulltext": "Qin T, Li P, Shen S (2017) VINS-Mono: a robust and versatile monocular visual-inertial state estimator. IEEE Trans Robot 34:1004\u20131020",
                        "@id": "7",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "VINS-Mono: A robust and versatile monocular visual-inertial state estimator"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85050754505",
                                "@idtype": "SGR"
                            }},
                            "ref-volisspag": {
                                "voliss": {"@volume": "34"},
                                "pagerange": {
                                    "@first": "1004",
                                    "@last": "1020"
                                }
                            },
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "T.",
                                    "@_fa": "true",
                                    "ce:surname": "Qin",
                                    "ce:indexed-name": "Qin T."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "P.",
                                    "@_fa": "true",
                                    "ce:surname": "Li",
                                    "ce:indexed-name": "Li P."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Shen",
                                    "ce:indexed-name": "Shen S."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Trans Robot"
                        },
                        "ce:source-text": "Qin T, Li P, Shen S (2017) VINS-Mono: a robust and versatile monocular visual-inertial state estimator. IEEE Trans Robot 34:1004\u20131020"
                    },
                    {
                        "ref-fulltext": "Ludwig SA, Burnham KD (2018) Comparison of Euler estimate using extended Kalman filter, Madgwick and Mahony on quadcopter flight data. In: International conference on unmanned aircraft systems (ICUAS)",
                        "@id": "8",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2018"},
                            "ref-title": {"ref-titletext": "Comparison of Euler estimate using extended Kalman filter, Madgwick and Mahony on quadcopter flight data"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85053931214",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "S.A.",
                                    "@_fa": "true",
                                    "ce:surname": "Ludwig",
                                    "ce:indexed-name": "Ludwig S.A."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "K.D.",
                                    "@_fa": "true",
                                    "ce:surname": "Burnham",
                                    "ce:indexed-name": "Burnham K.D."
                                }
                            ]},
                            "ref-sourcetitle": "International Conference on Unmanned Aircraft Systems (ICUAS)"
                        },
                        "ce:source-text": "Ludwig SA, Burnham KD (2018) Comparison of Euler estimate using extended Kalman filter, Madgwick and Mahony on quadcopter flight data. In: International conference on unmanned aircraft systems (ICUAS)"
                    },
                    {
                        "ref-fulltext": "Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: unified, real-time object detection. In: IEEE conference on computer vision and pattern recognition",
                        "@id": "9",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2016"},
                            "ref-title": {"ref-titletext": "You only look once: unified, real-time object detection"},
                            "refd-itemidlist": {"itemid": {
                                "$": "84986308404",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Redmon",
                                    "ce:indexed-name": "Redmon J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "S.",
                                    "@_fa": "true",
                                    "ce:surname": "Divvala",
                                    "ce:indexed-name": "Divvala S."
                                },
                                {
                                    "@seq": "3",
                                    "ce:initials": "R.",
                                    "@_fa": "true",
                                    "ce:surname": "Girshick",
                                    "ce:indexed-name": "Girshick R."
                                },
                                {
                                    "@seq": "4",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Farhadi",
                                    "ce:indexed-name": "Farhadi A."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Conference on Computer Vision and Pattern Recognition"
                        },
                        "ce:source-text": "Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: unified, real-time object detection. In: IEEE conference on computer vision and pattern recognition"
                    },
                    {
                        "ref-fulltext": "Redmon J, Farhadi A (2017) YOLO9000: better, faster, stronger. In: IEEE conference on computer vision and pattern recognition (CVPR)",
                        "@id": "10",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2017"},
                            "ref-title": {"ref-titletext": "YOLO9000: Better, faster, stronger"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85041900441",
                                "@idtype": "SGR"
                            }},
                            "ref-text": "CVPR",
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "J.",
                                    "@_fa": "true",
                                    "ce:surname": "Redmon",
                                    "ce:indexed-name": "Redmon J."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Farhadi",
                                    "ce:indexed-name": "Farhadi A."
                                }
                            ]},
                            "ref-sourcetitle": "IEEE Conference on Computer Vision and Pattern Recognition"
                        },
                        "ce:source-text": "Redmon J, Farhadi A (2017) YOLO9000: better, faster, stronger. In: IEEE conference on computer vision and pattern recognition (CVPR)"
                    },
                    {
                        "ref-fulltext": "Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale image recognition. ICLR",
                        "@id": "11",
                        "ref-info": {
                            "ref-publicationyear": {"@first": "2015"},
                            "ref-title": {"ref-titletext": "Very deep convolutional networks for large-scale image recognition"},
                            "refd-itemidlist": {"itemid": {
                                "$": "85083953063",
                                "@idtype": "SGR"
                            }},
                            "ref-authors": {"author": [
                                {
                                    "@seq": "1",
                                    "ce:initials": "K.",
                                    "@_fa": "true",
                                    "ce:surname": "Simonyan",
                                    "ce:indexed-name": "Simonyan K."
                                },
                                {
                                    "@seq": "2",
                                    "ce:initials": "A.",
                                    "@_fa": "true",
                                    "ce:surname": "Zisserman",
                                    "ce:indexed-name": "Zisserman A."
                                }
                            ]},
                            "ref-sourcetitle": "ICLR"
                        },
                        "ce:source-text": "Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale image recognition. ICLR"
                    }
                ]
            }}
        }
    },
    "affiliation": {
        "affiliation-city": "Tokyo",
        "@id": "60025272",
        "affilname": "The University of Tokyo",
        "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60025272",
        "affiliation-country": "Japan"
    },
    "coredata": {
        "srctype": "k",
        "eid": "2-s2.0-85140468359",
        "dc:description": "The goal of my paper is to create a new framework which provides sufficient semantic information for decision makings as a core component of applications such as SLAM (Simultaneous Localization And Mapping), robotics, AR (Augmented Reality), autonomous driving, etc. This framework does not provide dense point clouds. Rather, a scene is described with several features which the agent has been trained to recognize. Specifically, scenes are generated by extracting features from the space\u2019s occupancy, the location of the light source, shape of the object, colors, and textures in the images. The extraction of the features is conducted by ensemble with deep-learning based feature extractors, traditional machine learning algorithms and image processing techniques. The deep-learning based feature extractors are trained by a supervised learning with data augmentation over 3D models; and the results of inferences are evaluated by a depth camera and retrained by unsupervised learning. Using existed methods it is difficult to utilize semantic information for unknown objects. But the agent in this methodology tries to describe them as much as possible by utilizing information trained in advance. For an odometry module, which estimates attitudes and positions, is implemented by a typical feature-based visual odometry methodology. The camera coordinate frame\u2019s depth camera points and the pixel plane\u2019s points are optimized by Levenberg\u2013Marquardt algorithm after extracting a typical corner detection algorithm and tracking by an optical flow algorithm. Using several key-frames, sliding-windowed PnP (Perspective-n-Point), algorithms can be constructed. The visual odometry of the camera and the attitude estimation from the IMU (Inertial Measurement Unit) are loosely coupled. An ARS (Attitude Reference System) is built with a quaternion based linear Kalman filter, and mainly compensates the rotation error of the sliding-windowed PnP algorithm for each frame. The positions of the recognized objects are also included in the PnP algorithm to cover up the lack of features due to the lack of light or motion blur, which are a major problem in feature-based odometry. Since the re-recognized objects\u2019 positions anchor the odometry, the drift problem which commonly occurs can also be solved. This framework performs a rough 3D reconstruction by interpreting the scene with minimal computing resources, and obtains the location of the agent. Therefore, it offers a simple 3D map and a graph structure, as a by-product for applications using this framework, and provides the attitudes and positions of agents for each frame. For applications that do not necessarily require dense geometric results, I propose that this framework can be utilized as a flexible and versatile component with fewer computer resources.",
        "prism:coverDate": "2023-01-01",
        "prism:aggregationType": "Book Series",
        "prism:url": "https://api.elsevier.com/content/abstract/scopus_id/85140468359",
        "dc:creator": {"author": [{
            "ce:given-name": "Park",
            "preferred-name": {
                "ce:given-name": "Park",
                "ce:initials": "P.",
                "ce:surname": "Kunbum",
                "ce:indexed-name": "Kunbum P."
            },
            "@seq": "1",
            "ce:initials": "P.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60025272",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60025272"
            },
            "ce:surname": "Kunbum",
            "@auid": "57939221500",
            "author-url": "https://api.elsevier.com/content/author/author_id/57939221500",
            "ce:indexed-name": "Kunbum P."
        }]},
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/abstract/scopus_id/85140468359"
            },
            {
                "@_fa": "true",
                "@rel": "scopus",
                "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85140468359&origin=inward"
            },
            {
                "@_fa": "true",
                "@rel": "scopus-citedby",
                "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85140468359&origin=inward"
            }
        ],
        "prism:isbn": "9789811926341",
        "source-id": "19700186822",
        "citedby-count": "0",
        "prism:volume": "913",
        "subtype": "cp",
        "dc:title": "3D Reconstruction by Pretrained Features and Visual-Inertial Odometry",
        "openaccess": "0",
        "prism:issn": "18761119 18761100",
        "publishercopyright": "Â© 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",
        "subtypeDescription": "Conference Paper",
        "prism:publicationName": "Lecture Notes in Electrical Engineering",
        "prism:pageRange": "241-252",
        "prism:endingPage": "252",
        "openaccessFlag": "false",
        "prism:doi": "10.1007/978-981-19-2635-8_18",
        "prism:startingPage": "241",
        "dc:identifier": "SCOPUS_ID:85140468359",
        "dc:publisher": "Springer Science and Business Media Deutschland GmbH"
    },
    "idxterms": {"mainterm": [
        {
            "$": "Ensemble vision machine learning",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Machine-learning",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Odometry",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Perspective n points",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Point algorithms",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Scene reconstruction",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Scene understanding",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Simultaneous localization and mapping",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Vision machine",
            "@weight": "b",
            "@candidate": "n"
        },
        {
            "$": "Visual-inertial odometry",
            "@weight": "b",
            "@candidate": "n"
        }
    ]},
    "language": {"@xml:lang": "eng"},
    "authkeywords": {"author-keyword": [
        {
            "@_fa": "true",
            "$": "Ensemble vision machine learning"
        },
        {
            "@_fa": "true",
            "$": "Scene reconstruction"
        },
        {
            "@_fa": "true",
            "$": "Scene understanding"
        },
        {
            "@_fa": "true",
            "$": "SLAM"
        },
        {
            "@_fa": "true",
            "$": "Visual-inertial odometry"
        }
    ]},
    "subject-areas": {"subject-area": [{
        "@_fa": "true",
        "$": "Industrial and Manufacturing Engineering",
        "@code": "2209",
        "@abbrev": "ENGI"
    }]},
    "authors": {"author": [
        {
            "ce:given-name": "Park",
            "preferred-name": {
                "ce:given-name": "Park",
                "ce:initials": "P.",
                "ce:surname": "Kunbum",
                "ce:indexed-name": "Kunbum P."
            },
            "@seq": "1",
            "ce:initials": "P.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60025272",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60025272"
            },
            "ce:surname": "Kunbum",
            "@auid": "57939221500",
            "author-url": "https://api.elsevier.com/content/author/author_id/57939221500",
            "ce:indexed-name": "Kunbum P."
        },
        {
            "ce:given-name": "Takeshi",
            "preferred-name": {
                "ce:given-name": "Takeshi",
                "ce:initials": "T.",
                "ce:surname": "Tsuchiya",
                "ce:indexed-name": "Tsuchiya T."
            },
            "@seq": "2",
            "ce:initials": "T.",
            "@_fa": "true",
            "affiliation": {
                "@id": "60025272",
                "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60025272"
            },
            "ce:surname": "Tsuchiya",
            "@auid": "55754198600",
            "author-url": "https://api.elsevier.com/content/author/author_id/55754198600",
            "ce:indexed-name": "Tsuchiya T."
        }
    ]}
}}