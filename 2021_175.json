{"search-results":{"opensearch:totalResults":"230","opensearch:startIndex":"175","opensearch:itemsPerPage":"25","opensearch:Query":{"@role": "request", "@searchTerms": "TITLE-ABS-KEY ( hypercomplex OR hyper-complex OR hipercomplex OR quaternions OR octonions OR quaternionic ) AND TITLE-ABS-KEY ( signal OR image )", "@startPage": "175"},"link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/search/scopus?start=175&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "first", "@href": "https://api.elsevier.com/content/search/scopus?start=0&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "prev", "@href": "https://api.elsevier.com/content/search/scopus?start=150&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "next", "@href": "https://api.elsevier.com/content/search/scopus?start=200&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"},{"@_fa": "true", "@ref": "last", "@href": "https://api.elsevier.com/content/search/scopus?start=205&count=25&query=TITLE-ABS-KEY+%28+hypercomplex+OR+hyper-complex+OR+hipercomplex+OR+quaternions+OR+octonions+OR+quaternionic+%29+AND+TITLE-ABS-KEY+%28+signal+OR+image+%29&date=2021&sort=+pubyear&view=complete", "@type": "application/json"}],"entry": [{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115704761"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115704761?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115704761&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85115704761&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85115704761","dc:identifier":"SCOPUS_ID:85115704761","eid":"2-s2.0-85115704761","dc:title":"Full Quaternion Matrix and Random Projection for Bimodal Face Template Protection","dc:creator":"Xu Z.","prism:publicationName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","prism:issn":"03029743","prism:eIssn":"16113349","prism:isbn": [{"@_fa": "true", "$" :"9783030866075"}],"prism:volume":"12878 LNCS","prism:pageRange":"374-383","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-86608-2_41","dc:description":"Considering that the information representation among multimodal biometric is complementary and the necessity of privacy-sensitive information protection over cloud and Internet of Things, this paper presents a secure template protection algorithm for bimodal face images using full-type quaternion generic polar complex exponential transform and random projection. The bimodal face images are firstly encoded into a full quaternion matrix and quaternion generic polar complex exponential transform is used for generating primary features. Then sparse random projection is followed aiming at making the fused features non-invertibility and be able to reissued. In additionally, the generalized discriminant analysis is employed to reduce dimension of the selected features. Experimental results obtained on three face datasets have demonstrated that the proposed method presents good recognition performance in comparison with other existing methods, while it can ensure the secrecy and privacy of facial images.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60020256","afid":"60020256","affilname":"Capital Normal University","affiliation-city":"Beijing","affiliation-country":"China"}],"prism:aggregationType":"Book Series","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57221732766","authid":"57221732766","authname":"Xu Z.","surname":"Xu","given-name":"Zihan","initials":"Z.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55849891700","authid":"55849891700","authname":"Shao Z.","surname":"Shao","given-name":"Zhuhong","initials":"Z.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/7101867863","authid":"7101867863","authname":"Shang Y.","surname":"Shang","given-name":"Yuanyuan","initials":"Y.","afid": [{"@_fa": "true", "$" :"60020256"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57191272257","authid":"57191272257","authname":"Ren Z.","surname":"Ren","given-name":"Zhongshan","initials":"Z.","afid": [{"@_fa": "true", "$" :"60020256"}]}],"authkeywords":"Cancelable face template | Multi-biometric features | Quaternion generic polar complex exponential transform | Sparse random projection","source-id":"25674","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115443674"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115443674?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115443674&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85115443674&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85115443674","dc:identifier":"SCOPUS_ID:85115443674","eid":"2-s2.0-85115443674","dc:title":"Optimization of building model based on 5G virtual reality technology in computer vision software","dc:creator":"Zhuang Z.","prism:publicationName":"Mathematical Biosciences and Engineering","prism:issn":"15471063","prism:eIssn":"15510018","prism:volume":"18","prism:issueIdentifier":"6","prism:pageRange":"7936-7954","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.3934/mbe.2021393","dc:description":"The 5G virtual reality system needs to interact with the user to draw the scene in real time. The contradiction between the complexity of the scene model and the real-time interaction is the main problem in the operation of the virtual reality system. The model optimization strategy of architectural scene in virtual reality design is studied, and the method of architectural scene model optimization is summarized. This article aims to study the optimization of computer vision software modeling through 5G virtual reality technology. In this paper, the optimization of the architectural model is studied by the method of image gray scale transformation, computer vision detection technology and virtual modeling technology. The four experiments are comprehensive evaluation and quantitative evaluation, comparison of channel estimation performance of different pilot structures, comparison of calculated and true values of external azimuth elements, and the effect of window-to-wall ratio on energy consumption per unit of residential building. The results show that hollow bricks of building materials have a great impact on the environment. The values of the three pixel coordinates X, Y, and Z calculated by the unit quaternion method are 1.27, 1.3, and -6.11, respectively, while the actual coordinate positions are 1.25, 1.37, and -6.22, respectively. It can be seen that the outer orientation element value calculated by the quaternion-based spatial rear intersection method is not much different from the actual value, and the correct result can be accurately calculated.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60108768","afid":"60108768","affilname":"Dalian Neusoft University of Information","affiliation-city":"Dalian","affiliation-country":"China"}],"pubmed-id":"34814282","prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/50662117500","authid":"50662117500","authname":"Zhuang Z.","surname":"Zhuang","given-name":"Ziyou","initials":"Z.","afid": [{"@_fa": "true", "$" :"60108768"}]}],"authkeywords":"5G network | Building model | Virtual reality technology | Visual software","source-id":"5200152802","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115069822"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85115069822?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85115069822&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85115069822&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85115069822","dc:identifier":"SCOPUS_ID:85115069822","eid":"2-s2.0-85115069822","dc:title":"Catiloc: Camera image transformer for indoor localization","dc:creator":"Ghofrani A.","prism:publicationName":"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","prism:issn":"15206149","prism:volume":"2021-June","prism:pageRange":"1450-1454","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ICASSP39728.2021.9414939","dc:description":"In this paper the problem of single image indoor camera localization has been addressed. This is a difficult task, since no GPS is available and the training data being gathered for the indoor positioning system could be subject to many modifications such as occlusion, variation of illumination, or repetitive textures and patterns during the test, and these effects can easily fool any positioning system. In this paper, following the idea of self attention and the transformer networks, we customized the feature extraction system and the output extraction block of a recently used transformer in the image recognition task, so that to achieve the camera 3D position and 4D quaternion information. Moreover, an engineering implementation trick was employed, and the results were evaluated on the 7scenes dataset, and compared to the other state-of-the-art methods. The output results show a consistent outperformance with rather a simpler, and faster configuration.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/126912138","afid":"126912138","affilname":"AR/VR Solution Company","affiliation-city":"Alpha","affiliation-country":null},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/122797799","afid":"122797799","affilname":"Iran Broadcasting University (IRIBU)","affiliation-city":"Tehran","affiliation-country":"Iran"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57211940470","authid":"57211940470","authname":"Ghofrani A.","surname":"Ghofrani","given-name":"Ali","initials":"A.","afid": [{"@_fa": "true", "$" :"122797799"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/56712794800","authid":"56712794800","authname":"Toroghi R.M.","surname":"Toroghi","given-name":"Rahil Mahdian","initials":"R.M.","afid": [{"@_fa": "true", "$" :"122797799"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57215564444","authid":"57215564444","authname":"Tabatabaie S.M.","surname":"Tabatabaie","given-name":"Seyed Mojtaba","initials":"S.M.","afid": [{"@_fa": "true", "$" :"126912138"}]}],"authkeywords":"CNN | Indoor localization | Quaternion | Self attention | Transformer network","source-id":"110544","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114965941"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114965941?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114965941&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85114965941&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85114965941","dc:identifier":"SCOPUS_ID:85114965941","eid":"2-s2.0-85114965941","dc:title":"Speech emotion recognition using quaternion convolutional neural networks","dc:creator":"Muppidi A.","prism:publicationName":"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","prism:issn":"15206149","prism:volume":"2021-June","prism:pageRange":"6309-6313","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ICASSP39728.2021.9414248","dc:description":"Although speech recognition has become a widespread technology, inferring emotion from speech signals remains a challenge. Our paper addresses this problem by proposing a quaternion convolutional neural network (QCNN) based speech emotion recognition (SER) model in which Mel-spectrogram features of speech signals are encoded in an RGB quaternion domain. We demonstrate that our QCNN based SER model outperforms other real-valued methods in the Ryerson Audio- Visual Database of Emotional Speech and Song (RAVDESS, 8-classes) dataset, achieving, to the best of our knowledge, state-of-the-art results. The QCNN model also achieves comparable results with state-of-the-art methods in the Interactive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin EMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of 77.87%, 70.46%, and 88.78% for the RAVDESS, IEMOCAP, and EMO-DB datasets, respectively. Additionally, model size results reveal that the quaternion unit structure is significantly better able to encode internal dependencies than real-valued structures.","citedby-count":"18","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60026415","afid":"60026415","affilname":"Stony Brook University","affiliation-city":"Stony Brook","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57216523770","authid":"57216523770","authname":"Muppidi A.","surname":"Muppidi","given-name":"Aneesh","initials":"A.","afid": [{"@_fa": "true", "$" :"60026415"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/8403329000","authid":"8403329000","authname":"Radfar M.","surname":"Radfar","given-name":"Martin","initials":"M.","afid": [{"@_fa": "true", "$" :"60026415"}]}],"authkeywords":"Convolutional Neural Networks | Quaternion Deep Learning | Signal Processing | Speech Emotion Recognition","source-id":"110544","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114741068"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114741068?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114741068&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85114741068&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85114741068","dc:identifier":"SCOPUS_ID:85114741068","eid":"2-s2.0-85114741068","dc:title":"Wind Speed Forecasting Using the Stationary Wavelet Transform and Quaternion Adaptive-Gradient Methods","dc:creator":"Saoud L.S.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"127356-127367","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3111667","dc:description":"Accurate wind speed forecasting is a fundamental requirement for advanced and economically viable large-scale wind power integration. The hybridization of the quaternion-valued neural networks and stationary wavelet transform has not been proposed before. In this paper, we propose a novel wind-speed forecasting model that combines the stationary wavelet transform with quaternion-valued neural networks. The proposed model represents wavelet subbands in quaternion vectors, which avoid separating the naturally correlated subbands. The model consists of three main steps. First, the wind speed signal is decomposed using the stationary wavelet transform into sublevels. Second, a quaternion-valued neural network is used to forecast wind speed components in the stationary wavelet domain. Finally, the inverse stationary wavelet transform is applied to estimate the predicted wind speed. In addition, a softplus quaternion variant of the RMSProp learning algorithm is developed and used to improve the performance and convergence speed of the proposed model. The proposed model is tested on wind speed data collected from different sites in China and the United States, and the results demonstrate that it consistently outperforms similar models. In the meteorological terminal aviation routine (METAR) dataset experiment, the proposed wind speed forecasting model reduces the mean absolute error, and root mean squared error of predicted wind speed values by 26.5% and 33%, respectively, in comparison to several existing approaches.","citedby-count":"8","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60104134","afid":"60104134","affilname":"Khalifa University of Science and Technology","affiliation-city":"Abu Dhabi","affiliation-country":"United Arab Emirates"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60070785","afid":"60070785","affilname":"Ajman University","affiliation-city":"Ajman","affiliation-country":"United Arab Emirates"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60009506","afid":"60009506","affilname":"King Fahd University of Petroleum and Minerals","affiliation-city":"Dhahran","affiliation-country":"Saudi Arabia"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57209186714","authid":"57209186714","orcid":"0000-0003-4445-3135","authname":"Saoud L.S.","surname":"Saoud","given-name":"Lyes Saad","initials":"L.S.","afid": [{"@_fa": "true", "$" :"60104134"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/24830285200","authid":"24830285200","orcid":"0000-0002-2826-1515","authname":"Al-Marzouqi H.","surname":"Al-Marzouqi","given-name":"Hasan","initials":"H.","afid": [{"@_fa": "true", "$" :"60104134"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/7004663465","authid":"7004663465","orcid":"0000-0002-5287-1874","authname":"Deriche M.","surname":"Deriche","given-name":"Mohamed","initials":"M.","afid": [{"@_fa": "true", "$" :"60070785"},{"@_fa": "true", "$" :"60009506"}]}],"authkeywords":"quaternion valued neural network | RMSProp learning algorithm | stationary wavelet transform | Wind speed forecasting","source-id":"21100374601","fund-acr":"TRA","fund-no":"undefined","fund-sponsor":"Telecommunications Regulatory Authority","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114323974"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85114323974?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85114323974&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85114323974&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85114323974","dc:identifier":"SCOPUS_ID:85114323974","eid":"2-s2.0-85114323974","dc:title":"An image watermarking framework based on PSO and FrQWT","dc:creator":"Kumar S.","prism:publicationName":"Journal of Discrete Mathematical Sciences and Cryptography","prism:issn":"09720529","prism:volume":"24","prism:issueIdentifier":"5","prism:pageRange":"1293-1308","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1080/09720529.2021.1936901","dc:description":"This paper primarily explored the usefulness of singular value decomposition for the image watermarking in pursuit of overcoming its typically experienced issues. For this intention, an efficient watermarking scheme optimized by particle swarm optimization (PSO) is designed based on magic-matrix scrambling method and fractional quaternion wavelet transform (FrQWT). The central idea is to shuffle all picture elements of the watermark using magic-matrix scrambling method followed by its insertion into the fruit amplitude coefficients smartly. Another beauty of the designed proficiency is the usage of PSO algorithm to naturally optimize the parameters involved in watermarking strategy. Finally, the sturdyness of the designed watermarking algorithm is investigated against numerous intentional and unintentional attacks.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60107631","afid":"60107631","affilname":"University of Petroleum and Energy Studies","affiliation-city":"Dehradun","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60103785","afid":"60103785","affilname":"Graphic Era Deemed to be University","affiliation-city":"Dehradun","affiliation-country":"India"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/58262589700","authid":"58262589700","authname":"Kumar S.","surname":"Kumar","given-name":"Sanoj","initials":"S.","afid": [{"@_fa": "true", "$" :"60107631"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57203297300","authid":"57203297300","authname":"Singh M.K.","surname":"Singh","given-name":"Manoj K.","initials":"M.K.","afid": [{"@_fa": "true", "$" :"60107631"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56126548900","authid":"56126548900","authname":"Saini D.","surname":"Saini","given-name":"Deepika","initials":"D.","afid": [{"@_fa": "true", "$" :"60103785"}]}],"authkeywords":"68M25 (Computer security) | FrQWT | Image watermarking | Magic square matrix | PSO | SVD","source-id":"19700186892","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85113623160"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85113623160?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113623160&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85113623160&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85113623160","dc:identifier":"SCOPUS_ID:85113623160","eid":"2-s2.0-85113623160","dc:title":"Multi-scale Multi-block Covariance Descriptor for a Compact Face Texture Representation: Application to Kinship Verification","dc:creator":"Moujahid A.","prism:publicationName":"Advances in Science, Technology and Innovation","prism:issn":"25228714","prism:eIssn":"25228722","prism:pageRange":"39-49","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-14647-4_4","dc:description":"Division-based strategies for face representation are common methods for capturing local and global features and have proven to be effective and highly discriminative. However, most of these methods have been mainly considered for descriptors based only on one single type of features. In this chapter, we introduce an effective approach for face representation with application to kinship verification that relies on pyramid multi-level (PML) face representation, and which exploits second order statistics of several local texture features such as Local Binary Pattern (LBP), quaternionic local ranking binary pattern (QLRBP), gradients, and different color spaces. The proposed approach consists of two main components. First, we model the face image using a PML representation that seeks a multi-block-multi-scale representation where several local texture features are extracted from different blocks at each scale. Second, to achieve a global context information, we compute the covariance between local features characterizing each individual block in the PML representation. The resulting face descriptor has two interesting properties: (i) thanks to the PML representation, scales and face parts are explicitly encoded in the final descriptor without having to detect the facial landmarks, (ii) the covariance descriptor (second order statistics) encodes spatial features of any type allowing the fusion of several state-of-the art texture features. Experiments conducted on two challenging kinship databases provide results that outperform state-of-the-art Kinship verification algorithms.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60027856","afid":"60027856","affilname":"Universidad del Pais Vasco","affiliation-city":"Leioa","affiliation-country":"Spain"}],"prism:aggregationType":"Book Series","subtype":"ch","subtypeDescription":"Book Chapter","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6602925990","authid":"6602925990","authname":"Moujahid A.","surname":"Moujahid","given-name":"Abdelmalik","initials":"A.","afid": [{"@_fa": "true", "$" :"60027856"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55967501800","authid":"55967501800","authname":"Dornaika F.","surname":"Dornaika","given-name":"Fadi","initials":"F.","afid": [{"@_fa": "true", "$" :"60027856"}]}],"source-id":"21101032300","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85113326704"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85113326704?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85113326704&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85113326704&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85113326704","dc:identifier":"SCOPUS_ID:85113326704","eid":"2-s2.0-85113326704","dc:title":"Computationally Efficient Two-Dimensional DOA Estimation Algorithm Based on Quaternion Theory","dc:creator":"Lou Y.","prism:publicationName":"IEEE Signal Processing Letters","prism:issn":"10709908","prism:eIssn":"15582361","prism:volume":"28","prism:pageRange":"1764-1768","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/LSP.2021.3106150","dc:description":"In this letter, we present a novel computationally efficient DOA estimation algorithm based on quaternion theory for two-dimensional (2-D) direction-of-arrival (DOA) estimation. An orthogonal propagator method based on the cross-correlation of the quaternion models (OPM-CQM) is developed to alleviate the computation burden. To eliminate the effect of additive noise, we construct two quaternion-based signal models judiciously. Then, we obtain the statistics of the observed signals by performing the cross-correlation between the quaternion models. Meanwhile, the additive noise is eliminated without introducing other denoising methods. Moreover, the compact modeling approach based on quaternions provides a significant advantage to OPM-CQM in terms of computational effort. Simulations demonstrate that the proposed algorithm offers performance superiority in angular resolution compared with the non-quaternion schemes.","citedby-count":"4","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60003353","afid":"60003353","affilname":"Harbin Engineering University","affiliation-city":"Harbin","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56479928200","authid":"56479928200","orcid":"0000-0002-4512-781X","authname":"Lou Y.","surname":"Lou","given-name":"Yi","initials":"Y.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/12807673300","authid":"12807673300","orcid":"0000-0002-4767-0015","authname":"Gang Q.","surname":"Gang","given-name":"Qiao","initials":"Q.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57226578756","authid":"57226578756","orcid":"0000-0001-9393-925X","authname":"Qu X.","surname":"Qu","given-name":"Xinghao","initials":"X.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/56683666800","authid":"56683666800","orcid":"0000-0002-9720-8451","authname":"Zhou F.","surname":"Zhou","given-name":"Feng","initials":"F.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]}],"authkeywords":"acoustic vector sensor | direction-of-arrival estimation | orthogonal propagator method | Quaternion","source-id":"17316","fund-acr":"NSFC","fund-no":"U1806201","fund-sponsor":"National Natural Science Foundation of China","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112750643"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112750643?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112750643&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85112750643&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85112750643","dc:identifier":"SCOPUS_ID:85112750643","eid":"2-s2.0-85112750643","dc:title":"Multimodal image inpainting for an autonomous robot navigation application","dc:creator":"Gapon N.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:eIssn":"1996756X","prism:isbn": [{"@_fa": "true", "$" :"9781510644045"}],"prism:volume":"11785","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1117/12.2594476","dc:description":"Automatic 3-D recovery from multimodal images can be extremely useful for information extraction for the robot navigation application. In most cases, such a scene contains missing holes on depth maps that appear during the synthesis from multi-views. This paper presents an automated pipeline for processing multimodal images to 3-D digital surface models. The proposed approach uses the modified exemplar-based technique in quaternion space. We also perform depth completion by fusing data from multiple recorded multimodal images affected by occlusions. We propose an algorithm using the concepts of a sparse representation of quaternions, which uses a new gradient to calculate the priority function by integrating the structure of quaternions with local polynomial approximation - the intersection of confidence intervals). Moreover, the color information incorporates into the optimization criteria to obtain sharp inpainting results. Compared with state-of-the-art techniques, the proposed algorithm provides plausible restoration of the depth map from multimodal images, making them a promising tool for an autonomous robot navigation application.","citedby-count":"6","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60264822","afid":"60264822","affilname":"Russian Customs Academy","affiliation-city":"Lyubertsy","affiliation-country":"Russian Federation"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60018011","afid":"60018011","affilname":"Donskoj Gosudarstvennyj Tehniceskij Universitet","affiliation-city":"Rostov-on-Don","affiliation-country":"Russian Federation"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60014196","afid":"60014196","affilname":"Moscow State Technological University Stankin","affiliation-city":"Moscow","affiliation-country":"Russian Federation"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "6", "$" :"6"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56157301500","authid":"56157301500","authname":"Gapon N.","surname":"Gapon","given-name":"N.","initials":"N.","afid": [{"@_fa": "true", "$" :"60014196"},{"@_fa": "true", "$" :"60018011"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55802371401","authid":"55802371401","authname":"Voronin V.","surname":"Voronin","given-name":"V.","initials":"V.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56572834900","authid":"56572834900","authname":"Semenishchev E.","surname":"Semenishchev","given-name":"E.","initials":"E.","afid": [{"@_fa": "true", "$" :"60014196"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/36544752200","authid":"36544752200","authname":"Ilyukhin Y.","surname":"Ilyukhin","given-name":"Y.","initials":"Y.","afid": [{"@_fa": "true", "$" :"60018011"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/6602589778","authid":"6602589778","authname":"Bezuglov D.","surname":"Bezuglov","given-name":"D.","initials":"D.","afid": [{"@_fa": "true", "$" :"60264822"}]},{"@_fa": "true", "@seq": "6", "author-url":"https://api.elsevier.com/content/author/author_id/57204649388","authid":"57204649388","authname":"Zelenskii A.","surname":"Zelenskii","given-name":"A.","initials":"A.","afid": [{"@_fa": "true", "$" :"60014196"}]}],"authkeywords":"3-D recovery | anisotropic gradient | depth map | Image inpainting | neural network | quaternion space | robot navigation","article-number":"117850Y","source-id":"40067","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112692147"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112692147?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112692147&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85112692147&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85112692147","dc:identifier":"SCOPUS_ID:85112692147","eid":"2-s2.0-85112692147","dc:title":"Real and complex hedgehogs, their symplectic area, curvature and evolutes","dc:creator":"Martinez-Maure Y.","prism:publicationName":"Journal of Symplectic Geometry","prism:issn":"15275256","prism:eIssn":"15402347","prism:volume":"19","prism:issueIdentifier":"3","prism:pageRange":"567-606","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.4310/JSG.2021.V19.N3.A3","dc:description":"Classical (real) hedgehogs can be regarded as the geometrical real-izations of formal differences of convex bodies in Rn+1 . Like convex bodies, hedgehogs can be identified with their support functions. Adopting a projective viewpoint, we prove that any holomorphic function h: Cn → C can be regarded as the ‘support function’ of a complex hedgehog Hh in Cn+1 . In the same vein, we introduce the notion of evolute of such a hedgehog Hh in C2, and a natural (but apparently hitherto unknown) notion of complex curvature, which allows us to interpret this evolute as the locus of the centers of complex curvature. It is of course permissible to think that the development of a ‘Brunn-Minkowski theory for complex hedge-hogs’ (replacing Euclidean volumes by symplectic ones) might be a promising way of research. We give first two results in this di-rection. We next return to real hedgehogs in R2n endowed with a linear complex structure. We introduce and study the notion of evolute of a hedgehog. We particularly focus our attention on R4 endowed with a linear Kähler structure determined by the datum of a pure unit quaternion. In parallel, we study the symplectic area of the images of the oriented Hopf circles under hedgehog parametrizations and introduce a quaternionic curvature function for such an image. Finally, we consider briefly the convolution of hedgehogs, and the particular case of hedgehogs in R4n regarded as a hyperkähler vector space.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60001422","afid":"60001422","affilname":"Sorbonne Université","affiliation-city":"Paris","affiliation-country":"France"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6601967288","authid":"6601967288","authname":"Martinez-Maure Y.","surname":"Martinez-Maure","given-name":"Yves","initials":"Y.","afid": [{"@_fa": "true", "$" :"60001422"}]}],"source-id":"19200156910","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfree2read"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Bronze"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112644937"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112644937?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112644937&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85112644937&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85112644937","dc:identifier":"SCOPUS_ID:85112644937","eid":"2-s2.0-85112644937","dc:title":"Robust Image Hashing Based on Cool and Warm Hue and Space Angle","dc:creator":"Zhao Y.","prism:publicationName":"Security and Communication Networks","prism:issn":"19390114","prism:eIssn":"19390122","prism:volume":"2021","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1155/2021/3803481","dc:description":"Image hashing has attracted more and more attention in the field of information security. In this paper, a novel hashing algorithm using cool and warm hue information and three-dimensional space angle is proposed. Firstly, the original image is preprocessed to get the opposite color component and the hue component H in HSV color space. Then, the distribution of cool and warm hue pixels is extracted from hue component H. Blocks the hue component H, according to the proportion of warm hue and cool hue pixels in each small block, combined with the quaternion and opposite color component, constructed the cool and warm hue opposite color quaternion (CWOCQ) feature. Then, three-dimensional space, opposite color, and cool and warm hue are combined to obtain the three-dimensional space angle (TDSA) feature. The CWOCQ feature and the TDSA feature are connected and disturbed to obtain the final hash sequence. Experimental results show that the proposed algorithm has good security and has better image classification performance and shorter computation time compared with some advanced algorithms.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60082199","afid":"60082199","affilname":"Shanghai University of Electric Power","affiliation-city":"Shanghai","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56495734200","authid":"56495734200","orcid":"0000-0002-3247-4337","authname":"Zhao Y.","surname":"Zhao","given-name":"Yan","initials":"Y.","afid": [{"@_fa": "true", "$" :"60082199"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57226786397","authid":"57226786397","orcid":"0000-0001-7178-5608","authname":"Liu S.","surname":"Liu","given-name":"Shuai","initials":"S.","afid": [{"@_fa": "true", "$" :"60082199"}]}],"article-number":"3803481","source-id":"18000156707","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112574596"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112574596?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112574596&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85112574596&origin=inward"},{"@_fa": "true", "@ref": "full-text", "@href": "https://api.elsevier.com/content/article/eid/1-s2.0-S2214785321007744"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85112574596","dc:identifier":"SCOPUS_ID:85112574596","eid":"2-s2.0-85112574596","dc:title":"Penmanship and signal acknowledgment utilizing inertial pen","dc:creator":"Sophia Jasmine G.","prism:publicationName":"Materials Today: Proceedings","prism:eIssn":"22147853","prism:volume":"45","prism:pageRange":"8110-8114","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1016/j.matpr.2021.01.677","pii":"S2214785321007744","dc:description":"This paper exhibits an inertial-sensor based advanced pen, connected with Dynamic Time Traveling calculation for penmanship and signal acknowledgment. With favoured dealt with speed, client holds inertial pen to compose numeral or lowercase literatures and make indicator signals. The signs from accelerometer and gyro sensor are coordinated and gathered into a quaternion-based reciprocal channel to lessen the vital blunders from the gyro sensor which causes the inherent commotions and sign float are here and there diminishes the exactness of the direction estimation. This uses DTW based acknowledgment calculation with which it comprises of different structures like preparing, recognizable proof and afterward the recognition part. To get a prevalent class detachment for improved acknowledgment we have created to negligible between class to maximal class-based format determination technique. In the Trial results, the adequacy of the DTW-based calculation for web-based penmanship and motion utilizing the inertial pen is effectively approved.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60097144","afid":"60097144","affilname":"Kongu Engineering College","affiliation-city":"Erode","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60079731","afid":"60079731","affilname":"Sri Krishna College of Technology","affiliation-city":"Coimbatore","affiliation-country":"India"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56662179500","authid":"56662179500","authname":"Sophia Jasmine G.","surname":"Sophia Jasmine","given-name":"G.","initials":"G.","afid": [{"@_fa": "true", "$" :"60079731"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57215188426","authid":"57215188426","authname":"Magdalin Mary D.","surname":"Magdalin Mary","given-name":"D.","initials":"D.","afid": [{"@_fa": "true", "$" :"60079731"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56461212300","authid":"56461212300","authname":"Sheela A.","surname":"Sheela","given-name":"A.","initials":"A.","afid": [{"@_fa": "true", "$" :"60097144"}]}],"authkeywords":"Dynamic time traveling | Human PC Interface (HCI) | Inertial pen | Quaternion-based correlative filter","source-id":"21100370037","fund-acr":"NIH","fund-no":"L30AI120154","fund-sponsor":"National Institutes of Health","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112119364"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85112119364?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85112119364&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85112119364&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85112119364","dc:identifier":"SCOPUS_ID:85112119364","eid":"2-s2.0-85112119364","dc:title":"PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion Prediction","dc:creator":"Wang H.","prism:publicationName":"IEEE Transactions on Image Processing","prism:issn":"10577149","prism:eIssn":"19410042","prism:volume":"30","prism:pageRange":"6096-6106","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/TIP.2021.3089380","dc:description":"Human motion prediction, which aims to predict future human poses given past poses, has recently seen increased interest. Many recent approaches are based on Recurrent Neural Networks (RNN) which model human poses with exponential maps. These approaches neglect the pose velocity as well as temporal relation of different poses, and tend to converge to the mean pose or fail to generate natural-looking poses. We therefore propose a novel Position-Velocity Recurrent Encoder-Decoder (PVRED) for human motion prediction, which makes full use of pose velocities and temporal positional information. A temporal position embedding method is presented and a Position-Velocity RNN (PVRNN) is proposed. We also emphasize the benefits of quaternion parameterization of poses and design a novel trainable Quaternion Transformation (QT) layer, which is combined with a robust loss function during training. We provide quantitative results for both short-term prediction in the future 0.5 seconds and long-term prediction in the future 0.5 to 1 seconds. Experiments on several benchmarks show that our approach considerably outperforms the state-of-the-art methods. In addition, qualitative visualizations in the future 4 seconds show that our approach could predict future human-like and meaningful poses in very long time horizons. Code is publicly available on GitHub: Https://github.com/hongsong-wang/PVRNN.","citedby-count":"7","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60017161","afid":"60017161","affilname":"National University of Singapore","affiliation-city":"Singapore City","affiliation-country":"Singapore"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/127083826","afid":"127083826","affilname":"Intelligent Engineering Department","affiliation-city":"Beijing","affiliation-country":"China"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/125682334","afid":"125682334","affilname":"Beijing Academy of Artificial Intelligence","affiliation-city":"Beijing","affiliation-country":"China"}],"pubmed-id":"34185641","prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57190273192","authid":"57190273192","authname":"Wang H.","surname":"Wang","given-name":"Hongsong","initials":"H.","afid": [{"@_fa": "true", "$" :"60017161"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55478015100","authid":"55478015100","authname":"Dong J.","surname":"Dong","given-name":"Jian","initials":"J.","afid": [{"@_fa": "true", "$" :"127083826"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57226373924","authid":"57226373924","authname":"Cheng B.","surname":"Cheng","given-name":"Bin","initials":"B.","afid": [{"@_fa": "true", "$" :"125682334"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/36439415700","authid":"36439415700","authname":"Feng J.","surname":"Feng","given-name":"Jiashi","initials":"J.","afid": [{"@_fa": "true", "$" :"60017161"}]}],"authkeywords":"Human motion prediction | quaternion transformation | recurrent neural networks","article-number":"9467497","source-id":"25534","fund-no":"undefined","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111960372"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111960372?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111960372&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111960372&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111960372","dc:identifier":"SCOPUS_ID:85111960372","eid":"2-s2.0-85111960372","dc:title":"Quaternion-Based Two-Dimensional DOA Estimation for Coherent Underwater Sources without Eigendecomposition","dc:creator":"Lou Y.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"104142-104153","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3099595","dc:description":"For scenarios of coherent underwater sources at low signal-to-noise ratio (SNR), a novel quaternion-based DOA algorithm without eigendecomposition is proposed using a linear vector-hydrophone array. We construct four quaternion models by judiciously arranging the received data to fully utilize the statistical information of the incident signals. To avoid the high computational complexity caused by the eigenvalue decomposition (EVD), we introduce the computationally efficient propagator method (PM) to estimate the elevation angles of the observed signals. In the quaternion algebra framework, we statistically eliminate the additive noise, which makes the PM method exhibit a robust performance in low SNR. By fully exploiting the direction information embedded in the velocity components, we achieve a high-resolution two-dimensional (2-D) DOA estimation result with a linear vector-hydrophone array. The simulations demonstrate that the proposed method offers stable estimation performance compared with the existing non-quaternion schemes without the need for any pair matching between the estimated azimuth and elevation angles.","citedby-count":"5","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60003353","afid":"60003353","affilname":"Harbin Engineering University","affiliation-city":"Harbin","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56479928200","authid":"56479928200","orcid":"0000-0002-4512-781X","authname":"Lou Y.","surname":"Lou","given-name":"Yi","initials":"Y.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/57226578756","authid":"57226578756","orcid":"0000-0001-9393-925X","authname":"Qu X.","surname":"Qu","given-name":"Xinghao","initials":"X.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57226580431","authid":"57226580431","orcid":"0000-0001-6363-8106","authname":"Lu Y.","surname":"Lu","given-name":"Yinheng","initials":"Y.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/12807673300","authid":"12807673300","orcid":"0000-0002-4767-0015","authname":"Qiao G.","surname":"Qiao","given-name":"Gang","initials":"G.","afid": [{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"},{"@_fa": "true", "$" :"60003353"}]}],"authkeywords":"coherent signals | direction-of-arrival estimation | propagator method | Quaternion | vector hydrophone","article-number":"9494358","source-id":"21100374601","fund-acr":"NSFC","fund-no":"U1806201","fund-sponsor":"National Natural Science Foundation of China","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111888009"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111888009?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111888009&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111888009&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111888009","dc:identifier":"SCOPUS_ID:85111888009","eid":"2-s2.0-85111888009","dc:title":"A Quaternion Deterministic Monogenic CNN Layer for Contrast Invariance","dc:creator":"Moya-Sánchez E.U.","prism:publicationName":"SEMA SIMAI Springer Series","prism:issn":"21993041","prism:eIssn":"2199305X","prism:volume":"13","prism:pageRange":"133-152","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-74486-1_7","dc:description":"Deep learning (DL) is attracting considerable interest as it currently achieves remarkable performance in many branches of science and technology. However, current DL cannot guarantee capabilities of the mammalian visual systems such as lighting changes. This paper proposes a deterministic entry layer capable of classifying images even with low-contrast conditions. We achieve this through an improved version of the quaternion monogenic wavelets. We have simulated the atmospheric degradation of the CIFAR-10 and the Dogs and Cats datasets to generate realistic contrast degradations of the images. The most important result is that the accuracy gained by using our layer is substantially more robust to illumination changes than nets without such a layer.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60097745","afid":"60097745","affilname":"Centro Nacional de Supercomputación","affiliation-city":"Barcelona","affiliation-country":"Spain"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60025730","afid":"60025730","affilname":"Universidad Autónoma de Guadalajara","affiliation-city":"Zapopan","affiliation-country":"Mexico"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60007592","afid":"60007592","affilname":"Universitat Politécnica de Catalunya","affiliation-city":"Barcelona","affiliation-country":"Spain"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/123672783","afid":"123672783","affilname":"Gobierno de Jalisco","affiliation-city":"Guadalajara","affiliation-country":"Mexico"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/112893242","afid":"112893242","affilname":"Centro de Investigación en Óptica (CIO)","affiliation-city":"Leon","affiliation-country":"Mexico"}],"prism:aggregationType":"Book Series","subtype":"ch","subtypeDescription":"Book Chapter","author-count":{"@limit": "100", "@total": "5", "$" :"5"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/36667039500","authid":"36667039500","authname":"Moya-Sánchez E.U.","surname":"Moya-Sánchez","given-name":"Eduardo Ulises","initials":"E.U.","afid": [{"@_fa": "true", "$" :"123672783"},{"@_fa": "true", "$" :"60025730"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/6507409675","authid":"6507409675","authname":"Xambó-Descamps S.","surname":"Xambó-Descamps","given-name":"Sebastià","initials":"S.","afid": [{"@_fa": "true", "$" :"60007592"},{"@_fa": "true", "$" :"60097745"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/57204354992","authid":"57204354992","authname":"Salazar Colores S.","surname":"Salazar Colores","given-name":"Sebastián","initials":"S.","afid": [{"@_fa": "true", "$" :"112893242"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/57204354591","authid":"57204354591","authname":"Sánchez Pérez A.","surname":"Sánchez Pérez","given-name":"Abraham","initials":"A.","afid": [{"@_fa": "true", "$" :"123672783"}]},{"@_fa": "true", "@seq": "5", "author-url":"https://api.elsevier.com/content/author/author_id/7004065770","authid":"7004065770","authname":"Cortés U.","surname":"Cortés","given-name":"Ulises","initials":"U.","afid": [{"@_fa": "true", "$" :"60007592"},{"@_fa": "true", "$" :"60097745"}]}],"source-id":"21100834919","fund-acr":"CONACYT","fund-no":"285651","fund-sponsor":"Consejo Nacional de Ciencia y Tecnología","openaccess":"0","openaccessFlag":false,"freetoread":{"value": [{"$" :"all"},{"$" :"repository"},{"$" :"repositoryam"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Green"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111840865"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111840865?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111840865&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111840865&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111840865","dc:identifier":"SCOPUS_ID:85111840865","eid":"2-s2.0-85111840865","dc:title":"Comparative Analysis of Transform Domain Watermarking System Based on Performance Measures","dc:creator":"Agarwal N.","prism:publicationName":"Advances in Science, Technology and Innovation","prism:issn":"25228714","prism:eIssn":"25228722","prism:pageRange":"329-334","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1007/978-3-030-66218-9_38","dc:description":"In last few decades, data security has been a challenging issue to protect the copyright information and integrity of digital products. So digital watermarking is founded as one of the talented methods to defend digitized media. This paper defines functioning of major transform domain methods such as discrete cosine transform (DCT), quaternion Hadamard transform (QHT), discrete wavelet transform (DWT), discrete contourlet transform (DCT), and many more are discussed. Comparison of these techniques based on peak signal-to-noise ratio (PSNR) and normalized correlation (NC) is discussed in this paper. Various types of attacks are also evaluated on these techniques.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60104116","afid":"60104116","affilname":"Jaypee University of Information Technology, Solan","affiliation-city":"Solan","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60076033","afid":"60076033","affilname":"ABES Engineering College","affiliation-city":"Ghaziabad","affiliation-country":"India"}],"prism:aggregationType":"Book Series","subtype":"ch","subtypeDescription":"Book Chapter","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/56785167900","authid":"56785167900","authname":"Agarwal N.","surname":"Agarwal","given-name":"Namita","initials":"N.","afid": [{"@_fa": "true", "$" :"60104116"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/58039592400","authid":"58039592400","authname":"Kumar A.","surname":"Kumar","given-name":"Amit","initials":"A.","afid": [{"@_fa": "true", "$" :"60104116"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/55568517829","authid":"55568517829","authname":"Singh P.K.","surname":"Singh","given-name":"Pradeep Kumar","initials":"P.K.","afid": [{"@_fa": "true", "$" :"60076033"}]}],"authkeywords":"Discrete contourlet transform (DCT) | Discrete wavelet transform (DWT) | Peak signal-to-noise ratio (PSNR) | Quaternion hadamard transform (QHT) | Watermarking","source-id":"21101032300","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111446643"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111446643?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111446643&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111446643&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111446643","dc:identifier":"SCOPUS_ID:85111446643","eid":"2-s2.0-85111446643","dc:title":"Strategies for generating panoramic video images without information about scene correspondences for multispectral distributed aperture systems","dc:creator":"Kudinov I.A.","prism:publicationName":"Computer Optics","prism:issn":"01342452","prism:eIssn":"24126179","prism:volume":"45","prism:issueIdentifier":"4","prism:pageRange":"589-599","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.18287/2412-6179-CO-846.","dc:description":"We derive analytical expressions for calculating the number of elementary computational operations required to generate several personal regions of interest in a panoramic computer-vision distributed- aperture system using two alternative strategies: Strategy 1 involves acquisition of a complete panoramic frame, followed by the selection of personal regions of interest, while with strategy 2 the region of interest is directly formed for each user. The parameters of analytical expressions include the number of cameras in the distributed system, the number of users, and the resolution of panorama and user frames. The formulas obtained for the given parameters make it possible to determine a strategy that would be optimal in terms of a criterion of the minimum number of elementary computational operations for generating multiple personal regions of interest. The region of interest is generated using only a priori information about the internal and external camera parameters, obtained as a result of their photogrammetric calibration with a universal test object, and does not take into account information about scene correspondences at the boundaries of intersecting fields of view.","citedby-count":"3","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60095553","afid":"60095553","affilname":"Ryazan State Radio Engineering University","affiliation-city":"Ryazan","affiliation-country":"Russian Federation"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "3", "$" :"3"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/6603399007","authid":"6603399007","authname":"Kudinov I.A.","surname":"Kudinov","given-name":"Igor Alekseevich","initials":"I.A.","afid": [{"@_fa": "true", "$" :"60095553"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55980530400","authid":"55980530400","authname":"Nikiforov M.B.","surname":"Nikiforov","given-name":"Mikhail Borisovich","initials":"M.B.","afid": [{"@_fa": "true", "$" :"60095553"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/36463653700","authid":"36463653700","authname":"Kholopov I.S.","surname":"Kholopov","given-name":"Ivan Sergeevich","initials":"I.S.","afid": [{"@_fa": "true", "$" :"60095553"}]}],"authkeywords":"Camera calibration | Panoramic image | Quaternions","source-id":"21100203110","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111147428"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111147428?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111147428&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111147428&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111147428","dc:identifier":"SCOPUS_ID:85111147428","eid":"2-s2.0-85111147428","dc:title":"16th EAI International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness, QShine 2020","prism:publicationName":"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","prism:issn":"18678211","prism:eIssn":"1867822X","prism:isbn": [{"@_fa": "true", "$" :"9783030775681"}],"prism:volume":"381","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","dc:description":"The proceedings contain 19 papers. The special focus in this conference is on Heterogeneous Networking for Quality, Reliability, Security and Robustness. The topics include: Analysis of Spectrum Detection and Decision Using Machine Learning Algorithms in Cognitive Mobile Radio Networks; AutoMTS: Fully Autonomous Processing of Multivariate Time Series Data from Heterogeneous Sensor Networks; image Extrapolation Based on Perceptual Loss and Style Loss; comparison of Two Fourier Transform Methods in Modulation Measurement Profilometry; Research on Image Enhancement Model Based on Variable Order Fractional Differential CLAHE; optimum Parameter Estimation Under Additive Cauchy-Gaussian Mixture Noise; face Reconstruction with Specific Weight Mask; stability Analysis of Quaternion-Valued Neural Network with Non-differentiable Time-Varying Delays and Constant Delays; learn to Rectify Label Through Kernel Extreme Learning Machine; Hardware Trojan Detection Method Based on Multi-featured GEP; sleep Apnea Monitoring System Based on Channel State Information; Energy-Efficient DAC Scheme Based on Unit Capacitor Switching for SAR ADCs; The SDN-Governed Ad Hoc Swarm for Mobile Surveillance of Meteorological Facilities; research on Optimizing the Location and Capacity of Electric Vehicle Charging Stations; Research on Semantic Vision SLAM Towards Dynamic Environment; IAA Spectral Estimation in the Selective Range; robust Frequency Estimation Under Additive Mixture Noise.","citedby-count":"0","prism:aggregationType":"Book Series","subtype":"cr","subtypeDescription":"Conference Review","author-count":{"@limit": "100", "@total": "0"},"source-id":"21100220348","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111044544"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111044544?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111044544&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111044544&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111044544","dc:identifier":"SCOPUS_ID:85111044544","eid":"2-s2.0-85111044544","dc:title":"Reduced Biquaternion Stacked Denoising Convolutional AutoEncoder for RGB-D Image Classification","dc:creator":"Huang X.","prism:publicationName":"IEEE Signal Processing Letters","prism:issn":"10709908","prism:eIssn":"15582361","prism:volume":"28","prism:pageRange":"1205-1209","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/LSP.2021.3088049","dc:description":"RGB-D image classification based on convolutional neural networks have been extensively explored recently. However, they suffer from problems of effective representation of RGB-D image, intra-class variances and inter-class similarities. To address these problems, this letter proposes a novel RGB-D image classification framework based on reduced biquaternion stacked denoising convolutional autoencoder (RQ-SDCAE). The proposed framework can encode and extract the depth feature effectively by using the reduced biquaternion. The stacked training method is utilized to train the proposed reduced biquaternion convolutional autoencoder. Extensive evaluations for RGB-D image classification demonstrate that RQ-SDCAE outperforms the state-of-the-art methods.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60083519","afid":"60083519","affilname":"Nanchang Hangkong University","affiliation-city":"Nanchang","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57211156043","authid":"57211156043","orcid":"0000-0001-5569-4255","authname":"Huang X.","surname":"Huang","given-name":"Xiang","initials":"X.","afid": [{"@_fa": "true", "$" :"60083519"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/35748407200","authid":"35748407200","orcid":"0000-0001-6139-1410","authname":"Gai S.","surname":"Gai","given-name":"Shan","initials":"S.","afid": [{"@_fa": "true", "$" :"60083519"}]}],"authkeywords":"autoencoder | hypercomplex network | image classification | Reduced biquaternion | RGB-D","article-number":"9449952","source-id":"17316","fund-acr":"NCHU","fund-no":"20202BABL202038","fund-sponsor":"Nanchang Hangkong University","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111005907"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85111005907?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85111005907&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85111005907&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85111005907","dc:identifier":"SCOPUS_ID:85111005907","eid":"2-s2.0-85111005907","dc:title":"Deep quaternion Fourier transform for salient object detection","dc:creator":"Revathi T.","prism:publicationName":"Journal of Intelligent and Fuzzy Systems","prism:issn":"10641246","prism:eIssn":"18758967","prism:volume":"40","prism:issueIdentifier":"6","prism:pageRange":"11331-11340","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.3233/JIFS-202502","dc:description":"Salient object detection plays a vital role in image processing applications like image retrieval, security and surveillance in authentic-time. In recent times, advances in deep neural network gained more attention in the automatic learning system for various computer vision applications. In order to decrement the detection error for efficacious object detection, we proposed a detection classifier to detect the features of the object utilizing a deep neural network called convolutional neural network (CNN) and discrete quaternion Fourier transform (DQFT). Prior to CNN, the image is pre-processed by DQFT in order to handle all the three colors holistically to evade loss of image information, which in-turn increase the effective use of object detection. The features of the image are learned by training model of CNN, where the CNN process is done in the Fourier domain to quicken the method in productive computational time, and the image is converted to spatial domain before processing the fully connected layer. The proposed model is implemented in the HDA and INRIA benchmark datasets. The outcome shows that convolution in the quaternion Fourier domain expedite the process of evaluation with amended detection rate. The comparative study is done with CNN, discrete Fourier transforms CNN, RNN and masked RNN.","citedby-count":"1","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60107090","afid":"60107090","affilname":"Hindustan Institute of Technology and Science","affiliation-city":"Chennai","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60079728","afid":"60079728","affilname":"SSN College of Engineering, Kalavakkam","affiliation-city":"Kanchipuram","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60000590","afid":"60000590","affilname":"Universidade Federal de Juiz de Fora","affiliation-city":"Juiz de Fora","affiliation-country":"Brazil"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57212113473","authid":"57212113473","authname":"Revathi T.","surname":"Revathi","given-name":"T.","initials":"T.","afid": [{"@_fa": "true", "$" :"60079728"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/55385775500","authid":"55385775500","authname":"Rajalaxmi T.M.","surname":"Rajalaxmi","given-name":"T. M.","initials":"T.M.","afid": [{"@_fa": "true", "$" :"60079728"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/56586381700","authid":"56586381700","authname":"Sundara Rajan R.","surname":"Sundara Rajan","given-name":"R.","initials":"R.","afid": [{"@_fa": "true", "$" :"60107090"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/32867756900","authid":"32867756900","authname":"Freire W.P.","surname":"Freire","given-name":"Wilhelm Passarella","initials":"W.P.","afid": [{"@_fa": "true", "$" :"60000590"}]}],"authkeywords":"Convolutional neural networks | discrete quaternion Fourier transform | image enhancement | object detection | quaternion complex variable","source-id":"23917","fund-acr":"पऊवि","fund-no":"undefined","fund-sponsor":"Department of Atomic Energy, Government of India","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85110925138"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85110925138?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110925138&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85110925138&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85110925138","dc:identifier":"SCOPUS_ID:85110925138","eid":"2-s2.0-85110925138","dc:title":"Fusion of Handcrafted and Deep Features for Forgery Detection in Digital Images","dc:creator":"Walia S.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"99742-99755","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3096240","dc:description":"Content authentication of digital images has captured the attention of forensic experts and security researchers due to a multi-fold increase in the dissemination of multimedia data through the open and vulnerable Internet. Shrewd attackers successfully devise novel ways to challenge state-of-art forensic tools used for forgery detection in digital images. Feature engineering approaches have yielded up to 97% accuracy on benchmarked datasets. Deep learning approaches have shown promising results in various image classification problems but cannot find hidden patterns in digital images, which can reliably detect image forgeries. State-of-art accuracy of deep learning approaches for forgery detection is up to 98% on benchmarked datasets. The objective of the proposed approach is to further escalate the detection accuracy, pushing it near 100%. In this paper, a synergy of handcrafted features based on color characteristics and deep features using the image's luminance channel is employed to mine patterns responsible for accurate forgery detection. In the first Stream, 648-D Markov-based features are computed from the quaternion discrete cosine transform of the image. In the second Stream, the luminance channel of YCbCr colorspace is used to extract the Local Binary Pattern of the image. Further, local binary feature maps are fed to the pre-trained ResNet-18 model to obtain a 512-D feature vector called 'ResFeats' from the last layer of the model's convolutional base portion. The handcrafted features from Stream I and ResFeats from Stream II are combined to form an 1160-D feature vector. Further, classification is performed using a shallow neural network, and the method is tested on CASIA v1 and CASIA v2 datasets. The accuracy of the proposed fusion-based approach is 99.3% on benchmark datasets.","citedby-count":"25","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60116744","afid":"60116744","affilname":"University Institute of Engineering and Technology","affiliation-city":"Chandigarh","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60113843","afid":"60113843","affilname":"Maharaja Ranjit Singh Punjab Technical University, Bathinda","affiliation-city":"Bhatinda","affiliation-country":"India"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60103673","afid":"60103673","affilname":"Itä-Suomen yliopisto","affiliation-city":"Kuopio","affiliation-country":"Finland"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57188975386","authid":"57188975386","orcid":"0000-0002-8755-2873","authname":"Walia S.","surname":"Walia","given-name":"Savita","initials":"S.","afid": [{"@_fa": "true", "$" :"60116744"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/26021294900","authid":"26021294900","orcid":"0000-0001-9877-0238","authname":"Kumar K.","surname":"Kumar","given-name":"Krishan","initials":"K.","afid": [{"@_fa": "true", "$" :"60116744"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/26633172500","authid":"26633172500","orcid":"0000-0003-0115-1620","authname":"Kumar M.","surname":"Kumar","given-name":"Munish","initials":"M.","afid": [{"@_fa": "true", "$" :"60113843"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/55488358100","authid":"55488358100","orcid":"0000-0002-0078-5675","authname":"Gao X.Z.","surname":"Gao","given-name":"Xiao Zhi","initials":"X.Z.","afid": [{"@_fa": "true", "$" :"60103673"}]}],"authkeywords":"deep learning | Image forensics | image manipulations | local binary pattern | machine learning | Markov process | pre-trained networks","article-number":"9481119","source-id":"21100374601","fund-no":"undefined","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85110887255"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85110887255?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85110887255&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85110887255&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85110887255","dc:identifier":"SCOPUS_ID:85110887255","eid":"2-s2.0-85110887255","dc:title":"A Survey on Quaternion Algebra and Geometric Algebra Applications in Engineering and Computer Science 1995-2020","dc:creator":"Bayro-Corrochano E.","prism:publicationName":"IEEE Access","prism:eIssn":"21693536","prism:volume":"9","prism:pageRange":"104326-104355","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1109/ACCESS.2021.3097756","dc:description":"Geometric Algebra (GA) has proven to be an advanced language for mathematics, physics, computer science, and engineering. This review presents a comprehensive study of works on Quaternion Algebra and GA applications in computer science and engineering from 1995 to 2020. After a brief introduction of GA, the applications of GA are reviewed across many fields. We discuss the characteristics of the applications of GA to various problems of computer science and engineering. In addition, the challenges and prospects of various applications proposed by many researchers are analyzed. We analyze the developments using GA in image processing, computer vision, neurocomputing, quantum computing, robot modeling, control, and tracking, as well as improvement of computer hardware performance. We believe that up to now GA has proven to be a powerful geometric language for a variety of applications. Furthermore, there is evidence that this is the appropriate geometric language to tackle a variety of existing problems and that consequently, step-by-step GA-based algorithms should continue to be further developed. We also believe that this extensive review will guide and encourage researchers to continue the advancement of geometric computing for intelligent machines.","citedby-count":"12","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/126681961","afid":"126681961","affilname":"Cinvestav-Guadalajara Unit","affiliation-city":"Zapopan","affiliation-country":"Mexico"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/55995861000","authid":"55995861000","orcid":"0000-0002-4738-3593","authname":"Bayro-Corrochano E.","surname":"Bayro-Corrochano","given-name":"Eduardo","initials":"E.","afid": [{"@_fa": "true", "$" :"126681961"}]}],"authkeywords":"and biotechnology | artificial intelligence | biomedical engineering | Clifford algebra | computer vision | control engineering | electrical engineering and power systems | Geometric algebra | geometric and quantum computing | graphic engineering | image processing | machine learning | neural networks | quaternion algebra | robotics | screw theory | signal processing","article-number":"9488174","source-id":"21100374601","fund-no":"A1-S-10412","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherfullgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108904383"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108904383?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108904383&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85108904383&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85108904383","dc:identifier":"SCOPUS_ID:85108904383","eid":"2-s2.0-85108904383","dc:title":"An effective weighted vector median filter for impulse noise reduction based on minimizing the degree of aggregation","dc:creator":"Meng X.","prism:publicationName":"IET Image Processing","prism:issn":"17519659","prism:volume":"15","prism:issueIdentifier":"1","prism:pageRange":"228-238","prism:coverDate":"2021-01-01","prism:coverDisplayDate":"January 2021","prism:doi":"10.1049/ipr2.12023","dc:description":"Impulse noise is regarded as an outlier in the local window of an image. To detect noise, many proposed methods are based on aggregated distance, including spatially weighted aggregated distance, n nearest neighbour distance, local density, and angle-weighted quaternion aggregated distance. However, these methods ignore the weight of each pixel or have limited adaptability. This study introduces the concept of degree of aggregation and proposes a weighting method to obtain the weight vector of the pixels by minimizing the degree of aggregation. The weight vector obtained gives larger components on the signal pixels than on the noisy pixels. Then it is fused with the aggregated distance to form a weighted aggregated distance that can reasonably characterise the noise and signal. The weighted aggregated distance, along with an adaptive segmentation method, can effectively detect the noise. To further enhance the effect of noise detection and removal, an adaptive selection strategy is incorporated to reduce the noise density in the local window. At last, noisy pixels detected are replaced with the weighted channel combination optimization values. The experimental results exhibit the validity of the proposed method by showing better performance in terms of both objective criteria and visual effects.","citedby-count":"8","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60003028","afid":"60003028","affilname":"Wuhan Institute of Technology","affiliation-city":"Wuhan","affiliation-country":"China"}],"prism:aggregationType":"Journal","subtype":"ar","subtypeDescription":"Article","author-count":{"@limit": "100", "@total": "4", "$" :"4"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57225010014","authid":"57225010014","authname":"Meng X.","surname":"Meng","given-name":"Xiangxi","initials":"X.","afid": [{"@_fa": "true", "$" :"60003028"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/24080890200","authid":"24080890200","authname":"Lu T.","surname":"Lu","given-name":"Tongwei","initials":"T.","afid": [{"@_fa": "true", "$" :"60003028"}]},{"@_fa": "true", "@seq": "3", "author-url":"https://api.elsevier.com/content/author/author_id/35739113000","authid":"35739113000","authname":"Min F.","surname":"Min","given-name":"Feng","initials":"F.","afid": [{"@_fa": "true", "$" :"60003028"}]},{"@_fa": "true", "@seq": "4", "author-url":"https://api.elsevier.com/content/author/author_id/56406646300","authid":"56406646300","authname":"Lu T.","surname":"Lu","given-name":"Tao","initials":"T.","afid": [{"@_fa": "true", "$" :"60003028"}]}],"source-id":"5400152646","fund-no":"D20181504","openaccess":"1","openaccessFlag":true,"freetoread":{"value": [{"$" :"all"},{"$" :"publisherhybridgold"}]},"freetoreadLabel":{"value": [{"$" :"All Open Access"},{"$" :"Hybrid Gold"}]}},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108795033"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108795033?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108795033&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85108795033&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85108795033","dc:identifier":"SCOPUS_ID:85108795033","eid":"2-s2.0-85108795033","dc:title":"Multi-template multi-channel image matching using tensor products of low-dimensional clifford algebras","dc:creator":"DelMarco S.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:eIssn":"1996756X","prism:isbn": [{"@_fa": "true", "$" :"9781510643055"}],"prism:volume":"11734","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1117/12.2585801","dc:description":"The rapid expansion of data generated from various sensors, devices, and applications offers the opportunity to exploit complex data relationships in new ways. Multi-dimensional data often appears as multiple, separate data channels, for which multi-channel data representations and analysis techniques have been developed. Alternately, for image matching, multiple-template image matching techniques have been developed. Multi-template approaches use multiple, often single-channel, templates, exhibiting intra-class variations. Same-class test image exemplars must match all reference templates. In this paper, we combine multiple-template matching techniques with multi-channel data representations to provide multi-template, multi-channel image matching. We represent image data with pixels taking values in tensor products of low-dimensional Clifford Algebras, for which Fourier transforms exist. Fourier domain matching provides a computational processing improvement over spatial correlation-based matchers. The tensor product approach provides a decomposition of higher-dimensional algebras into combinations of lower dimensional algebras for which Fourier transforms apply. The tensor product approach produces a performance advantage, on data with the appropriate inter-channel correlation characteristics, through exploitation of additional data channel correlations. When these add constructively, a matching performance benefit occurs. We define an anti-involution mapping on a tensor product space, which leads to a definition of image correlation over these spaces. We prove that the correlation satisfies an extended inner product definition. We prove a Cauchy-Schwartz inequality, which validates use of the image correlation as a matcher. We present an example, using synthetic image data, where the approach provides superior matching performance over classical score sum fusion.","citedby-count":"0","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60017016","afid":"60017016","affilname":"BAE Systems Inc.","affiliation-city":"Arlington","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "1", "$" :"1"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/8835566600","authid":"8835566600","authname":"DelMarco S.","surname":"DelMarco","given-name":"Stephen","initials":"S.","afid": [{"@_fa": "true", "$" :"60017016"}]}],"authkeywords":"Clifford algebra | Correlation | Image | Multi-channel | Quaternion","article-number":"1173404","source-id":"40067","fund-no":"undefined","openaccess":"0","openaccessFlag":false},{"@_fa": "true", "link": [{"@_fa": "true", "@ref": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108636709"},{"@_fa": "true", "@ref": "author-affiliation", "@href": "https://api.elsevier.com/content/abstract/scopus_id/85108636709?field=author,affiliation"},{"@_fa": "true", "@ref": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85108636709&origin=inward"},{"@_fa": "true", "@ref": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85108636709&origin=inward"}],"prism:url":"https://api.elsevier.com/content/abstract/scopus_id/85108636709","dc:identifier":"SCOPUS_ID:85108636709","eid":"2-s2.0-85108636709","dc:title":"An in-situ real-time hidden damage inspection on C-17 globemaster III composite aileron using LSP technique under thermal excitation","dc:creator":"Fong R.Y.","prism:publicationName":"Proceedings of SPIE - The International Society for Optical Engineering","prism:issn":"0277786X","prism:eIssn":"1996756X","prism:isbn": [{"@_fa": "true", "$" :"9781510640115"}],"prism:volume":"11591","prism:pageRange":null,"prism:coverDate":"2021-01-01","prism:coverDisplayDate":"2021","prism:doi":"10.1117/12.2585247","dc:description":"A non-contact, full-field vision-based non-destructive inspection (V-NDI) system was developed with multiple damages detection capabilities in composite structures under thermal excitation. In contrast to point-based nondestructive inspection (P-NDI) systems employing laser Doppler vibrometer (LDV) with discrete wavefield captured by pointwise scanning, the V-NDI system captures higher spatial resolution wavefield by a CMOS camera for every time instance without repeating the experiment tens of thousand times to reassemble the wavefield like P-NDI. An Advanced Damage Processing Network (ADPNet) was proposed with laser speckle photometry (LSP) employed in a V-NDI system for hidden damages inspection. The LSP/ADPNet system relies on observing the variation of speckle clouds in time sequence without a baseline and is very insensitive to ambient noise with statistics-based image processing where traditional holography/ESPI suffers greatly. Other advances of LSP/ADPNet system are its robust tolerance of laser coherence, larger illumination area, flexible choice of correlation functions, and more advanced post-processing techniques such as Bayesian updating/inference or unsupervised image segmentation that can be readily applied. Thermal excitation can have very large power throughput in hundreds of watts compared to traditional PZT actuator, merely in a few watt ranges. Laser speckle itself is the result of self-interference scatter field reflected from a rough surface, and each speckle can be treated as a sensing point from a randomly distributed speckle cloud. By observing the variation of speckle cloud on the structure surface, displacement related quantities in higher dimensions (e.g. hypercomplex envelope, phase between real-valued signal and its quadrature, phase congruency, etc.) can be deduced by the Riesz bp transform and correlated in time sequence to highlight the location of hidden damages in a very effective way. Another novelty of this paper is to make a super compact real-time LSP system on LabVIEW FPGA by applying ADPNet comprised of the Riesz bp transform, non-linear filter bank and unsupervised image segmentation to quantify/characterize barely visible impact damages (BVID) on a C-17 Globemaster III composite aileron. To conclude, the images processed by LSP/ADPNet of a V-NDI system show a very good agreement with ultrasonic C-scan and pulse laser/LDV wavefield reconstruction results. It is also demonstrated to be more accurate and robust than Digital Image Correlation (DIC) for minute deformation (sub-nano to nano meter) measurement and large area (meter by meter) inspection under industrial environment.","citedby-count":"2","affiliation": [{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60029780","afid":"60029780","affilname":"National Institute of Aerospace","affiliation-city":"Hampton","affiliation-country":"United States"},{"@_fa": "true", "affiliation-url":"https://api.elsevier.com/content/affiliation/affiliation_id/60004923","afid":"60004923","affilname":"NC State University","affiliation-city":"Raleigh","affiliation-country":"United States"}],"prism:aggregationType":"Conference Proceeding","subtype":"cp","subtypeDescription":"Conference Paper","author-count":{"@limit": "100", "@total": "2", "$" :"2"},"author": [{"@_fa": "true", "@seq": "1", "author-url":"https://api.elsevier.com/content/author/author_id/57224932715","authid":"57224932715","authname":"Fong R.Y.","surname":"Fong","given-name":"Rey Yie","initials":"R.Y.","afid": [{"@_fa": "true", "$" :"60004923"},{"@_fa": "true", "$" :"60029780"}]},{"@_fa": "true", "@seq": "2", "author-url":"https://api.elsevier.com/content/author/author_id/7201350037","authid":"7201350037","authname":"Yuan F.G.","surname":"Yuan","given-name":"Fuh Gwo","initials":"F.G.","afid": [{"@_fa": "true", "$" :"60004923"},{"@_fa": "true", "$" :"60029780"}]}],"authkeywords":"ADPNet | BVID | DIC | FPGA | Hypercomplex domain | LSP | Non-destructive inspection | Riesz bp transform | Unsupervised image segmentation | Wavefield reconstruction","article-number":"115910Q","source-id":"40067","fund-acr":"LaRC","fund-no":"undefined","fund-sponsor":"Langley Research Center","openaccess":"0","openaccessFlag":false}]}}